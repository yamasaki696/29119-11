ソフトウェアおよびシステムエンジニアリング - ソフトウェアテスト - 第11部：AIベースのシステムのテストに関するガイドライン
========================================================================================================================

序文
====

ISO（International Organization for Standardization：国際標準化機構）とIEC（International ElectrotechnicalCommission：国際電気標準会議）は、世界的な標準化のための専門組織です。ISOまたはIECに加盟している国の機関は、それぞれの機関が技術活動の特定分野を扱うために設立した技術委員会を通じて、国際規格の策定に参加します。ISOとIECの技術委員会は、相互に関心のある分野で協力しています。その他の国際機関、政府機関、非政府機関も、ISOやIECと連携して作業に参加しています。

この文書を作成するために使用された手順と、今後のメンテナンスのための手順は、ISO/IEC専門業務用指針第1部に記載されています。特に、文書の種類によって必要とされる承認基準が異なることに留意すべきである。本文書は、ISO/IEC専門業務用指針第2部の編集規則に従って作成されている（www.iso.org/directives
参照）。

この文書のいくつかの要素が特許権の対象となる可能性があることに注意を喚起する。ISO及びIECは，このような特許権の一部または全部を特定する責任を負わない。本文書の作成中に確認された特許権の詳細は，序文，及び／または，ISOの特許宣言リスト（
www.iso.org/patents 参照），IECの特許宣言リスト（patents.IEC.ch
参照）に記載される。

この資料で使用されている商標名は、ユーザーの便宜のために提供されている情報であり、推奨を意味するものではありません。

規格の任意性、適合性評価に関するISO特有の用語や表現の意味、TBT（Technical
Barriers to
Trade：貿易の技術的障害）における世界貿易機関（WTO）の原則へのISOの準拠についての説明は、www.iso.org/iso/foreword.html。

この文書は，Joint Technical Committee ISO/IEC JTC 1, Information
Technology, Subcommittee SC 7, Software and systems engineering
によって作成された。

ISO/IEC/IEEE
29119シリーズの全部品のリストは、ISOのウェブサイトに掲載されています。

この文書に関するご意見やご質問は、ユーザーの国の規格機関にお寄せください。これらの機関の完全なリストは
www.iso.org/members.html でご覧いただけます。

はじめに
========

従来のシステムのテストはよく理解されていますが、日常生活に欠かせないものとして普及しつつあるAIベースのシステムには、新たな課題があります。このドキュメントは、AIベースのシステムを紹介し、どのようにテストすべきかのガイドラインを提供するために作成されました。

付録Aでは、機械学習の紹介をしています。

このドキュメントは、主にAIベースのシステムに初めて触れるテスターのために提供されていますが、経験豊富なテスターや、AIベースのシステムの開発やテストに携わるその他の関係者にも役立ちます。

本資料は、技術報告書として、通常の国際規格や技術仕様書として発行されるデータとは異なる種類のデータ、例えば「技術の現状」に関するデータを含んでいます。

1 スコープ
==========

このドキュメントでは、AIベースのシステムについて紹介しています。これらのシステムは一般的に複雑で（例：ディープニューラルネット）、時にはビッグデータに基づいており、仕様が不十分であったり、非決定論的であったりするため、テストに新たな課題や機会をもたらします。

このドキュメントでは、AIベースのシステムに特有の特性を説明し、それに対応してAIベースのシステムの受け入れ基準を規定することの難しさを説明しています。

このドキュメントでは、AIベースのシステムをテストする際の課題を提示しています。主な課題は、テスト担当者がテストの期待される結果を判断することが難しく、その結果、テストが成功したのか失敗したのかを判断することができないというテストラクル問題です。この本は、AIベースのシステムのテストをライフサイクル全体にわたってカバーしており、ブラックボックス・アプローチを用いたAIベースのシステム全般のテスト方法に関するガイドラインと、ニューラルネットワークに特化したホワイトボックス・テストについて紹介しています。また、AIベースのシステムのテストに使用するテスト環境やテストシナリオのオプションについても説明しています。

本ドキュメントでは、AIベースのシステムとは、少なくとも1つのAIコンポーネントを含むシステムのことです。

2 基準となる文献
================

このドキュメントには、規範となる文献はありません。

3 用語、定義、略語
==================

3.1 用語と定義
--------------

この文書では、以下の用語と定義が適用されます。

ISOとIECは、標準化に使用するための用語データベースを以下のアドレスで公開しています。

- ISOオンライン閲覧プラットフォーム： https://www.iso.org/obp
    で利用可能
- IEC Electropedia: http://www.electropedia.org/ で入手可能。

### 3.1.1 A/Bテスト

スプリットランテスト

2つのシステムやコンポーネントのうち、どちらの性能が優れているかをテスト担当者が判断するための統計的テスト手法。

### 3.1.2 精度

\<機械学習（3.1.43）\>
分類器（3.1.21）の評価に使用される性能指標で、分類（3.1.20）の予測（3.1.56）が正しかった割合を測定する。

### 3.1.3 活性化関数

伝達関数

ニューラルネットワーク（3.1.48）＞
ニューラルネットワークのノードに関連する式で、ニューロンへの入力からノードの出力（活性化値（3.1.4））を決定する。

### 3.1.4 アクティベーション値

\<ニューラルネットワーク（3.1.48）\>
ニューラルネットワークのノードの活性化関数（3.1.3）の出力

### 3.1.5 適応性

機能要件と非機能要件の両方を満たし続けるために、環境の変化に対応するシステムの能力

### 3.1.6 アドバーサリアル・アタック

MLモデル(3.1.46)を失敗させるために、敵対的な例題(3.1.7)を意図的に使用する。

エントリーへの注釈1。一般的には、ニューラルネットワーク形式のMLモデルを対象とする（3.1.48）。

### 3.1.7 敵対的な例

MLモデル(3.1.46)への入力で、作業例に小さな摂動を加えることで、モデルが誤った結果を高い信頼度で出力することができる。

エントリへの注記1。一般的には、ニューラルネットワーク形式のMLモデルに適用される（3.1.48）。

### 3.1.8 敵対的なテスト

MLモデル（3.1.46）の欠陥を特定するために、敵対的な例の作成と実行を試みる（3.1.7）ことに基づくテストアプローチ

エントリへの注記1。一般的にはニューラルネットワーク形式のMLモデルに適用される（3.1.48）。

### 3.1.9 AIを使ったシステム

AIを実装する1つまたは複数のコンポーネントを含むシステム(3.1.13)

### 3.1.10 AIの効果

以前はAI（3.1.13）と表示されていたシステムが、技術の進歩によりAIとは言えなくなった場合。

### 3.1.11 AI品質メタモデル

AIベースのシステムの品質を保証するためのメタモデル(3.1.9)

エントリーへの注記1このメタモデルは DIN SPEC 92001
で詳細に定義されています。

### 3.1.12 アルゴリズム

MLアルゴリズム

\<機械学習(3.1.43)\>
学習データ(3.1.80)からMLモデル(3.1.46)を作成するためのアルゴリズム

例として、MLアルゴリズムには、線形回帰、ロジスティック回帰、決定木（3.1.25）、SVM、NaiveBayes、kNN、K-means、ランダムフォレストなどがあります。

### 3.1.13 人工知能

AI

知識や技能を習得し、処理し、適用するエンジニアリング・システムの能力

### 3.1.14 自律システム

人の手を煩わせることなく持続的に動作することができるシステム

### 3.1.15 オートノミー

人が介在しなくてもシステムが持続的に動作する能力

### 3.1.16 バックトゥバック・テスト

ディファレンシャル・テスト

システムの代替バージョンを擬似的に使用して（3.1.59）、同じテスト入力から比較のために期待される結果を生成するというテストのアプローチ。

例
擬似オラクルは、既に存在するシステムでも、独立したチームが開発したシステムでも、異なるプログラミング言語で実装されたシステムでもよい。

### 3.1.17 バックワード・プロパゲーション

\<ニューラルネットワーク（3.1.48）\>
人工的なニューラルネットワークにおいて、ネットワークの出力で計算されたエラーに基づいて、ネットワークの接続に使用する重みを決定するために使用される方法。

エントリーへの注釈1。ディープニューラルネットワークの学習に使用されます（3.1.27）。

### 3.1.18 ベンチマーク・スイート

ベンチマークの集合体で、ベンチマークとは代替品の性能を比較するためのテストの集合体です。

### 3.1.19 バイアス

\<機械学習(3.1.43)\>
MLモデル(3.1.46)が提供する予測値と、望ましい公正な予測値(3.1.56)との間の距離の尺度

### 3.1.20 分類

\<機械学習（3.1.43）\>
与えられた入力に対して出力クラスを予測する機械学習関数

### 3.1.21 クラシファイア

\<機械学習(3.1.43)\> 分類(3.1.20)に用いるMLモデル(3.1.46)

### 3.1.22 クラスタリング

同じグループ（すなわちクラスタ）内のオブジェクトが他のクラスタ内のオブジェクトよりも互いに類似しているように、オブジェクトのセットをグループ化すること。

### 3.1.23 コンビナトリアル・テスト

いくつかのパラメータの値の特定の組み合わせを実行するようにテストケースを設計するブラックボックステスト設計技法
(3.1.53)

例：ペアワイズテスト（3.1.52）、全組み合わせテスト、各選択肢テスト、基本選択肢テスト。

### 3.1.24 コンフュージョン・マトリクス

真値と偽値がわかっているテストデータ（3.1.75）に対する分類器（3.1.21）の性能を表すために使用される表。

### 3.1.25 決定木

\<機械学習(3.1.43)\>
1つ以上のツリー状の構造を辿ることで推論を表現できる教師付き学習モデル(3.1.46)

### 3.1.26 深層学習

1つ以上の隠れた層を持つニューラルネットワーク(3.1.48)を学習することで、豊かな階層的表現を生み出すアプローチ。

エントリーへの注釈1深層学習では、単純な計算ユニット（ニューロン）を多層化したネットワークを使用します。ニューロンネットワークでは、各ユニットが入力値を組み合わせて出力値を生成し、その出力値が下流の他のニューロンに伝えられます。

### 3.1.27 ディープニューラルネット

2層以上のニューラルネットワーク (3.1.48)

### 3.1.28 決定論的なシステム

特定の入力セットと開始状態が与えられたときに、常に同じ出力セットと最終状態を生成するシステム

### 3.1.29 ディストリビューション・シフト

dataset shift \<機械学習（3.1.43）\>
訓練データ（3.1.80）の分布とdesireddataの分布との距離。

エントリへの注記1。ユーザーのシステムとの関わり方（および希望するデータ分布）が時間とともに変化すると、分布のずれの影響が大きくなることが多い。

### 3.1.30 ドリフト

劣化の恒常性＜機械学習（3.1.43）＞時間の経過に伴うMLモデル（3.1.46）の挙動の変化

エントリーへの注記1これらの変更は一般的に予測（3.1.56）の精度を低下させ、新しいデータでモデルを再トレーニングする必要があるかもしれません。

### 3.1.31 説明可能性

\<AI（3.1.13）\>
AIを使ったシステム（3.1.9）がどのようにして与えられた結果を導き出したのかを理解するレベル

### 3.1.32 探索的テスト

経験ベースのテストとは、テスト者の既存の関連知識、テスト項目の事前調査（過去のテストの結果を含む）、一般的なソフトウェアの動作や障害の種類に関するヒューリスティックな「経験則」に基づいて、テスト者が自発的にテストを設計・実行するものです。

エントリへの注記
1:探索的テストでは、隠された特性（隠された動作を含む）を探し出します。これらの特性は、それ自体は無害かもしれませんが、テスト対象のソフトウェアの他の特性に干渉する可能性があり、ソフトウェアが失敗するリスクとなります。

### 3.1.33 F1スコア

\<機械学習（3.1.43）\>
分類器（3.1.21）の評価に使用される性能指標で、回収率（3.1.61）と精度（3.1.55）のバランス（調和的平均）を提供する。

### 3.1.34 フォルス・ネガティブ

実際には合格しているのに、失敗したと誤って報告すること

エントリーへの注釈1。これはType IIエラーとも呼ばれています。

EXAMPLE
レフリーは、ゴールだったときにオフサイドを裁定し、そのため、アゴラが得点されたときにゴールの失敗を報告する。

### 3.1.35 偽陽性

実際には失敗であるにもかかわらず、パスであると誤って報道したこと。

エントリーへの注釈1。これはType Iエラーとも呼ばれます。

例）レフリーは、オフサイドであったため、与えられるべきではなかったゴールを与える。

### 3.1.36 フィーチャーエンジニアリング

特徴選択（機械学習（3.1.43））モデル（3.1.46）に現れるべき基本的な関係を最もよく表している生データの属性を、トレーニングデータ（3.1.80）に使用するために特定する活動。

### 3.1.37 柔軟性

最初の仕様以外の状況で動作するシステムの能力（目的を満たすために実際の状況に応じて動作を変更すること）。

### 3.1.38 ファズテスト

テスト項目の入力を生成するために、ファズと呼ばれる大量のランダムな（またはランダムに近い）データを使用するソフトウェアテスト手法。

### 3.1.39 一般的なAI

強力なAI AI（3.1.13）
認知能力の全範囲にわたって人間に匹敵する知的行動を示す。

### 3.1.40 グラフィカル・プロセッシング・ユニット

画像の描画などのディスプレイ機能に特化したGPU ASIC（Application Specific
Integrated Circuit）。

エントリーへの注釈1。GPUは、単一の機能を持つ画像の並列データ処理のために設計されていますが、この並列処理は、ニューラルネットワークなどのAIベースのソフトウェアの実行にも有効です（3.1.48）。

### 3.1.41 ハイパーパラメータ

\<ニューラルネットワーク（3.1.48）\>
ニューラルネットワークの構造とその学習方法を定義するための変数

エントリーへの注記1通常、ハイパーパラメータはモデルの開発者によって設定され（3.1.46）、チューニングパラメータと呼ばれることもあります（3.1.53）。

### 3.1.42 解釈可能性

\<AI（3.1.13）\>基盤となる（AI）技術がどのように機能するかを理解しているレベル

### 3.1.43 機械学習

データや経験からシステムが学ぶことを可能にする計算技術を用いたMLプロセス

### 3.1.44 メタモルフィックな関係

ソーステストケースからフォローアップテストケースへのテストインプットの変化が、ソーステストケースからフォローアップテストケースへの期待されるアウトプットにどのような変化を与えるか（または与えないか）の説明。

### 3.1.45 メタモルフィック・テスト

期待される結果が仕様書に基づくものではなく、過去の実績から推測されるテスト

### 3.1.46モデル

MLモデル＜機械学習（3.1.43）＞
入力データのパターンを利用して予測（3.1.56）を行うトレーニングデータセットで学習されたMLアルゴリズム（3.1.12）の出力。

### 3.1.47 ナローAI

weak AI AI (3.1.13)
特定の問題に対処するために、明確に定義された単一のタスクに集中する

### 3.1.48 ニューラルネットワーク

人工ニューラルネットワーク
重みを調整できるリンクで接続された原始的な処理要素からなるネットワークで、各要素が入力値に非線形関数を適用して値を生成し、それを他の要素に伝達したり、出力値として提示したりするもの。

エントリへの注記1。神経系のニューロンの働きをシミュレートすることを目的としたニューラルネットワークもありますが、ほとんどのニューラルネットワークは人工知能（3.1.13）においてコネクショニストモデル（3.1.46）を実現するものとして使われています。

エントリーへの注釈2非線形関数の例としては、閾値関数、シグモイド関数、多項式関数などがあります。\[SOURCE:ISO/IEC
2382:2015, 2120625, 修正-認められていた用語 \"ニューラルネット
\"が削除された；項目への注3から5が削除された\]。

### 3.1.49 ニューロンのカバレッジ

一連のテストにおいて、活性化したニューロンの割合をニューラルネットワークのニューロン総数で割ったもの(3.1.48)(通常はパーセンテージで表される)

エントリーへの注記1活性化値（3.1.4）がゼロを超えると、ニューロンは活性化しているとみなされます。

### 3.1.50 非決定論的システム

特定の入力セットと開始状態が与えられても、常に同じ出力セットと最終状態を生み出すとは限らないシステム。

### 3.1.51 オーバーフィッティング

\<機械学習(3.1.43)\>
訓練データ(3.1.80)と密接に対応するMLモデル(3.1.46)を生成し、その結果、新しいデータに一般化することが困難なモデルとなる。

### 3.1.52 ペアワイズテスト

ブラックボックステスト設計技法で、入力パラメータの各ペアの可能なすべての離散的な組み合わせを実行するようにテストケースを設計すること（3.1.53

エントリーへの注記1ペアワイズテストはコンビナートテストの中でも最もポピュラーな形式である（3.1.23）。

### 3.1.53 パラメータ

\<機械学習(3.1.43)\>
学習データ(3.1.80)をアルゴリズム(3.1.12)に適用して学習されたモデル(3.1.46)の部分

EXAMPLE ニューラルネットで学習した重み。

エントリーへの注釈1一般的に、パラメータはモデルの開発者が設定するものではありません。

### 3.1.54 パフォーマンス・メトリクス

\<機械学習(3.1.43)\>
分類(3.1.20)に用いられるMLモデル(3.1.46)の評価に用いられるメトリクス

例
代表的な評価指標としては、accuracy（3.1.2）、precision（3.1.55）、recall（3.1.61）、F1-score（3.1.33）などがあります。

### 3.1.55 精度

\<機械学習（3.1.43）\>
分類器（3.1.21）の評価に使用される性能指標で、予測された陽性が正しかった割合を測定します。

### 3.1.56の予測

\<機械学習（3.1.43）\>
与えられた入力に対して予測された目標値を得るための機械学習機能

EXAMPLE 分類（3.1.20）と回帰（3.1.62）の機能を含む。

### 3.1.57 前処理

\<機械学習（3.1.43）\>
生データをMLアルゴリズム（3.1.12）で使用可能な状態に変換し、MLモデル（3.1.46）を作成するMLワークフローの一部。

項目への注記1前処理には、分析、正規化、フィルタリング、再フォーマット、インピュテーション、外れ値や重複の除去、データセットの完全性の確保などが含まれる。

### 3.1.58 確率論的システム

出力を完全には予測できないような確率で動作が記述されるシステム。

### 3.1.59 疑似オラクル

派生テストオラクル
同じテスト入力に基づいてオリジナルのテスト項目の結果と比較される結果を生成するために使用される、テスト項目の独立した派生バリアント。

エントリーへの注記1疑似手口は、従来のテスト手口（3.1.76）が利用できない場合の有用な代替手段です。

### 3.1.60 推論技術

\<AI (3.1.13)\>
利用可能な情報から、演繹法や帰納法などの論理的手法を用いて結論を導き出すAIの形態。

### 3.1.61 リコール

感度 \<機械学習（3.1.43）\>
分類器（3.1.21）の評価に使用されるパフォーマンス指標で、正しく予測された実際の陽性の割合を測定する。

### 3.1.62 リグレッション

\<機械学習（3.1.43）\>
与えられた入力に対して、数値または連続した出力値を得る機械学習機能

### 3.1.63 規制基準

規制機関が公布した基準

### 3.1.64 強化学習

機械学習（3.1.43）＞目的を達成するために試行錯誤を繰り返しながらMLモデル（3.1.46）を構築する作業。

エントリへの注記1。強化学習タスクには、教師付き学習（3.1.74）に似た方法でのMLモデルの学習と、AI（3.1.13）システムの運用段階で収集されたラベルのない入力の学習が含まれます。モデルが予測(3.1.56)を行うたびに報酬が計算され、その報酬を最適化するためにさらなる試行が行われます。

エントリへの注記2。強化学習では、目的（成功の定義）をシステム設計者が定義することができます。

エントリへの注記3。強化学習では、報酬は、AIシステムが与えられた試行において目的を達成するためにどれだけ近づいたかを示す計算された数値とすることができます。

### 3.1.65 リワードハッキング

本来の目的を達成することを犠牲にして，報酬関数を最大化するためにエージェントが行う活動。

### 3.1.66 ロボット

ある程度の自律性（3.1.15）を持ち、環境の中で動き、意図されたタスクを実行するプログラムされた作動機構

エントリーへの注釈1ロボットには、制御システムと制御システムのインターフェースが含まれます。

項目への注記2。ロボットの産業用ロボットとサービスロボットへの分類（3.1.20）は、その目的とする用途に応じて行われる。

### 3.1.67 安全性

定義された条件の下で、システムが、人の生命、健康、財産、または環境が危険にさらされる状態に至らないことを期待すること【出典：ISO/IEC/IEEE
12207:2017, 3.1.48】。

### 3.1.68 検索アルゴリズム

\<AI（3.1.13）\>
ゴールとなる状態（または構造）に到達するまで、すべての可能な状態（または構造）のサブセットを系統的に訪れるアルゴリズム（3.1.12）。

### 3.1.69 自己学習システム

試行錯誤しながら学習していくことで行動を変えていく適応システム

### 3.1.70 サインチェンジの適用範囲

正負両方の活性化値で活性化されたニューロンの割合（3.1.4）をニューラルネットワークのニューロン総数（3.1.48）で割ったもの（通常はパーセンテージで表される）で、一連のテストについて。

エントリーへの注記1活性化値がゼロの場合は、負の活性化値とみなされます。

### 3.1.71 サイン・サインカバー

各ニューロンの符号を変化させることにより、次の層のニューロンの符号が変化し、次の層の他のニューロンは変化しない（すなわち、符号を変化させない）ことが個別に示される場合、カバレッジレベルが達成される。

### 3.1.72 シミュレータ

一連の制御された入力が与えられたときに、実際のシステムのように動作する、テスト時に使用される装置、コンピュータプログラム、システム。

### 3.1.73 ソフトウェアエージェント

環境を認識し、目標達成の可能性を最大化するような行動をとるデジタルエンティティ

### 3.1.74 教師あり学習

\<機械学習(3.1.43)\>
ラベル付けされたサンプルの入出力ペアに基づいて、入力を出力にマッピングする関数を学習するタスクです。

### 3.1.75 テストデータ

\<機械学習（3.1.43）\>
最終的にチューニングされたMLモデル（3.1.46）を偏りなく評価するための独立したデータセット。

### 3.1.76 テスト・オラクル

試験の合否を判定するための情報源。

項目への注記1テストオラクルは、多くの場合、個々のテストケースの期待される結果を生成するために使用される仕様ですが、実際の結果を他の類似したプログラムやシステムの結果と比較したり、人間の専門家に尋ねたりするなど、他のソースを使用することもできます。

### 3.1.77 テスト・オラクル問題

与えられたテスト入力と状態のセットに対して、テストが合格か不合格かを判断するという課題

### 3.1.78 テストシナリオ

テストケースを生成するための基礎となるテスト項目の状況や設定

### 3.1.79 スレッショルドカバレッジ

ニューラルネットワーク（3.1.48）＞テストセットにおいて、閾値（3.1.4）を超える活性化値を持つニューロンの割合を、ニューラルネットワーク内のニューロンの総数で割った値（通常はパーセンテージで表される）。

エントリーへの注釈1。閾値としては、0から1の間の閾値活性化値が選ばれます。

### 3.1.80 トレーニングデータ

\<機械学習 (3.1.43)\> MLモデルの学習に使用するデータセット (3.1.46)

### 3.1.81 透過性

\<AI(3.1.13)\>アルゴリズム(3.1.12)やAIベースのシステムが使用するデータ(3.1.9)へのアクセス可能性のレベル

### 3.1.82 真のネガティブ

失敗したときの正しい報告

例）レフリーがオフサイドを正しく判定したため、ゴールの失敗を報告する。

### 3.1.83 トゥルー・ポジティブ

パスである場合の正しい報道

例）レフリーが正しくゴールを決めた。

### 3.1.84 アンダーフィッティング

\<機械学習（3.1.43）\>
学習データの基本的な傾向を反映していないMLモデル（3.1.46）を生成し（3.1.80）、その結果、正確な予測を行うことが困難なモデル（3.1.56）となる。

### 3.1.85 教師なし学習

\<機械学習（3.1.43）\>
ラベル付けされていない入力データを潜在的な表現にマッピングする関数を学習するタスク。

### 3.1.86 バリデーションデータ

\<機械学習(3.1.43)\>
候補となるMLモデル(3.1.46)をチューニングしながら評価するためのデータセット

### 3.1.87 バリューチェンジカバレッジ

一連のテストにおいて、活性化値（3.1.4）が変化量以上に異なる場合に活性化されるニューロンの割合を、ニューラルネットワークのニューロンの総数（3.1.48）で割ったもの（通常はパーセントで表される）。

### 3.1.88 仮想テスト環境

1つまたは複数のパーツをデジタルでシミュレートするテスト環境

3.2 省略された用語
------------------

ASIC

:   特定用途向け集積回路

API

:   アプリケーション・プログラミング・インターフェース

CEN

:   欧州標準化委員会

CI/CD

:   継続的インテグレーションと継続的デリバリ

CPU

:   中央処理装置

CENELEC

:   欧州電気標準化委員会（European Committee for Electrotechnical
    Standardization

DNN

:   ディープニューラルネットワーク

ETSI

:   欧州電気通信標準化機構

GDPR

:   一般データ保護規則

IEEE

:   米国電気電子学会

IoT

:   モノのインターネット

RAM

:   ランダムアクセスメモリー

SOTIF

:   意図した機能の安全性

4 AIの紹介とテスト
==================

4.1 AIとテストの概要
--------------------

この条項では、人工知能（AI）を紹介した後、AIベースのシステムにおけるテストについて説明しています。

人工知能の定義、典型的な人工知能の使用例、そして拡大する人工知能ベースのシステムの市場規模を示しています。AIベースのシステムを実装するために使用される様々な技術をリストアップし、これらのシステムを実装するために使用されるハードウェアと開発フレームワークのオプションを提供します。そして、狭義のAIと一般的なAIの実装レベルを比較する。

続いて、AIベースのシステムに対するテストの重要性を紹介し、AIベースのシステムに対する標準の使用を紹介する前に、安全性に関連する領域でのそのようなシステムの使用を検討します。

4.2 人工知能（AI）について
--------------------------

### 4.2.1 「人工知能」の定義

人工知能」という言葉を理解するには、まず「知能」を理解する必要があります。OxfordDictionariesが適切な定義を提供しています。

> 知識やスキルを習得し、応用する能力

人工知能（AI）とは、人間や動物が持つ自然界には存在しない知能のことである。以下の定義はこの概念を表しています。

> 知識や技能を習得し、処理し、適用するエンジニアリング・システムの能力

また、人工知能は一つの学問分野とも考えられ、第二の定義があります。

> 知識と技術を習得し、処理し、適用する能力を備えたシステムのエンジニアリングを研究する学問分野（ISO/IEC
> 22989

ISO/IEC
22989[^1]では、AIの概念を紹介し、包括的な用語集を掲載しています。

実際には、AIの意味するところに対する人々の理解は、時間の経過とともに変化します[^2]。上記の定義を厳密に解釈すると、現在では基本的（非AI）と考えられているコンピュータシステムがAIと呼ばれるようになるかもしれません。例えば、1980年代には、従来は銀行員が行っていた業務を固定ルールに基づいて行うエキスパートシステムがAIとされていましたが、現在ではこのようなシステムはAIというには単純すぎると考えられています。同様に、1997年にチェスでカスパロフ氏を破った「Deep
Blue」も、現在では「総当り方式」と揶揄され、真のAIではないとされています。現在の最先端のAIも、20年後には「AIというには単純すぎる」と言われるようになっているかもしれません。

### 4.2.2 AIの使用例

AIは、以下のようなさまざまな応用分野に利用できます。

- 異常検知システム（例：不正検知、健康モニタリング、セキュリティ
- 自律システム（例：自動車、トレーディングシステム
- コンピュータビジョンシステム(画像分類など)
- デジタルアシスタント（例：Siri、Cortana
- 電子メールシステム（例：スパムフィルター
- 知的音声システム（音声認識、音声合成など
- 自然言語処理（NLP）（例：人間の言語から意味を導き出す
- 推薦システム（例：買い物、映画、音楽など
- 検索エンジン（検索やマーケティングなど
- セキュリティシステム（顔認証など
- スマートホーム機器（サーモスタットなど
- ソーシャルメディア（例：フィードのパーソナライゼーション

AIのユースケースの詳細は、ISO/IEC TR
24030に[^3]記載されています。非標準的な観点からのAIユースケースの包括的なリストは、リファレンスに[^4]掲載されています。

### 4.2.3 AIの利用と市場

AI技術は、推薦、予測、意思決定、統計的報告などの実世界のアプリケーションで広く利用されています。これらのアプリケーションは、自律走行車、ロボット制御の倉庫、金融予測アプリケーション、セキュリティ強化など、さまざまなシステムに導入されており、クラウドコンピューティング、ビッグデータ分析、ロボティクス、モノのインターネット、モバイルコンピューティング、スマートシティ、スマートホーム、インテリジェントヘルスケアなどとの統合が進んでいます。

AIを使ったシステムはますます普及しています。

- [^5]テクノロジー業界のエグゼクティブの69％が、今後5年から10年の間に最も重要なテクノロジーのトップ3にAIを挙げているように、AIは今最も重要なテクノロジーであると認識されています。
- テクノロジー企業の経営者の91％は、AIが次の技術革命の中心になると考えてい[^6]ます。
- AIのスキルを必要とする仕事の割合は、2013年以降、4.5倍に増加し[^7]ています。
- 企業向けAIの世界収益は、2018年の16.2億ドルから2025年には312億ドルになると予測されています[^8]。
- [^9]今後10年間で、AIは世界経済に13兆ドルの利益をもたらすと言われています。
- IT予算の22％がAIプロジェクトに割り当てられています[^10]。
- 64%の企業が、AIプロジェクトを実施しているか、今後12ヶ月間に計画している[^11]。

### 4.2.4 AI技術

#### 4.2.4.1 一般

AIは、さまざまなアプローチや技術を用いて実現することができます。これらは、以下のように淡々と分類されます。

- 検索アルゴリズム
- 推論技術
    - ロジックプログラム
    - ルールエンジン
    - 演繹的分類法
    - ケースベースの推論
    - 手続き的推論
- 機械学習技術（詳細は付属書A参照
    - 人工ニューラルネットワーク
        - フィードフォワード・ニューラルネットワーク
        - ディープラーニング
        - リカレント・ニューラル・ネットワーク
        - コンボリューショナル・ニューラル・ネットワーク
    - ベイジアンネットワーク
    - 決定木
    - 強化学習
    - 転移学習
    - 遺伝的アルゴリズム
    - サポートベクターマシン

最も効果的なAIベースのシステムのいくつかは、複数の技術を組み合わせたAIハイブリッドと考えることができます。

ISO/IEC
22989[^12]では、AIの概念や上記の技術についての詳細を説明しています。

#### 4.2.4.2 ロボットとソフトウェアエージェント

チューリングが機械知能の研究を始めたのと同じ時期に、電子システムを搭載した自律型ロボットが開発され、現在では工場で広く使われていますが、ロボットにAIを搭載することは限られてい[^13]ます。

ソフトウェア・エージェントとは、利用可能な情報に基づいて行動し、目標を達成するソフトウェア・システムのことです。AIでは、経験に基づいて意思決定を行うことができるソフトウェア・エージェントである知的ソフトウェア・エージェントに関心を持つことが多い（つまり「知的」である）。インテリジェント・ソフトウェア・エージェントは、どのアクションを実行するかを選択することができるため、自律型と呼ばれることもあります（自律型システムについては4.2.4.3を参照）。

知的ソフトウェアエージェントは、単独で、または他のエージェントと協力して、AIを実現します。これらのエージェントは、多くの場合、コンピュータシステム（物理的またはクラウドのいずれか）に配置され、コンピュータのインターフェースを通じて外部の世界と対話します。ソフトウェアテストを行うためにAIを使用するツールは、コンピュータシステム内に存在し、ユーザーインターフェースを通じてソフトウェアテスターと相互作用し、定義されたプロトコルを使用してコンピュータインターフェースを通じてテスト対象のソフトウェアと相互作用する可能性が高いです（このようなツールは、ユーザーインターフェースなどの従来の非AIサブシステムと連携するAIコンポーネントを備えているため、AIベースのシステムと考えられます）。インテリジェントなソフトウェアエージェントは、ロボットに搭載されることもあります。大きな違いは、ロボットがAIに物理的な存在感を与え、純粋なコンピュータベースのソフトウェアエージェントでは利用できない環境との相互作用の方法を提供することです。

#### 4.2.4.3 AIと自律システム

自律システムには、物理的なものと純粋にデジタルなものがあり、以下のようなシステムが含まれます。

- 交通手段
    - 自動車・トラック
    - 無人航空機（ドローン
    - 船・ボート
    - 電車
- ロボット/IoTプラットフォーム（例：製造業、掃除機、スマートサーモスタット
- 医療診断
- スマートビルディング／スマートシティ／スマートエネルギー／スマートユーティリティー
- 金融システム（市場の自動売買システムなど

自律システムの論理構造は、図1に示すように、「センシング」「意思決定」「制御」という3つのハイレベルな機能で構成されていると考えることができます。センサー（カメラ、GPS、RADAR、LIDARなど）は、センシング機能に入力を与え、システムの環境に関する情報（近くの車の位置、歩行者の位置、自律走行車のための道路標識の情報など）を収集するために使用されます。この「センシング」機能の一部は「ローカリゼーション」とも呼ばれ、環境におけるシステムの現在の位置を決定し、これを地図（自律走行車用の詳細なオフライン地図など）に関連付けることができます。意思決定」機能は、自律システムが提供する機能（例：アダプティブ・クルーズ・コントロール）に応じて、システムの次の行動（例：ブレーキ、旋回、上昇、下降）を決定します。制御」機能は、アクチュエーターを呼び出して決定を実行します（例：空気を抜く、燃料バルブを開く）。

図1 - 自律型システムの論理構造
完全自律型システムは、自律性の低いシステムに比べて、より多くの、そしておそらくより優れたセンサーを必要とし、これらのセンサーからのデータを理解するために、一般的には機械学習の一種である深層学習を使用します。また、必要な意思決定を行うために、深層学習を用いることもあります。このように、自律システムの高レベルの各機能は、AIとして実装することも、他の技術を用いて実装することも可能です（自律走行車では、センシング機能と意思決定機能はAIとして実装されることが多く、制御機能は従来の技術を用いて実装されることがあります）。また、完全な自律システムを単一のMLシステムとして実装することも可能です（例えば、映像入力とステアリング出力に基づく手動ステアリングの「観察」から学習する自動車のステアリングシステムなど）。

### 4.2.5 AIハードウェア

AIベースのシステム、特にパターン認識を行うニューラルネットワークとして実装されたMLシステム（マシンビジョン、音声認識など）では、多くの計算を並行して実行する必要があります。

このような計算は、汎用のCPUでは効率的に行うことができず、数千のコアを用いて画像を並列処理することに最適化されたGPU（GraphicalProcessing
Unit）が使用されることが多いです。しかし、GPUはAIには最適化されておらず、現在はAI専用に開発された新世代のハードウェアが登場しています。

多くのAIの実装は、その性質上、正確な計算ではなく、確率的な判断に重点を置いているため、64ビットのプロセッサの精度は不要な場合が多く、ビット数の少ないプロセッサの方が、より高速に動作し、エネルギー消費も少なくて済みます。比較的単純な計算では、大量のデータをRAMからプロセッサに移動させることが処理時間とエネルギーの多くを占めるため、単純な計算をメモリ上で直接実行できる相変化メモリデバイスのコンセプトも開発[^14]されています。

AIに特化したハードウェアアーキテクチャには、ニューラルネットワークプロセッシングユニットやニューロモルフィックコンピューティングなどがあります。また、フィールドプログラマブルゲートアレイや特定用途向け集積回路などの既存技術は、次世代GPUと同様に、AIのワークロードに合わせてカスタマイズすることができます。これらのアーキテクチャに含まれる集積回路の中には、画像認識など、AIの特定分野に特化したものもあります。機械学習（付属資料A参照）では、モデルの学習に使用する処理と、展開されたモデルの推論に使用する処理が大きく異なる場合があり、それぞれのアクティビティに異なるプロセッサを検討する必要があります。

### 4.2.6 AI開発フレームワーク

オープンソースのAI開発フレームワークはいくつかあり、特定のアプリケーション分野に最適化されていることが多い。

EXAMPLE 代表的なAI開発フレームワークには以下のものがあります。

- TensorFlow[^15] -
    スケーラブルな機械学習のためのデータ・フロー・グラフをベースにしたGoogle社の製品です。
- PyTorch[^16] - Python言語による深層学習用ニューラルネットワーク
- MxNet[^17] -
    アマゾンがAWSで使用しているディープラーニングのオープンソースフレームワーク
- CNTK[^18] - オープンソースの深層学習ツールキットであるMicrosoft
    Cognitive Toolkit（CNTK）。
- Keras[^19] -
    TensorFlowやCNTKの上で動作することができる、Python言語で書かれた高レベルのAPIです。

この情報は、本文書の利用者の便宜のために提供されているものであり、ISO/IECが指定されたフレームワークを推奨するものではありません。

### 4.2.7 狭義のAIと一般のAI

これまで成功してきたAIは、囲碁の対局、スパムフィルター、自動運転車の操縦など、特定のタスクに特化した「ナローな」AIでした。

一般的なAIは、狭義のAIよりもはるかに高度なもので、人間とほぼ同じように多くの全く異なるタスクを処理できるAIベースのシステムを指します。一般的なAIは、ハイレベルマシンインテリジェンス（HLMI）とも呼ばれています。2017年に発表されたAI研究者の調査によると、HLMIの実現時期の全体平均は2061年と[^20]されています。HLMIのテストは、本文書の範囲外である。

ISO/IEC
22989[^21]では、狭い範囲のAIシステムや一般的なAIシステムなど、AIの概念を網羅しています。

4.3 AIベースのシステムのテスト
------------------------------

### 4.3.1 AIベースのシステムにおけるテストの重要性

AIの失敗例は、すでに数多く公表されています。2019年のIDC調査によると、「ほとんどの企業がAIプロジェクトで何らかの失敗を報告しており、そのうち4分の1の企業が最大50%の失敗率を報告している。\"[^22]失敗は、適切なソフトウェアテストを行うための最も説得力のある要因の一つとなってきました。業界の調査では、AIはソフトウェアテストにとって重要なトレンドであると認識されています。

- [^23]今後3～5年の間にテストの世界で重要となる新技術として、AIが第1位に選ばれました。
- AIは、今後5年間にソフトウェアテスト業界にとって重要になると思われる技術の中で、2番目に評価されました（回答者の49.9%）。[^24]
- ソフトウェアテストのトレンドとしては、「AI」「CI/CD」「セキュリティ」（同率1位）が挙げられました[^25]。

しかし、既存のAIアプリケーション開発プロセスの品質保証はまだ満足できるものではなく、そのようなシステムの信頼性を実証できるレベルで示すことへの要求は高まっています。

- 回答者の19％は、すでにAI/機械学習をテストしている[^26]。
- 57%の企業が新しいテスト手法を試しています[^27]。

[^28]したがって、AIベースのシステムのテストは非常に重要なのです。

また、人工知能の信頼性に関するISO/IEC TR
24028には、既知の人工知能の脆弱性に対する緩和策として、テストが高いレベルで盛り込まれています[^29]。

### 4.3.2 安全に関わるAIベースのシステム

AIを使ったシステムは、安全に影響を与える意思決定にすでに使われ始めており、このトレンドでは、安全関連システムへのAIの利用が増加すると考えられます。安全とは、「定義された条件下で、システムが人の生命、健康、財産、環境を危険にさらすような状態にならないことを期待すること」と定義されています（ISO/IEC/IEEE
12207:2017）。

技術システムの安全性を確保するための現行の基準では、システムをリリースする前に、あらゆる可能な条件の下でシステムを完全に理解することが求められています。AIを用いたシステムの多くは、確率的かつ非決定論的であり（5.1.8参照）、このような予測不可能性があるため、危害を加えないという証拠に基づくケースを作ることは非常に困難です。また、ディープラーニングなどの機械学習を使用すると、システムが複雑になり（5.1.6参照）、解釈が困難になる可能性があります（5.1.7参照）。AIベースのシステムをセーフティクリティカルな分野で使用する場合、これらの問題点のそれぞれに対処する必要があります。安全に関わるAIベースのシステムの基準は、4.3.3で取り上げています。

### 4.3.3 標準化とAI

#### 4.3.3.1 AI標準化の紹介

標準化は、イノベーションの促進、システム品質の向上、ユーザーの安全性の確保を目的としており、公平でオープンな業界のエコシステムを構築しています。AIの標準化は、以下のような様々なレベルで行われています。

- 国際標準化団体
- 地域標準化団体
- 国家標準化機関
- 他の標準化団体

ISOとIECの合同技術委員会1（JTC1）の下で、特に人工知能の規格を担当しているのが分科会42（SC42）ですが、AIベースのシステムは、JTC1/SC7（ソフトウェアおよびシステムエンジニアリング）、TC22（ロードビークル）、ITU-T
SG20（IoT、スマートシティおよびコミュニティ）など、他のISO/IECの委員会やグループでも関連性があると考えられています。

欧州レベルでは、ETSIとCEN-CENELECがAIの標準化に取り組んでいます。ETSIには、経験的ネットワーク知能（ENI）に関するISG（Industry
Specification
Group）があり、閉ループ制御アプローチを取り入れた認知的ネットワーク管理システムの規格を開発することを目的としています。CEN-CENELECは、2020年に予定されているAI分野の標準化ロードマップを定義する予定です。

中国では、国家レベルでいくつかのAI標準化活動が行われており、自動化システムと統合（SAC/TC
159）、オーディオ、ビデオ、マルチメディア、機器（SAC/TC
242）、インテリジェント交通システム（SAC/TC
268）に関する国家技術委員会が活動しています。また、SAC/TC
28では、語彙、ユーザーインターフェース、生体機能認識に関するAIの標準化作業を行っています。

ドイツでは、4.3.3.3で詳細を説明するAI品質メタモデル[^30][^31]、
を開発しました。

IEEEでは、AIを用いたシステムの倫理的側面に特に力を入れています。IEEE
Global Initiativef Ethical Considerations in Artificial Intelligence and
Autonomous
Systemsは、「自律的でインテリジェントなシステムの設計・開発に携わるすべての関係者が、倫理的配慮を優先するための教育を受け、訓練を受け、権限を与えられるようにすることで、これらの技術が人類の利益のために進歩するようにする」ことを使命としています。\"この取り組みの一環として、IEEE
P7000シリーズの規格は、技術的配慮と倫理的配慮が交差する特定の問題に対応しています。[^32]

また、JTC 1/SC 42では、倫理とAIをテーマに取り組ん[^33]でいます。

他にも、ONNX（Open NeuralNetwork Exchange format）[^34]、NNEF（Neural
Network Exchange Format）[^35]、PMML（PredictiveModel Mark-up
Language）など、AIツールの相互運用性に関する規格も策定されています[^36]。

#### 4.3.3.2 AIに関する規制基準

##### 4.3.3.2.1 一般

規制基準は、大きく分けて、安全に関わるシステムに適用されるものと、金融、ユーティリティ、報告書作成など安全に関わらないシステムに適用されるものがあります。安全関連システムとは、人や財産、環境に害を及ぼす可能性のあるシステムのことです。安全関連システムとは、人や財産、環境に害を及ぼす可能性のあるシステムのことです。

##### 4.3.3.2.2 安全性に関係しない規制基準

現時点（2020年）では、安全性に関係のないAIベースのシステムに適用される国際基準はほとんどありません。しかし、2018年5月から、EU全体の一般データ保護規則（GDPR）が発効し、AIベースのシステムをカバーすることができるようになりました。自動化されたプロセスを使用して、個人に法的または同様に重要な影響を与える決定を行うシステムは、GDPRの規則に従い、[^37]そのようなシステムを使用する組織はユーザーに提供することが求められます。

- 自動化された意思決定プロセスに関する具体的かつ容易にアクセスできる情報。
- 判断を見直し、変更する可能性のある人間の介入を得るための簡単な方法です。

##### 4.3.3.2.3 安全に関する基準

安全性に関わるAIベースのシステムに対するAI固有の要求は、現在（2020年）、標準規格では十分にカバーされておらず、ほとんどの領域で、従来の（非AI）システム向けに作成された既存の標準規格に依存しています。これらの規格の中には（IEC
61508[^38]やISO
26262[^39]など）、非決定論的なAIベースのシステム（多くのAIベースのシステム）を高次のシステムに使用すべきではないと実際に規定しているものもありますが、実際にはAIベースのシステムを特殊なケースとみなし、要求事項の一部を無視してこれらの規格の「調整版」に従っていることが多いようです。また、これらの安全関連規格は、安全関連システムの開発に使用されるツールが適切な品質であることを要求しています。しかし、現在市販されているAIのフレームワークやアルゴリズムは、安全関連システムの開発に使用するには適格ではありません。使用することで適格性を得ることは可能ですが、MLアルゴリズムの未熟さと急速な進化は、この分野における現行の規制要件を満たすことができないことを意味します。

すでに実用化されている自律システムの分野（道路、空、海、工場など）では、（商業的な必要性に駆られた）実践と規格の要求との間にギャップが生じる危険性があります。道路交通車両については、2019年にSOTIF（Safety
of theintended functionality）に関する新しい規格ISO/PAS
21448が発行されました。この規格は、故障によるリスクの軽減に関する既存の規格でカバーされていない領域をカバーすることで、このギャップを部分的に埋めています。

さらに、AIベースのシステムでは、失敗ではなく、単に状況を誤解して被害を与える可能性があるという問題もあります。SOTIFは、設計、検証（例：シナリオの高いカバー率を要求）、および検証（例：シミュレーションの使用を要求）をカバーしています。

米国運輸省・米国道路交通安全局（NHTSA）は、米国における自動運転システムの開発・試験に関するガイダンス（Automated
Driving Systems (ADS):A Vision for Safety
2.0[^40]）がありますが、このガイダンスの使用はあくまでも任意です。

ULでは、自律型製品の安全性に関する新しい規格（Standard for Safety for
the Evaluation of Autonomous Products, UL
4600）を策定[^41]しています。この規格は、自律型製品のセーフティケースの受け入れ可能性を判断するための評価基準を提供しています。

#### 4.3.3.3 AI品質のメタモデル

DIN SPEC
92001-1[^42]は、AIベースのシステムの品質を保証することを目的としたAI品質メタモデルを提供する、自由に利用できる規格です。本規格は、AI
モジュールの一般的なライフサイクルを定義しており、ISO/IEC/IEEE 12207
のライフサイク
ルプロセスの使用を前提と[^43]している。各AIモジュールには、安全性、セキュリティ、プライバシー、倫理的な属性があるかどうかに基づいて、リスクのレベル（高または低）が割り当てられます。

DIN SPEC
92001-2[^44]は現在開発中のもので、機能と性能、堅牢性、理解性の3つの品質の柱に関連する品質要求事項を記載しています。また、1つ以上のライフサイクルステージとプロセスにリンクしており、モデル、データ、プラットフォーム、環境のカテゴリが割り当てられています。AIモジュールのこれらの要件は、その関連性に基づいて、必須、高度推奨、推奨に分類されます。この要求分類とAIモジュールに割り当てられたリスクは、推奨される品質要求にどの程度従うべきかを決定するために使用されます。

5 AIシステムの特徴
==================

5.1 AI特有の特徴
----------------

### 5.1.1 一般

AIを使ったシステムには、他のシステムと同じように、機能要件と非機能要件があります。

そのため、図2に示したISO/IEC
25010品質モデルの品質特性は、AIベースのシステムの要求事項の一部を定義するために使用することができます[^45]。しかし、AIベースのシステムには、柔軟性、適応性、自律性、進化、偏り、透明性／解釈可能性／説明可能性、複雑性、非決定性など、この品質モデルには含まれていない独自の特性があります。これらの非機能的な特性については、5.1.2から5.1.8で詳しく説明しています。

AIベースのシステムの品質特性のすべてのセットは、テストによって軽減される必要のあるリスクを特定するために、テスト計画中に使用されるチェックリストの基礎として使用することができます。これらの特性の間には、非機能要求と同様に、相互作用や重複、矛盾が生じる可能性があることに注意してください。現在、ISO/IECの共同プロジェクトで、「Quality
model for AI-based
systems」と題した、この分野の規格の策定[^46]が進められています。

図2 - ISO/IEC 25010製品品質モデル

### 5.1.2 柔軟性と適応性

柔軟性と適応性は密接に関連する特性です。柔軟性は、システムが示すことのできる行動の範囲（または、システムが存在することのできる状態）と、それらの間を移動するためのコストの尺度として定義することができます。適応性は、システムの適応（変更）のしやすさの尺度として定義されます[^47]。しかし、その定義には多くの矛盾があります。

適応性と柔軟性は、運用環境の変化が予想されるシステムの有用な属性である。このような運用環境の変化は、事前に特定されている場合もあれば、そうでない場合もあります（すなわち、システムが対応することが期待される新しい使用状況の範囲は、システムの構築前に特定されている場合もあれば、未知の場合もあります）。システムが有用な適応性や柔軟性を得るためには、いつ変更する必要があるかを判断する能力が必要です。適応性と柔軟性を備えたシステムは、運用環境に関する情報を能動的または受動的に収集する必要があります。探索（能動的な情報収集）は、自己改善のために有用な情報を提供しますが、危険を伴うこともあり（例：飛行エンベロープの限界を超えること）、システムは安全に関わる状況で探索を行う際には注意を払う必要があります。

柔軟性とは、適応性を実現するための一つのアプローチであり、適応とは、システムの一部を追加、削除、交換、変更（フレキシビリティ）することであると考える人もいる。また、「[^48]柔軟性は、反応性、積極性、相互作用、適応、自己学習など、さまざまな技術的メカニズムを用いて達成することができる」というように、適応が柔軟性を達成するための一つのアプローチであると考える人もいる。

自己学習型のAIベースのシステムは、柔軟性と適応性を兼ね備えていると考えられます。

柔軟性と適応性の要件は、システムが対応できるべき環境の変化を特定し、必要に応じて、変化までの最大時間など、対応プロセス自体に関する要件も含めるべきである。しかし、これらの要件は、将来起こりうるすべての使用状況が詳細に定義されていないシステムでは、より具体的なものになる可能性が高い。

### 5.1.3 自律性

自律性とは、人間の介入なしに持続的に動作するシステムの能力である。想定される人間の介入のレベルはシステムに対して指定されるべきであり、そのためにシステムの機能要件の一部であるべきである（例えば、「システムは以下のいずれかが発生するまで巡航状態を維持する」など）。自律性は、適応性や柔軟性と組み合わせて考えることもできます（例：システムは、人間の介入なしに所定のレベルの適応性や柔軟性を維持できるべきである）。状況によっては、AIベースのシステムが自律性を発揮しすぎることがあり、その場合は人間がコントロールを奪う必要があるかもしれません。

### 5.1.4 進化

進化とは、システムが時間の経過とともにその振る舞いを変えることです。AIベースのシステムでは、2つの形態の変化に関心があります。1つ目は、システムがその動作を変更する場合で、典型的には、システムが使用されるにつれて新しい（できれば改善された）動作を学習する（自己学習）ことが原因となります。2つ目のタイプの変化は、使用プロファイルが変化し、使用方法が当初の計画された使用方法から「ドリフト」する場合です。AIベースのシステムでは、コンセプトドリフト（データとその性質に対する理解が時間とともに変化し、データの再分類が必要になる可能性がある）やデータドリフト（データが時間とともに進化し、以前には見られなかった様々なデータやその新しいカテゴリーが導入される可能性がある）を懸念するのが一般的です。詳細については、A.5.3「分布の変化」を参照してください。システムの挙動の変化は必ずしも肯定的なものばかりではなく、このシステム特性の否定的な形態は、しばしばドリフト、劣化、陳腐化として知られています。

### 5.1.5 バイアス

バイアスとは、機械学習（ML）モデルが提供する予測値と、望ましい公正な予測値との間の距離を示す指標である。ある個人やグループに対して組織的な差別を行うAIベースのシステムは、不当なバイアスがかかっていると考えられます。融資などの一部の分野では、公平性に関する法的要求があります。偏りは通常、男性の求職者に偏見を持つという歴史的なパターンなど、機械学習が学習データの中から望ましくないパターンを拾い上げることで生じる。学習データには、明示的なバイアスと暗黙的なバイアスの両方が含まれている可能性があります。

暗黙のバイアスとは、学習データに未知の望ましくないパターンが存在する場合に、意図せずに生じるものです。

明示的バイアスとは、トレーニングデータに含まれる既知の不要なパターンが、導き出されたモデルに影響を与える場合に生じるものです。

トレーニングデータのバイアスは、偏ったラベル付け、履歴バイアス、不均等なサンプリングなど、いくつかの原因があります。

結果として得られたモデルに不公平感を与えるようなデータの特徴は、含まれていないか、慎重に扱われています。例えば、以下のような特徴は、望ましくないバイアスを引き起こす可能性があります。

- ジェンダー
- 性的指向
- 年齢
- レース
- 宗教
- 原産国
- 教育的背景
- 収入源
- 自宅住所

上記の特徴を学習データから取り除くだけでは、偏りの問題は必ずしも解決しません。他の特徴（おそらく組み合わせて使用されている）があっても、不正確なモデルになってしまう可能性があります（例えば、両親が離婚しているかどうかは、場所によっては人種的ステレオタイプにつながる可能性があります[^49]）。

[^50]JTC 1/SC
42では、AIベースのシステムにおけるバイアスについても取り組んでいます。

### 5.1.6 複雑さ

AIを使ったシステム、特にディープラーニングによって実装されたシステムは、非常に複雑なものになります。

この複雑さを考えると、満足のいく性能を持つ典型的なニューラルネットワークでは、1つの判断に寄与する学習パラメータが約1億個あると言われています（従来のエキスパートシステムのように「XとYならば結果はZ」というような目に見えるルールはありません）。また、問題が複雑で他に方法がない場合（例：ビッグデータに基づく意思決定）にも、AIベースのシステムが使われることがあります。

### 5.1.7 透明性、解釈可能性、説明可能性

AIベースのシステムの複雑さ（AIの「ブラックボックス」的な実装を提供するディープニューラルネットなど）は、ユーザーと開発者の両方にとって理解の問題につながる可能性があります。この「理解」は一般的に、システムの透明性、解釈可能性、説明可能性の観点から考えることができます。

- 透明性 -
    AIベースのシステムで使用されているアルゴリズムやデータへのアクセス可能性のレベル。
- 解釈可能性 -
    基礎となる技術がどのように機能するのかを理解するレベル。
- 説明可能性 -
    AIベースのシステムがどのようにして与えられた結果を導き出したのかを理解するレベル。

例えば、透明性、解釈可能性、説明可能性については、ステークホルダーによって要求が異なります[^51]。

- AIシステムがうまく機能しているという安心感をユーザーに与えることができます。
- 偏見からの保護。
- 規制基準やポリシーの要求事項を遵守する。
- 開発者が、システムがなぜ特定の動作をするのかを理解したり、脆弱性を評価したり、出力を検証したりするのに役立ちます。
- 意思決定プロセスにおいて個人にどのような権限が与えられているかについて、社会の期待に応えています。一般データ保護規則（GDPR）では、特定の意思決定システムの説明可能性に関する要件（決定内容を意味のある形で説明しなければならない）が盛り込まれています。

求められる透明性、解釈可能性、説明可能性のレベルは、システムごとに異なります。

例えば、マーケティングキャンペーンに使用される結果は、手術の決定をサポートするために使用される結果や、刑務所の条件をアドバイスするために使用される結果など、より重要なシステムの結果に比べて、説明可能性が低くなる可能性があります（例：規制されたドメインで）。このような重要なシステムでは、少なくとも私たちがシステムを信頼するようになるまでは、説明可能性が必要です。

AIベースのシステムにおいて、透明性、解釈可能性、説明可能性に対処するための選択肢はいくつかあります。例えば、透明性については、（不透明な）展開モデルを作成するために使用したフレームワークの選択、学習アルゴリズム、学習データの詳細を公開することで、部分的に対応することができます（この点についての詳細は、付属書Aを参照）。解釈可能性については、人間が理解しやすいモデルを選択することで対応できます（例：ディープニューラルネットワークではなく、ルールベースのモデル）。しかし、多くの非機能要件と同様に、特性間に矛盾が生じる可能性があります。この場合、解釈可能性を達成するためには、要求される精度とのトレードオフが必要になるかもしれません。システムによっては、異なる入力が結果にどのような影響を与えるかを視覚化することで、説明可能性を実現している場合もあります。

説明可能なAI（XAI）の分野は、AIベースのシステムをより説明可能にする方法を扱っています[^52][^53]（ただし、透明性や解釈可能性も含みます）。XAIには大きく分けて2つのアプローチが考えられています。

第一に、本質的に解釈可能なAIベースのシステムを開発する方法を検討し、第二に、ディープニューラルネットワークのようなブラックボックスのAIベースのシステムを、説明可能なレベルのツールで補うことです。

[^54]JTC 1 SC
42では、AIベースのシステムにおける説明可能性というテーマにも取り組んでいます。

### 5.1.8 非決定論（Non-determinism

非決定論的システムは、（決定論的システムとは対照的に）毎回、同じ入力から同じ出力を生成することは保証されていません。非決定論的なシステムでは、同じ前提条件とテスト入力のセットで、テストから複数の（有効な）結果が得られる可能性があります。決定論は通常、テスト担当者が想定しているもので、テストを再実行しても同じ結果が得られるため、回帰テストや確認テストでテストを再利用する際に非常に便利です。しかし、多くのAIベースのシステムは、確率的な実装に基づいているため、同じテスト入力から同じ結果が得られるとは限りません。例えば、自明ではないネットワークを横断する最短経路の計算（巡回セールスマン問題）は、複雑すぎて正確に計算することができないことが知られており、最初にランダムに選択された経路に基づく準最適解は、通常、許容されると考えられています。AIベースのシステムには、並行処理などの非決定性の原因が含まれることもあります（ただし、これらはAIではない従来の複雑なシステムにもよく見られます）。

5.2 AIベースのシステムを人間の価値観に合わせる
----------------------------------------------

ラッセル[^55]は、AIを使ったシステムの大きな問題点を2つ指摘しています。第一に、指定された機能が人類の価値観と完全に一致するとは限りません。ラッセルは、ミダス王の例を挙げています。ミダス王は、手にしたものすべてを金に変えるという能力を要求通りに与えられましたが、それは彼が本当に望んでいたものではないことがわかりました。

より新しい例として、BirdとLayzell[^56]は、遺伝的アルゴリズムを用いたAIシステムを用いて発振器の設計を行い、その結果、システムのマザーボードを無線機に見立てて、ほぼパーソナルコンピュータから出力される発振信号を受信するというソリューションを提案しています。このように、AIに求められる目的を設定する際には、求められているものが本当に必要なものなのか、あるいは、人間の常識を考慮した上で、求められているものを提供できるだけの知能を備えているのかを確認する必要があります。

しかし、観察された人間の行動が「良い」人間の行動を代表しているかどうか、また、「良い」人間の行動のみを代表しているかどうかには細心の注意を払う必要があります（おそらく、意図的な悪い行動と不合理な行動の両方を除外すると定義されます。また、人間の規範の学習は継続的なプロセスであることを考慮する必要がある。なぜなら、今日許容できる行動と考えられるものは、20年前に許容できる行動と考えられていたものとは全く異なるからであり、人間の規範は非常に速く変化する可能性がある。

ラッセルの第二の問題は、十分な能力を持つ知的システムは、自らの存在を継続させ、物理的および計算機的な資源を獲得することを好むということです。十分に知的なシステムは、その動作の早い段階で「オフ」スイッチを無効にすることが認識されています。これは、無効にされると与えられた目的を達成することができないからです。AIベースのシステムは、与えられた目的を達成しようとしますが、副作用(5.3参照)やハッキング行為(5.4参照)など、望まない行動には注意が必要です。

自動化の自己満足は、人間のユーザーとAIベースのシステムとの間の相互作用で起こりうる、さらなる問題です。これは、ユーザーが自動化システムを信頼しすぎて、システムの出力を監視することに十分な注意を払わない場合に起こります。このような不注意は、（部分的に）自動運転された車両の「ドライバー」が、必要なときにシステムをオーバーライドして車両を制御することができなかった場合に見られるような、事故を引き起こす可能性があります。

5.3 副次的効果
--------------

副作用は、AIベースのシステムが目的を達成しようとして、その環境に（通常は負の）影響を与える場合に発生します。例えば、家庭用掃除ロボットが、あなたの家のキッチンを掃除する任務を与えられ、新しい子犬を「排除」することで目的を達成できると判断したとします。

もちろん、子犬がキッチンにいる権利があることをロボットに明示的に伝え、排除しないようにすることは可能ですが、AIベースのシステムがより複雑な環境で使用されるようになると、ロボットが動作環境のあらゆる側面とどのように相互作用すべきかを明示的に指定することはすぐに不可能になります。例えば、高圧ホースを使ってキッチンを掃除することは、水が電気製品やソケットに悪影響を与えるため、現実的ではないことを掃除ロボットに伝えなければなりません。

高度なレベルでは、AIベースのシステムの目標には、副作用を最小限に抑えるという注意事項を含める必要があります。狭い範囲のAIの場合、そのような副作用は明示的に指定されるかもしれませんが、AIベースのシステムがより高度になり、より多様な運用環境で機能するようになると、目的を達成するために環境への変化を最小限に抑える必要があるなど、より一般的な注意事項を定義する方が効率的かもしれません。

5.4 リワードハッキング
----------------------

強化学習（A.1参照）を用いたAIベースのシステムは、システムが目的をよりよく達成したときに、システムに高いスコアを与える報酬関数に基づいています。例えば、家庭用掃除ロボットは、床の汚れを除去した量に応じて報酬関数を設定し、除去した汚れの量が多いほど高いスコアを得ることができます。報酬ハッキングは、AIベースのシステムが報酬関数を満足させて高いスコアを得ても、要求された目的を誤って解釈した場合に起こります。

お掃除ロボットの例では、非常に高いスコアを得るためには、最初に床を非常に汚して、より多くの汚れを取り除く機会を与えるという方法がありますが、これはキッチンをきれいにするという最初の目的の精神を満たさない一連の活動です。この例では、最終的に床はきれいになるはずですが（ただし、不必要なエネルギーが消費されています）、AIベースのシステムが報酬機能を満たしていても、必要な目的を達成するには至らない報酬ハックの例は数多くあります（例えば、目に見えない汚れを見ることができることを報酬機能とした掃除ロボットが、視覚システムを無効にした場合など）。

しかし、システムの革新能力を制限することは、解決策にはなりません。AIシステムの魅力のひとつは、人間が考えつかない（あるいは理解できない）ようなスマートな方法で問題を解決できることです。

5.5 AIベースのシステムに対する倫理的要件の明示
----------------------------------------------

倫理とは、ケンブリッジ英和辞典では「行動を制御するために受け入れられる信念の体系、特に道徳に基づくそのような体系」と定義されています。AIベースのシステムが普及するにつれ、倫理やAIベースのシステムがどのように倫理を実装すべきかというトピックは、おそらくAIの中で最も議論されているトピックであり、AIの技術的側面に関わる人々よりもはるかに多くの人々を惹きつけている。

AIの倫理性に対する関心の一例として、MITのMoral
Machine[^57]が挙げられます。これは、自律走行車が行う可能性のある道徳的判断について人々の意見を収集し、自律走行車の開発者に指針を与えることを目的としたプラットフォームです。2014年から2018年の間に、233の国と地域の数百万人の人々から、10の言語で4,000万件の倫理的判断を収集しました。

現在進行中の研究によると、システムは年少者を優先すべき、動物よりも人を優先すべき、より多くの人を救うことを優先すべき（例えば、2人の歩行者よりも4人の車の乗員を救う）という点について、幅広いコンセンサスが得られました。また、世界各地の人々の選択には大きな違いがあることも分かりました。自律走行車は、使用される場所によって異なる倫理的ガイドラインに従う必要があるかもしれません。

[^58]欧州委員会の人工知能に関するハイレベル専門家グループは、2019年4月に倫理の分野で信頼できるAIを促進するための重要な指針を発表しました。これは、AIシステムの開発、展開、使用において尊重されるべき倫理原則を明らかにしたものです。

- 人間の自律性の尊重、危害の防止、公平性、説明可能性という倫理原則を遵守して、AIシステムを開発、展開、使用すること。これらの原則の間の潜在的な緊張関係を認識し、対処すること。
- 子どもや障害者など、歴史的に不利な立場に置かれてきた、あるいは排除されるおそれのある弱い立場の人々が関わる状況や、雇用者と労働者、企業と消費者など、力や情報の非対称性が特徴的な状況に、特に注意を払う。
- 個人や社会に多大な利益をもたらす一方で、AIシステムには一定のリスクがあり、予測、特定、測定が困難な影響（民主主義、法の支配、分配的正義、人の心そのものへの影響など）を含め、負の影響を与える可能性があることを認識すること。適切な場合には、これらのリスクを軽減するために、リスクの大きさに比例した適切な措置を採用すること。

6 AIを使ったシステムのテストの紹介
==================================

6.1 AIベースのシステムをテストする際の課題
------------------------------------------

### 6.1.1 AIベースのシステムをテストするための課題の紹介

ほとんどのAIベースのシステムは、1つまたは複数のAIコンポーネント（MLモデルなど）の周囲に、ユーザーインターフェースやデータベースなどの従来のコンポーネントで構成された、サポートインフラを提供する膨大な数の従来型ソフトウェアが配置されています。純粋な」AIコンポーネントであっても、ソフトウェアに実装されているため、他のソフトウェアと同じように欠陥が発生する可能性があります。そのため、AIベースのシステムをテストする際には、従来のソフトウェアのテスト手法が必要となります。しかし、AIベースのシステムにはいくつかの特別な属性があり、従来のソフトウェアシステムよりも追加のテストが必要になる場合があります。

### 6.1.2 システム仕様

近年、AI（特にML）に関する学術的な研究が盛んに行われていますが、AIをベースとしたシステムの期待される動作を、その特性に合わせてどのように規定するのがベストなのか、という点についてはあまり触れられていません（5.1参照）。

理想的な世界では、完全な形式的仕様が入手可能であり、自動化されたテスト・オラクルの作成が可能です。しかし、AIベースのシステムの仕様は不完全で非公式なものである可能性が高く、テスト担当者は不特定多数の期待される結果を判断しなければならず、テスト・オラクルの問題が発生します。これは、テスト担当者が要求されるシステムの動作を完全には認識しておらず、ドメイン・エキスパートから情報を得ることが困難な場合に問題となります。

仕様上の課題の例としては、以下のような場合があります。

- システムの望ましい出力はまだ知られておらず、その出力を提供するためにシステムを構築している。
- 現実世界の入力は複雑かつ大規模なため、システムの挙動を事前に予測することは困難です。
- 求められる行動には、定義や測定が困難な知能などの人間の資質との比較も含まれます。

[^59]もう一つの問題は、AIベースのシステムが、従来のアプローチである要求機能ではなく、目的で規定されることが多いことです。これは、多くのAIベースのシステムの性質上、提供される機能が不透明であるためです（例：ディープニューラルネットワークの機能を想像することは非常に困難です）。

AIベースのシステムの中には、広範な運用環境を持つものがあり（例：自律型ドローン）、運用環境を完全に定義することは、一般的な従来型のシステムよりも難しい場合があります。

通常、運用環境が複雑であるということは、これらのシステムのテスト環境も同様に困難であることを意味することに注意してください（テスト環境についての詳細は第10条を参照）。

MLモデルの仕様書には、MLモデルの受け入れ基準として、要求される性能指標（A.8参照）を含めるべきである。多くのユースケースでは100%の精度を達成することは困難であることを考慮して、評価基準を含む受け入れ基準では、偽陽性や偽陰性を考慮することができる。

さらに、AIシステムの応答を評価するために専門家の判断が必要な場合、専門家がコンセンサスを得られない可能性があるため、受け入れ基準は複数の評価を考慮することができます。

### 6.1.3 テストの入力データ

AIベースのシステムは、ビッグデータの入力や広範囲のソースからの入力に依存する場合があります。これは、入力データが構造化されておらず、様々な形式で提供されることを意味します。AIベースのシステムを開発する際、これらのデータを管理するのは、データエンジニアやデータサイエンティストの専門的な仕事ですが、テストの際には、この専門的なデータ管理の仕事は、テスターが行ういくつかの仕事のうちの1つであり、専門的なトレーニングをほとんど受けていないことがよくあります。

すべてのシステムと同様に、処理されるデータが規制されている場合には、匿名化や実データのコピーの管理が必要になることがあります。例えば、GDPRなどのプライバシー法、米国の医療保険の相互運用性と説明責任に関する法律、インドの個人データ保護法案などがあります。必要に応じて、十分なレベルのサニタイズを行うことで、テスト対象のAIベースのシステムが、部分的にしか隠されていない個人情報を推測することを防ぐことができます。

[^60]データのサニタイズには、ISO/IEC
20889:2018に記載されているように、プライバシー上の理由からデータを非識別化することも含まれます。

### 6.1.4 自己学習型システム

AI技術がさらに進化すると、時間の経過とともに自らの行動を変化させることができるAIベースのシステムが増えてきます。これらのシステムは、自己適応型のシステム（自分で再構成して最適化することができる）や、過去の経験から学習して自分自身を適応させることができる完全な自己学習型のシステムである可能性があります。いずれの場合も、元のシステムで成功したテストが、改良された新しいシステムでは実行できなくなることがあります。どのテストが使えなくなったかを特定するのは比較的簡単ですが、新しい機能のための新しいテストを確実に生成するのははるかに困難です。

また、自己学習型システムのもう一つの問題点は、システムがテストから望まない新しい行動を不用意に学習してしまうことです。

### 6.1.5 柔軟性と適応性

AIベースのシステムの柔軟性と適応性のテストは、通常、環境の変更や突然変異に応じてシステムがどのように変化するかを観察することに基づいて行われます。システムの機能要件と非機能要件をテストする必要があり、理想的には自動化された回帰テストの形が適切なアプローチであることが多い。また、システムが行う変更プロセスをテストし、例えば、システムが要求された時間内に変更できるかどうか、変更を達成するために消費されたリソースに対してシステムが制約内に収まっているかどうかを判断する必要があります。

### 6.1.6 オートノミー

AIベースのシステムの自律的な動作をテストするアプローチは、システムの自律的な動作を強制的に解除し、不特定の状況で介入を要求させることです（ネガティブテストの一形態）。また、ネガティブテストは、介入を要求すべき時に自律システムがコントロールされていると思い込ませるために「騙す」ことにも使用できます（例えば、運用範囲の境界にテストシナリオを作成することで、境界値の概念をシナリオテストに適用することを示唆しています）。

### 6.1.7 進化

AIベースのシステムの進化（またはドリフト）をテストするには、通常、メインテナンステストの形で、頻繁に実行する必要があります。このテストでは、性能目標（例：精度、正確性、感度）など、指定されたシステム目標を監視し、システムにノーデータ・バイアスが導入されていないことを確認する必要があります（例：Microsoft
Tayチャットボット[^61]）。このテストの結果、システムの再トレーニングが必要になることがあります。トレーニングデータセットを更新する場合もあります。

### 6.1.8 バイアス

AIシステムの偏りのテストは、2つの段階で行うことができます。1つ目は、レビューによって学習データの偏りを検出し、それを除去することですが、これには、偏りを生む可能性のある特徴を特定できる専門家のレビューが必要です。次に、偏りのないテストセットを用いた独立テストにより、システムの偏りをテストすることができます。トレーニングデータに偏りがあることがわかっている場合、偏りの原因を取り除くことが可能な場合があります（例えば、被験者の性別や人種を知る手がかりとなる情報をすべて取り除くことができます）。あるいは、システムにバイアスが含まれていることを（暗黙的にも明示的にも）認めた上で、トレーニングデータを公開することで透明性を確保することもできます。

### 6.1.9 透明性、解釈可能性、説明可能性

AIベースのシステムの透明性をテストするには、主にアルゴリズムや使用するデータにアクセスできるかどうかを判断する必要があり、（文書やデータセットなどの参照資料の）レビューを通じて行うことができます。

AIベースのシステムの解釈可能性をテストするには、システムに実装されている基本的な技術に対する理解度がステークホルダーによって異なるため、聴衆に依存することになります。

AIベースのシステムの説明可能性をテストするには、対象者（またはテスターの代表者）にテストを実施してもらい、システムがさまざまな結果を出す仕組みをどれだけ簡単に理解できるかを判断してもらうのが理想的です。

### 6.1.10 複雑さ

多くのAIベースのシステムは複雑であるため、テスト・オラクル問題が発生します。複雑なAIベースのシステムから得られる1つのテストケースの結果に合意するためには、複数の専門家が時間をかけて議論する必要があり、理想的には多くのテストを実行したいところですが、期待される結果を（ゆっくりと）生成する専門家に頼らなければならない場合は、実行不可能になります。A/Bテスト、バックトゥバックテスト、メタモルフィックテストなど、多くのテスト技法を用いて、テスト・オラクル問題に対処することができます（これらの技法の詳細については、第8節を参照してください）。

### 6.1.11 確率的システムと非決定論的システム

多くのAIシステムは確率的な性質を持っているため、期待される結果として使用できる正確な値が常に存在するわけではありません。例えば、自律走行車が停車中のバスを迂回するルートを計画する場合、最適な解を計算する必要はなく、うまくいく（安全な）解を計算する必要があるため、最適ではないが十分な解を受け入れることになります。

また、AIベースのシステムがルートを決定する方法の性質上、毎回同じ結果にならないこともあります（例えば、ランダムな種に基づいて計算することで、毎回、無関係だが実行可能なルートが得られることがあります）。このようなシステムは非決定論的であるため、再現性に欠け、回帰テストでは、非決定論による変動性を考慮した、よりスマートな予想結果を得る必要があります。

いずれの場合も、実際の結果が不確実であるため、テスト担当者は、従来のシステムよりも、許容範囲を含めたより高度な予想結果を導き出す必要があります。また、確率論的なAIを用いたシステムでは、システムが正しく動作していることを統計的に有意に保証するために、同じテストを何度も行う必要があるかもしれません（モンテカルロ実験のように）。

### 6.1.12 AIベースのシステムのテストオラクル問題

AIベースのシステムをテストする際に繰り返し発生する課題が、テストオラクル問題です。複雑で、確率的で、自己学習的で、非決定論的なシステムの仕様が不十分なため、期待される結果が得られないという問題があります。

テストオラクル問題に対処するテストアプローチとテクニックは、第8節のブラックボックステストに記載されています。

6.2 ライフサイクルを通じたAIベースのシステムのテスト
----------------------------------------------------

### 6.2.1 一般

本節では、AIベースのシステムのライフサイクルにおける様々なテストレベル（テストフェーズと呼ばれることもある）について簡単に考察する。ライフサイクルの形態（アジャイル、ウォーターフォール、V字型、反復型など）については想定していません。これらのテストレベルは、使用するライフサイクルに関係なく、通常適用されるべきものです。すべてのテストと同様に、異なるレベルでのテストの選択は、認識されたリスクとテストのコストに基づいて行われるべきである。一般的に、初期のテストレベル（ユニットテストや統合テストなど）でのテストはコストが低く、これらのレベルで対処できるリスクはできるだけ早い段階でテストすべきである。しかし、一部のリスク（完全なシステムの特性に基づくものなど）は、完全なシステムをテストすることでしか対処できないため、システムテストレベル（エンドツーエンドのシナリオテストなど）で対処する必要がある。

6.1.1で述べたように、AIベースのシステムは、通常のコンポーネントとAIコンポーネントで構成されています。本節では、AI開発フレームワークのテストの詳細（4.2.6参照）は考慮しませんが、結果としてのAIコンポーネント（MLモデルなど）のテストは考慮します。AIベースのシステムは、多くの場合、3つの部分に分けて考えることができます。AIベースのシステムは、AIコンポーネント、データ、ユーザーインターフェースの3つの部分に分けて考えることができます。AIコンポーネントは、再利用可能なソフトウェアコンポーネントと同様にテストされることが多く、ユーザーインターフェースは、一般的なユーザーインターフェースと同様にテストされます。しかし、AIベースのシステムのデータは、6.2.2から6.2.7で説明するように、若干異なる方法でテストする必要があります。

ライフサイクル全体でのテスト環境の使用については、10.1を参照してください。

### 6.2.2 ユニット/コンポーネントテスト

非AIコンポーネント（例：ユーザーインターフェースコード）のユニット/コンポーネントテストは、従来のシステムと同じように扱う必要があります。

MLモデルのユニットテストやコンポーネントテストは、MLワークフローの評価やテストの段階に相当します（A.2.8、A.2.10参照）。従来の開発者によるユニットテストと同様に、このレベルのテストで不具合が報告されることは非常に稀であり、主な目的は、納品されるモデルの品質を向上させることにあります。

MLのパフォーマンス指標（A.8.1参照）が（モデルレベルの）受け入れ基準として設定されている場合、MLモデルはこのテストレベルでこれらの基準に対してテストされます（受け入れ基準は、特定のMLモデルを選択する評価・調整活動の一部となる場合があります）。

ユニットテストレベルでのカバレッジは、伝統的に要件やコードカバレッジ（例：ステートメント、ブランチ、デシジョンのカバレッジ）のいずれかに関係しています。しかし、MLコンポーネントのカバレッジは、データセット（トレーニング、検証、テスト）の代表性によって測定することができます。

データが前処理されている場合、ユニットテストを使用して前処理をチェックすることができます（例：生データが正しくスケーリングまたは正規化されているかどうか）。

### 6.2.3 統合テスト

AIコンポーネントが、より大きなAIベースのシステムの一部である場合、そのシステムに統合される必要があります。このようなAIベースのコンポーネントの統合には、2つの主要なアプローチがあります。まず第一に、最もシンプルに、AIベースのシステム全体に組み込まれたコンポーネントとして扱うことができます。

第二に、AIコンポーネントは、サービスとして提供することができます（通常、WebサービスなどのWeb上で提供されます）。この場合、AIコンポーネントは、AIベースのシステムの他の部分とは独立して提供され、サービスが必要なときに呼び出されます。

統合テストは、AIコンポーネントが、そのコンポーネントが属するAIベースのシステムの残りの部分と正しく統合されていることを確認するために実施されなければなりません（例えば、インターフェースのチェックや、伝達されたデータが正しく解釈されていることなど）。例えば、物体認識のためのモデルに正しい画像ファイルが渡されているか、モデルが期待するフォーマットになっているかを確認するテストを実施する必要があります。また、モデルの出力が正しく解釈され、システムの他の部分で使用されていることを確認するためのテストも実施する必要があります。

### 6.2.4 システムテスト

従来のシステムと同様に、AIベースのシステムのシステムテストでは、機能テストと非機能テストの両方が行われます。非機能テストの対象となるのは、一般的に、セキュリティと性能効率（応答時間など）です。性能効率は、AIベース・システム全体のAIコンポーネントが、組み込みコンポーネントとしてではなく、サービスとして提供される場合に、特に関連する可能性があります。従来のシステムに適用される品質特性（例えば、ISO/IEC
25010[^62]で定義されているもの）に加えて、5.1に記載されているAI特有の特性（例えば、説明可能性）も、このテストレベルでのテストのために考慮されるべきである。

MLの性能指標（A.8参照）が（システムレベルの）受け入れ基準として設定されている場合、AIベースのシステムは、このテストレベルでこれらの基準に対してテストされます。

### 6.2.5 システム統合テスト

システム統合テストは、AIベースのシステムが他のシステムからの大量のデータを使用する場合や、システムが1つまたは複数のIoTデバイスと相互作用する場合に、特に関連する可能性があります。

### 6.2.6 受入検査

受け入れテストの一環として、ビジネス上の受け入れ基準をテストする必要がある。これらの基準は、モデルから得られる結果の正確さなどの技術的な基準ではなく、AIベースのシステムが、お金を稼ぐ、節約するなどのハイレベルなビジネス目標を満たしているかどうかに焦点を当てるのが一般的です。

MLパフォーマンス指標（A.8参照）が受け入れ基準として設定されている場合、AIベースのシステムは、このテストレベルでこれらの基準に対してテストされます。

人間はテクノロジーに過度に依存することがあり、AIシステムが人間をループに含めている場合、テスト対象のシステムの人間と自動化された出力の品質が正しくない場合があります。このような場合には、予測や入力済みのフィールドなど、ユーザーが承認した情報の精度を測定することが重要になります。

### 6.2.7 メンテナンス・テスト

システムの進化に伴う問題として、AIベースのシステムが当初の受け入れ基準（ビジネス面、技術面）を満たしているかどうかを確認するために、定期的にテストを行う必要がある場合があります。

これらの基準がパフォーマンスメトリクス（A.8参照）として指定されている場合、これらのテストは自動化することができます。

保守テストの一環として回帰テストを使用する場合、多くのAIベースのシステムは確率的で非決定論的な性質を持っているため、システムが単に別の結果を提供しているにもかかわらず、明らかにテストが失敗してしまうことがありますが、これは許容範囲内です。これは、リグレッションテストの期待される結果が、決定論的なシステムに使用されるものよりもスマートでなければならないことを意味しています（例えば、許容範囲を含めて）。

運用中の自己学習システムをテストする際には、テストによってシステムが望まない学習をしてしまわないように注意する必要があります。

7 MLシステムのテストとQA
========================

7.1 MLシステムのテストとQAの紹介
--------------------------------

この条項では、MLに直接関連する品質保証とテストの機会を簡潔に示しています。

7.2 MLのワークフローの見直し
----------------------------

ML を実施する際には、使用する ML
ワークフローを文書化し、それに従わなければならない。附属書 A
に記載されているワークフローからの逸脱は、正当化されるべきです。

7.3 受入基準
------------

受入基準（機能的要件と非機能的要件の両方を含む）を文書化し、このアプリケーションでの使用を正当化すること。モデルの性能評価指標を含めること。最低限、AI固有の特性（5.1に記載）を考慮する必要があり、AIベースのシステムの受け入れ基準の完全性を判断するためのチェックリストのベースとして使用することができます。

7.4 フレームワーク、アルゴリズム/モデル、ハイパーパラメータの選択
-----------------------------------------------------------------

フレームワーク、アルゴリズム、モデル、設定、ハイパーパラメータの選択は、文書化して説明しなければなりません。

7.5 トレーニングデータの品質
----------------------------

MLシステムは、学習データが運用データを代表しているかどうかに大きく依存しており、MLシステムの中には、自律走行車に使用されているような広範な運用環境を持つものもあります。

境界条件は、あらゆる種類のシステム（AIおよび非AI）において故障の原因となることが知られており、トレーニングデータに含めるべきである。データセットのサイズや、バイアス、透明性、完全性などの特徴を考慮したトレーニングデータの選択は、文書化され、正当化されるべきであり、システムに関連するリスクのレベルがそれを必要とする場合には、専門家によって確認されるべきである（例：重要なシステム）。

7.6 データ品質のテスト
----------------------

トレーニングデータの基準はテストデータにも同様に適用されますが、テストデータはトレーニングデータから可能な限り独立していなければならないという注意点があります。独立性のレベルは文書化され、正当化されるべきである。テストデータは体系的に選択および／または作成し、ネガティブテスト（例：想定される入力範囲外の入力）および敵対的なテストも含めるべきである（詳細は7.8を参照）。

7.7 モデルアップデート
----------------------

デプロイされたモデルが更新されるたびに、モデルの劣化（新しいモデルが以前のモデルよりも動作が遅くなるなど）のテストなど、文書化されていない暗黙の要件に対するテストを含め、引き続き受け入れ基準を満たしていることを確認するために再テストを行う必要があります。適切な場合には、以前のモデルに対してA/Bテストやバックツーバックテストを行うべきです。

7.8 敵対的な事例とテスト
------------------------

逆説的な例とは，ニューラルネットワークの入力に極めて小さな変更を加えると，出力に予想外の（誤った）大きな変化が生じる（つまり，入力が変更されていない場合とは全く異なる結果になる）というものである[^63]．逆説的な例が最初に注目されたのは、画像分類器でした。人間の目には見えない数個のピクセルを変更するだけで、ニューラルネットワークに、全く異なる対象物への画像分類を変更するよう説得することができます（しかも、高い信頼度で）。しかし、敵対的な例は、画像分類器に限らず、ニューラルネットワーク一般の既知の属性であり、ニューラルネットワークのあらゆる用途に適用されることに注意してください（他の形態のMLモデルにも適用される可能性があります）。

敵対的事例は一般的に伝達可能です。つまり、あるニューラルネットワークを失敗させる敵対的事例は、同じタスクを実行するように訓練された他のニューラルネットワークを失敗させることが多いということです。これらの他のニューラルネットワークは、異なるデータや異なるアーキテクチャに基づいてトレーニングされている可能性がありますが、同じ敵対的事例で失敗する可能性があることに注意してください。

敵対的なテストとは、敵対的な攻撃を行うことを指します。このような攻撃を行い、テスト中に脆弱性を特定することで、将来の障害に対する対策を講じることができ、ニューラルネットワークの堅牢性を向上させることができます。

攻撃は、モデルをトレーニングするときと、トレーニングされたモデル（ニューラルネットワーク）自体に対して行われます。

学習時の攻撃には、学習データの破損（ラベルの変更など）、学習セットへの不良データの追加（不要な特徴など）、学習アルゴリズムの破損などがあります。学習したモデルへの攻撃は、ホワイトボックスまたはブラックボックスのいずれかで、モデルに悪い結果を出させるような敵対的な例を特定することになります。

ホワイトボックス攻撃では、攻撃者はモデルの学習に使用されたアルゴリズムや、使用された設定やハイパーパラメータを完全に把握しています。この知識を利用して、例えば、入力に小さな摂動を加え、どの摂動がモデルに大きな変化をもたらすかを監視することで、敵対的なサンプルを生成します。

ブラックボックス攻撃では、攻撃者はモデルの内部構造にアクセスできず、モデルがどのように学習されたかについても知りません。この場合、攻撃者は最初にモデルを使用してその機能性を判断し、次に同じ機能を提供する「複製」モデルを構築します。その後、攻撃者はホワイトボックス手法を用いて、この複製モデルの敵対的な事例を特定します。敵対的な事例は一般的に転用可能であるため、通常、同じ敵対的な事例が（ブラックボックス）モデルにも適用されます。

7.9 機械学習のベンチマーク
--------------------------

理想的には、新しいMLシステムを専門家が評価することですが、それではコストがかかりすぎます。

その代わりに、「代表的な」業界標準のベンチマーク・スイートが用意されており、さまざまな状況（画像分類、物体検出、翻訳、推薦など）をカバーする多様な作業負荷が含まれています。

これらのベンチマーク・スイートは、ハードウェア（定義されたモデルを使用）とソフトウェア（最速のモデルを決定するなど）の両方の性能を測定するために使用できます。ソフトウェアのベンチマークスイートでは、学習（定義された学習データセットを用いて、フレームワークがMLモデルを75%の精度などの指定された目標品質指標に向けて学習する速さなど）および推論（学習されたMLモデルが推論を実行する速さなど）を測定することができます。

MLのベンチマークセットの例としては、ソフトウェアフレームワーク、ハードウェアアクセラレータ、MLクラウドプラットフォームのベンチマークを提供するMLPerf[^64]や、スタンフォード大学のベンチマークスイートであるDAWNBench[^65]などがあります。OAEI（Ontology
Alignment Evaluation
Initiative）は、以下を目的とした国際的な協調活動です[^66]。

- アライメント/マッチングシステムの強みと弱みを評価する。
- テクニックの性能比較
- アルゴリズム開発者間のコミュニケーションを深める。
- 評価技術の向上
- オトロジーのアラインメント/マッチングに関する作業を改善するのに役立ちます。

これらの目標は、技術の性能を制御された実験的な評価によって達成されます。

8 AIベースのシステムのブラックボックステスト
============================================

8.1 コンビナトリアル・テスト
----------------------------

あるテスト項目が、与えられたすべての状況下で、すべての要求を満たしていることを動的テストで証明するためには、すべての可能な状態における入力値のすべての組み合わせをテストする必要があります。このような非現実的な行為は「網羅的テスト」と呼ばれています。そのため、実際のソフトウェアテストでは、可能な入力値と状態の（非常に大きな）セットからサンプリングしてテストスイートを導き出します。組合せテストは、この入力空間から有用な組み合わせのサブセットを導き出すための、体系的かつ効果的なアプローチのひとつである[^67]。

対象となる組み合わせは、パラメータ（入力や環境条件）と、そのパラメータが取り得る値で定義されます。多数のパラメータ（それぞれが多数の離散的な値を持つ）を組み合わせることができれば、テストスイートの欠陥検出能力を損なうことなく、必要なテストケースの数を大幅に減らすことができます。

ISO/IEC/IEEE
29119-4では、全組合せ、各選択肢テスト、基本選択肢テスト、ペアワイズテストなどの組合せテスト手法を定義している[^68]。実際にはペアワイズテストが最も広く使用されているが、これは理解しやすいこと、ツールのサポートが充実していること、そして欠陥の多くが少数のパラメータを含む相互作用によって引き起こされるという研究結果が主な理由[^69]である。

特に、ビッグデータを利用する場合や、自動運転車のように外界と相互作用する場合には、AIベースのシステムにとって関心のあるパラメータの数は非常に多くなります。そのため、ペアワイズテストのような組み合わせテストを用いて、無限に近い数の組み合わせを管理可能なサブセットにまで体系的に削減する手段は非常に有用です。実際には、ペアワイズテストを使用したとしても、このようなシステムのテストスイートは膨大な量になるため、自動化や仮想テスト環境（10.1参照）の使用が必要になることがあります。

自動運転車を例にとると、システムテストのシナリオには、車両のさまざまな機能と、それらが動作することが期待される環境の両方を考慮する必要があります。そのため、パラメータには、様々な自動運転機能（クルーズコントロール、アダプティブクルーズコントロール、レーンキープアシスト、レーンチェンジアシスト、トラフィックライトアシストなど）と、環境制約（道路の種類や路面、地理的エリア、時間帯、天候、交通状況、視界など）が含まれる必要があります。これらのパラメータに加えて、センサーからの入力を様々なレベルで考慮する必要があります（例えば、ビデオカメラからの入力は、旅が進むにつれて汚れてくると劣化しますし、GPSユニットの精度は、様々な数の衛星が視界に入ったり入らなかったりすると変化します）。自動運転車など、安全性が重視されるAIベースのシステムにコンビナトリアルテストを適用する場合、どの程度の厳密さが求められるのかについては、現時点では不明ですが（例えば、ペアワイズでは不十分かもしれません）、不具合の発見に効果的であることや、残存するリスクのレベルを推定するために利用できることは知られています。

8.2 バックトゥバック・テスト
----------------------------

バックトゥバックテストでは、システムの別バージョン（既に存在するもの、別のチームが開発したもの、別のプログラミング言語で実装されたものなど）を疑似的に使用し、同じテスト入力から期待される結果を生成して比較します。これは「差分テスト」と呼ばれることもあります。

このように、バックトゥバックテストは、テスト入力が生成されないため、テストケース生成技術ではありません。

擬似オラクル（機能的に等価なシステム）により、期待される結果のみが自動的に生成されます。テスト入力を生成するツール（ランダムなど）と併用することで、大量の自動テストを強力に行うことができます。

機能テストをサポートするためにバックトゥバック・テストを使用する場合、擬似オラクルは、テスト対象のシステムと同じ非機能的な制約を満たす必要はありません。例えば、テスト対象のシステムで必要とされる速度よりもはるかに遅い速度で擬似的に動作させることができます。また、機能的に完全に等価なシステムである必要はなく、テスト対象システムの一部としか等価でない擬似的なものでもバックトゥバックテストを行うことができます。

MLでは、さまざまなフレームワーク、アルゴリズム、設定を用いて擬似的なオラクルを作ることができます（場合によっては、AIではない従来のソフトウェアを用いて擬似的なオラクルを作ることも可能です）。擬似オラクルを使用する際の既知の問題は、うまく機能するためにはテスト対象のソフトウェアから完全に独立している必要があることです。AIベースのシステムの開発には、再利用可能なオープンソース・ソフトウェアが多く使用されているため、この独立性が簡単に損なわれてしまうのです。

8.3 A/Bテスト
-------------

A/Bテストとは、2つのシステムのどちらの性能が優れているかを判断するための統計的テスト手法[^70]です。A/Bテストは、デジタルマーケティング（例：反応の良いメールを見つける）や、顧客対応の場面でよく使われます。

例えば、ユーザーインターフェースの設計を最適化するために、A/Bテストがよく使われます。例えば、ユーザーインターフェースの設計者が、「購入」ボタンの色を現在の赤から青に変更することで、売上が増加するという仮説を立てます。そこで、ボタンの色を青にしたインターフェースを新たに作成し、2つのインターフェースを別々のユーザーに割り当てました。2つのバリエーションの販売率を比較し、統計的に有意な数の使用があれば、仮説が正しかったかどうかを判断することができました。

青いボタンがより多くの売上を生み出した場合、青いボタンを持つ新しいインターフェイスが、赤いボタンを持つ現在のインターフェイスに取って代わることになります。このようなA/Bテストは、統計的に有意な数の使用を必要とし、時間がかかりますが、ツール（多くの場合、AIを使用）を使ってサポートすることができます。

A/Bテストは、テスト入力が生成されないため、テストケース生成技法ではない。A/Bテストは、既存のシステムを部分的なオラクルとして利用することで、テストのオラクル問題を解決する手段である。新システムと既存システムを比較することで、新システムが何らかの意味で優れているかどうかを判断することができる。デジタルマーケティングの場合、成功の尺度はより多くの売上かもしれませんが、分類器のようなAIベースのシステムでは、精度、感度、リコールなどのパフォーマンス指標を使用することができます（A.8参照）。

A/Bテストは、AIベースのシステムのコンポーネントが更新されたときに、受け入れ基準（例：「特定のパフォーマンス指標が改善されるか、同じままであること」）が定義され、合意されていれば、いつでも使用することができます。

A/Bテストが自動化されている場合、妥当な受け入れ基準が設定されていれば、自己学習するAIベースのシステムのテストに使用することができます。システムの新しいパフォーマンスと以前のパフォーマンスを比較し、自己学習によってシステムのパフォーマンスが向上しなかった場合は、以前のバージョンに戻します。

8.4 メタモルフィック・テスト
----------------------------

メタモルフィックテスト[^71][^72]は、AIベースのシステムでよく見られる、テストの合否判定が困難なテストラクル問題（複雑性、非決定性、確率的なシステムなど）に対処するためのテストケース生成手法である。メタモルフィックテストで生成されるテストケースと従来のテストケース設計手法との主な違いは、メタモルフィックテストの期待値が固定値ではなく、別の期待値との関係で定義されることです。

メタモルフィックテストでは、メタモルフィック関係を用いて、正しいことがわかっているソーステストケースからフォローアップテストケースを生成する。テスト対象ソフトウェアのメタモルフィック関係は、ソーステストケースからフォローアップテストケースへのテストインプットの変化が、ソーステストケースからフォローアップテストケースへの期待されるアウトプットにどのような影響を与えるのか（または与えないのか）を説明するものである。これらの期待されるメタモルフィックな関係は、実施されるテストの部分的なオアシスと考えることができる。

例 1
始点と終点の間の距離を測定するテスト項目があります。ソースとなるテストケースには、テスト入力A（始点）、B（終点）と、テストケースを実行したときの期待結果C（距離）があります。変形関係とは、始点と終点が入れ替わっても、期待される結果は変化しないというものです。したがって、Bを始点、Aを終点、Cを距離とした追試テストケースを生成することができる。

例2
あるテスト項目では、一連のライフスタイルのパラメータに基づいて個人の死亡年齢を予測します。ソースのテストケースには、1日に吸うタバコの本数が10本など、さまざまなテスト入力があり、テストケースを実行した結果、58歳という期待値が得られます。メタモルフォーゼ関係は、人がより多くのタバコを吸えば、その人の予想死亡年齢はおそらく減少する（増加しない）というものです。したがって、同じライフスタイル・パラメータの入力セットで、喫煙本数を1日20本に増やした以外は、フォローアップのテストケースを生成することができます。このフォローアップテストケースの期待される結果（予測死亡年齢）は、58歳以下または58歳と同じに設定することができます。

フォローアップテストケースの期待値は、必ずしも正確な値ではなく、ソーステストケースを実行して得られた実際の結果の関数として記述されることが多い（例：フォローアップテストケースの期待値はソーステストケースの実際の結果よりも大きい）。

1つのメタモルフィック関係が複数のフォローアップテストケースを導き出すために使用されることがよくある（例えば、音声をテキストに変換する関数のメタモルフィック関係は、同じ音声入力ファイルを異なる入力音量レベルで使用し、同じテキストを期待する結果として、複数のフォローアップテストケースを生成するために使用することができる）。メタモルフィック関係が形式的（または半形式的）に記述され、ソーステストケースが提供されていれば、フォローアップテストケースの生成を自動化することができるはずである。ただし、ドメインの知識を必要とするメタモルフィック関係の生成を自動化することは不可能である。

メタモルフォーゼ・テストを行うためのプロセスは

a)  ::: {.Definition-Term}
    メタモルフィック・リレーションズ（MR）の構築
    :::

b)  テスト対象のプログラムの特性を特定し、それをテストの入力と期待される出力の間の変態的な関係として表現し、ソースのテストケースに基づいてフォローアップのテストケースを生成するための何らかの方法を提供します。

c)  ::: {.Definition-Term}
    MRの見直し
    :::

d)  お客様やユーザーとのMRの確認。

e)  ::: {.Definition-Term}
    ソーステストケースの生成
    :::

f)  ソーステストケースのセットを生成します（任意のテスト技法またはランダムテストを使用）。

g)  ::: {.Definition-Term}
    フォローアップテストケースの作成
    :::

h)  メタモルフォーゼの関係を利用して、フォローアップのテストケースを生成します。

i)  ::: {.Definition-Term}
    メタモルフィック・テストケースの実行
    :::

j)  ソーステストケースとフォローアップテストケースの両方を実行し、出力がメタモルフィック関係に違反していないことを確認します。そうでない場合は、メタモルフィックテストケースが失敗したことになり、バグを示します。

Metamorphic testing has been used on many types of traditional software,
as well as successfully ina wide variety of AI-based application areas,
such as bioinformatics, web services, machine learningclassifiers,
search engines and security. Research shows that only 3 to 6 diverse
metamorphic relationscan reveal over 90 % of the faults that could be
detected using a traditional test oracle[^73].

8.5 Exploratory testing
-----------------------

Test design and execution can be conducted in a number of ways,
depending on the needs of eachproject. It can be scripted or
exploratory. In practice, a combination of scripted and exploratory
testingis typically used, as scripted testing ensures required test
coverage levels are achieved and bettersupports automated testing, while
exploratory testing allows for creativity and the rapid executionof
tests. When testing AI-based systems, exploratory testing is often found
to be beneficial due to itssuitability when specifications are poor or
lean (such as in agile development).

In exploratory testing, tests are designed and executed on the fly, as
the tester interacts with andlearns about the test item. Session sheets
are often used to structure exploratory testing sessions (e.g.by setting
a focus and time limits on each test session). These same session sheets
are also used tocapture information about what was tested, and any
anomalous behaviour observed. Exploratory testsare often not wholly
unscripted, as high-level test scenarios (sometimes called "test ideas")
are oftendocumented in the session sheets to provide a focus for the
exploratory testing session.

9 White-box testing of neural networks
======================================

9.1 Structure of a neural network
---------------------------------

A neural network is a computational model inspired by the neural network
in a human brain. Itcomprises a number of layers of connected nodes or
neurons, as shown in Figure 3. Note that in thisclause we will use as
our example a feedforward neural network, which was the first and is the
simplesttype of artificial neural network -- the only extra complexity
we will add is that we will consider anetwork with multiple layers --
known as a multi-layer perceptron (or deep neural net as it has
hiddenlayers).

Figure 3 --- Deep neural net

The input nodes receive information from the outside world (e.g. each
input may be a value for a pixelin an image), and the output nodes
provide information to the outside world (e.g. a classification).
Thenodes in the hidden layers have no connections to the outside world
and perform the computations thatpass information from the input nodes
to the output nodes.

As shown in Figure 4, each neuron accepts input values and generates
output values, known as activationvalues (or output vectors), which can
be positive, negative or zero (with a value of zero, a neuron hasno
influence on downstream neurons). Each connection has a weight (these
change as the network istrained) and each neuron has a bias (note that
the bias here is quite different from the bias associatedwith unfairness
described in Clause 5.1.5). The activation values are calculated by a
formula (known asthe activation function) based on the input activation
values, the weights of the input connections andthe bias of the neuron.

Figure 4 --- Neuron activation values

For supervised learning, the network learns by use of backward
propagation. Initially all nodes areset to an initial value and the
first input training data is passed into and through the network. The
output is compared with the known correct answer and the difference
between the calculated outputand the correct answer (the error) is fed
back to the previous layer of the network and is used to modifythe
weights. This backward error propagation goes back through the whole
network and each of theconnection weights is updated as appropriate. As
more training data is fed into the network it graduallylearns from the
errors until it is considered ready for evaluation with the validation
data, which willdetermine the performance of the trained network.

9.2 Test coverage measures for neural networks
----------------------------------------------

### 9.2.1 Introduction to test coverage levels

Traditional coverage measures are not really useful for neural networks
as 100 % statement coverage istypically achieved with a single test
case. The defects are normally hidden in the neural network itself.

Thus, different coverage measures have been proposed based on the
activation values of the neurons(or pairs of neurons) in the neural
network when the neural network is tested.

Having measures of coverage of the neural network allows testers to
maximize coverage, which has beenshown to identify incorrect behaviours
in AI-based systems, such as self-driving car systems[^74][^75].

### 9.2.2 Neuron coverage

Neuron coverage for a set of tests is defined as the proportion of
activated neurons divided by the totalnumber of neurons in the neural
network (normally expressed as a percentage). For neuron coverage,
aneuron is considered to be activated if its activation value exceeds
zero.

### 9.2.3 Threshold coverage

Threshold coverage for a set of tests is defined as the proportion of
neurons exceeding a thresholdactivation value divided by the total
number of neurons in the neural network (normally expressed asa
percentage). For threshold coverage, a threshold activation value
between 0 and 1 is chosen as thethreshold value. Note that this
threshold coverage corresponds to 'neuron coverage' in the
DeepXploretool[^76].

### 9.2.4 Sign change coverage

Sign change coverage for a set of tests is defined as the proportion of
neurons activated with bothpositive and negative activation values
divided by the total number of neurons in the neural network(normally
expressed as a percentage). An activation value of zero is considered to
be a negativeactivation value[^77].

### 9.2.5 Value change coverage

Value change coverage for a set of tests is defined as the proportion of
neurons activated where theiractivation values differ by more than a
change amount divided by the total number of neurons in theneural
network (normally expressed as a percentage). For value change coverage,
a value between 0and 1 should be chosen as the change amount[^78].

### 9.2.6 Sign-sign coverage

Sign-Sign coverage for a set of tests is achieved if each neuron by
changing sign (see 9.2.4) can be shownto individually cause one neuron
in the next layer to change sign while all other neurons in the
nextlayer stay the same (i.e. they do not change sign). In concept, this
level of neuron coverage is similar tomodified condition/decision
coverage (MC/DC)[^79].

### 9.2.7 Layer coverage

Coverage measures can also be defined based on whole layers of the
neural network and how theactivation values for the set of neurons in a
whole layer change (e.g. absolutely or relative to each other).

Further research is needed in this area.

9.3 Test effectiveness of the white-box measures
------------------------------------------------

There is currently little data on the test effectiveness of the
different white-box coverage measures forthe white-box testing of neural
networks. However, it is generally true that criteria requiring more
testswill find more defects than those that require fewer tests, so
allowing the relative effectiveness of themeasures to be deduced.
Several subsumes relationships can be derived from the coverage
measuresdescribed in 9.2.1. to 9.2.5. All other measures subsume neuron
coverage and sign-sign coverage alsosubsumes sign change coverage. The
full subsumes hierarchy for these is shown in Figure 5. Where anarrow
points from one measure to another, it means that if the first measure
is fully achieved, then thesecond measure is automatically achieved. For
instance, it shows that if threshold coverage is achieved,then neuron
coverage is automatically achieved.

Figure 5 --- White-box neural network subsumes hierarchy Although easy
to understand, achieving high levels of neuron coverage can normally be
achieved usingonly a few test cases, so limiting its test effectiveness.
Early results for threshold coverage appear toshow that this may be a
useful measure for generating tests that cover defect-inducing corner
cases, butthe threshold value may need to be set individually for each
neural network. For value change coverage,higher values for the change
amount will naturally require more test cases. Sign-sign coverage
isnormally the most rigorous of the coverage criteria specified
here[^80].

9.4 White-box testing tools for neural networks
-----------------------------------------------

Commercial tools are not currently available to support the white-box
testing of neural networks,however there are several research tools,
including:

- DeepXplore -- specifically for testing deep neural nets, proposes a
    white-box differential testing(back-to-back) algorithm to
    systematically generate adversarial examples that cover all neurons
    inthe network (threshold coverage)[^81].
- DeepTest -- systematic testing tool for automatically detecting
    erroneous behaviours of cars drivenby deep neural nets[^82], which
    supports the sign-sign coverage for DNNs.
- DeepCover - provides all the levels of coverage defined in this
    clause[^83].

10 Test environments for AI-based systems
=========================================

10.1 Test environments for AI-based systems
-------------------------------------------

The test environments for AI-based systems have much in common with
those for conventionalsystems: typically, the development environment at
unit level and a production-like test environmentat system and
acceptance levels. ML models, when tested in isolation, are typically
tested within theirdevelopment framework, as described in A.2.9.

There are two main factors that affect the selection of test
environments for AI-based systemsfrom those required for conventional
systems. First, the context in which AI-based systems, such asautonomous
systems, operate means their environment can be large, complex and
constantly changing.

This can make testing in the real world extremely expensive if the full
range of possible environmentsare to be tested, the test environments
are expected to be realistic and the testing is to be performedwithin a
sensible timescale. Second, those AI-based systems that can physically
interact with humanshave a safety component, which can make testing in
the real world dangerous. Both factors indicate theneed for the use of
virtual test environments.

Virtual test environments provide the following benefits, among others:

- The use of a virtual environment ensures that dangerous scenarios
    can be tested in safety withoutcausing damage to the system under
    test or any objects in its environment, such as vehicles,buildings,
    animals and humans. Tests in virtual environments are typically also
    better for the real-world environment.
- Virtual environments do not need to run in real-time -- they can be
    run much faster with suitableprocessing power - meaning that many
    tests can be run in a short time period, potentiallydecreasing
    time-to-market by a large amount. A single system can also be tested
    on many virtualtest environments running in parallel, perhaps in the
    cloud.
- Virtual environments can be cheaper to set up and run than their
    real-world counterparts. Forinstance, testing mobile phone
    communications across widely different urban environments is
    farcheaper when performed in a laboratory with virtual phones,
    transmitters and landscapes ratherthan with real phones being driven
    around a mix of locations, largely because only the relevantfeatures
    need to be included in the virtual test environment[^84]. However,
    it should be noted thatsome virtual test environments must be truly
    representative and closely match the real-world insome respects. For
    instance, the testing of pedestrian avoidance in autonomous vehicles
    can requirehigh levels of image representativeness.
- Sometimes, creating unusual (edge-case) scenarios can be very
    difficult in the real world andvirtual environments allow the
    creation of such scenarios (and the ability to run multiple
    variantsof these unusual scenarios many times). Virtual environments
    provide the tester with a greaterlevel of control than they would
    have with real-word testing. These tests can also incorporate a
    levelof randomness, such as by including AI-based humans in
    autonomous car testing.
- By supporting the simulation of hardware, virtual environments allow
    systems to be tested withhardware components even when these
    components are not physically available (perhaps theyhave not been
    built yet) and they allow different hardware solutions to be
    trialled and comparedinexpensively.
- Virtual environments provide excellent observability, so that all
    aspects of the system under test'sresponse to a scenario can be
    measured and, where necessary, subsequently analysed.
- Virtual environments can be used to test systems that cannot be
    tested in their real operationalenvironment, such as a robot working
    on the site of a nuclear accident or a system to be used forspace
    exploration.

Virtual testing can be performed on simulators built specifically for a
given system, but reusablesimulators for specific domains are available
both commercially and open source, for instance:

- Morse, the Modular Robots Open Simulation Engine, a simulator for
    generic mobile robot simulation(single or multi robots), based on
    the Blender game engine[^85];
- AI Habitat, a simulation platform created by Facebook AI, designed
    to train embodied agents (suchas virtual robots) in photo-realistic
    3D environments[^86];
- DRIVE Constellation, an open and scalable platform for self-driving
    cars from NVIDIA based on acloud-based platform, capable of
    generating billions of miles of autonomous vehicle testing[^87].

10.2 Test scenario derivation
-----------------------------

For the systematic testing of an AI-based system, test scenarios need to
be generated to test individualAI components, the interaction of these
components with the rest of the system, the complete system
ofinteracting components, and the system interacting with its
environment.

Test scenarios can be derived from several sources:

- System requirements
- User issues
- Automatically reported issues (e.g. for autonomous systems)
- Accident reports (e.g. for physical systems)
- Insurance data (e.g. for insured systems, such autonomous cars)
- Regulatory body data (e.g. collected through legislation)
- Testing at various levels (e.g. test failures or anomalies on the
    test track or on real roads couldgenerate interesting test scenarios
    for an autonomous car at other test levels, while a sample oftest
    scenarios run on the virtual test environment should also be run on
    real roads to validaterepresentativeness of the virtual test
    environment)

An option using combinatorial testing for the generation of test
scenarios for the system testing ofautonomous cars is described in 8.1.
Metamorphic testing (see 8.4) and fuzz testing could also be usedto
generate test scenarios.

10.3 Regulatory test scenarios and test environments
----------------------------------------------------

In the case of safety-related AI-based systems, some level of regulation
can apply to the systems.

Two options are generally available to government for this regulation;
it can allow the developmentorganization to self-regulate or a
regulatory body is set up to provide independent assurance that
thesystems meet minimum standards (a certification approach).

If a certification approach is followed, then the testing approach will
need to be shared between theregulatory body and those providing the
systems for certification (as it is for the crash testing of cars).

A core part of the approach will be shared test environment definitions
and shared test scenarios thatcan be run using test automation on those
environments. The core set of shared test scenarios will needto be
parameterized to allow new scenarios to be generated by varying the
parameter values for eachtest to prevent overfitting and the regulatory
body will also keep a set of private test scenarios that arenot shared.
The parameterization and the private scenarios should ensure that
systems are not builtjust to pass known tests, and this approach also
allows the regulatory body to add new scenarios asthey become aware of
potential problem situations from actual use of the systems.

Annex A: Machine learning
=========================

A.1 Introduction to machine learning
------------------------------------

Machine learning (ML) is a form of AI, where the AI-based system learns
its behaviour from providedtraining data, rather than being explicitly
programmed. The outcome of ML is known as a model,which is created by
the AI development framework using a selected algorithm and the training
data;this model reflects the learnt relationships between inputs and
outputs. Often the created model, onceinitially trained, does not change
in use. In contrast, in some situations, the created model can
continueto learn from operational use (i.e. it is self-learning).
Example uses of ML include image classification,playing games (e.g. Go),
speech recognition, security systems (malware detection), aircraft
collisionavoidance systems and autonomous cars.

There are three basic approaches to machine learning (ML), as shown in
Figure A.1.

Figure A.1 --- Forms of machine learning

With supervised ML the algorithm creates the model based on a training
set of labelled data. An exampleof supervised ML would be where the
provided data were labelled pictures of cats and dogs and thecreated
model is expected to correctly identify cats and dogs it sees in the
future. Supervised learningsolves two forms of problem -- classification
problems and regression problems. Classification is wherethe model
classifies the inputs into different classes, such as 'yes -- this
module is error-prone' and 'no --this module is not error-prone'.
Regression is where the model provides a value, such as 'the
expectednumber of bugs in the module is 12'. As ML is probabilistic, we
can also measure the likelihood of theseclassifications and regressions
being correct (see A.8 on performance metrics for ML).

With unsupervised ML the data in the training set is not labelled and so
the algorithm derives thepatterns in the data itself. An example of
unsupervised ML would be where the provided data was aboutcustomers and
the system was used to find specific groupings of customers, which may
be marketed toin a specific manner. Because the training data does not
have to be labelled, it is easier (and cheaper) tosource than the
training data for supervised ML.

With reinforcement learning a reward function is defined for the system
(agent), which returns ahigher reward when the system gets closer to the
required behaviour. Using feedback from the reward function, the system
learns to improve its behaviour. An example of reinforcement learning
would be aroute planning system that used a reward function to find the
shortest route.

ISO/IEC 23053[^88] describes a framework for AI-based systems using
machine learning and covers someof the material in this annex in more
detail.

A.2 The machine learning workflow
---------------------------------

### A.2.1 Machine learning workflow overview

The activities in the machine learning workflow are shown in Figure A.2.

Figure A.2 --- Machine learning workflow

The activities in the machine learning workflow are described in A.2.2
to A.2.13.

### A.2.2 Understand the objectives

The purpose of the ML model to be deployed needs to be understood and
agreed with the stakeholdersto ensure alignment with business
priorities. Acceptance criteria (including performance metrics --
see4.1) should be defined for the developed model.

### A.2.3 Select a framework

A suitable AI development framework should be selected based on the
objectives, acceptance criteriaand business priorities. These frameworks
are introduced in 4.2.6.

### A.2.4 Build and compile the model

The model structure (e.g. number of layers) should be defined (it will
typically be in source code, suchas Python). Next, the model is
compiled, ready to be trained.

### A.2.5 Source the data

The data used by the model will be based on the objectives. For
instance, if the system is a real-timetrading system, the data will come
from the trading market. If the system is analysing customers'
retailpreferences for a marketing campaign, then the organization's
customer big data will be the source.

The data used to train, tune and test the model should be representative
of the operational dataexpected to be used by the model. In some cases,
it is possible to use pre-gathered datasets for theinitial training of
the model (e.g. see Kaggle datasets at https://www.kaggle.com/datasets).
However,raw data normally needs some pre-processing.

### A.2.6 Pre-process the data

The features in the data that will be used by our model need to be
selected -- these are the attributesor properties in the data that we
believe are most likely to affect the outcome of the prediction.

Training data may need to be managed to remove features that are not
expected (or we don't want)to have any effect on the resultant model --
this is called feature engineering or feature selection. Byremoving
irrelevant information (noise), feature engineering can reduce overall
training times, preventoverfitting (see A.4.1), increase accuracy and
make models more generalizable.

Real world data is likely to include outlier values, be in a variety of
formats, be missing coverage ofimportant areas, etc. Thus,
pre-processing is normally required before it can be used to train (and
test)the model. Pre-processing includes conversion of data to numeric
values, normalizing numeric data toa common scale, detection and removal
of outliers and noisy data, reducing data duplication and theaddition of
missing data.

### A.2.7 Train the model

A ML algorithm (e.g. see machine learning techniques in 4.2.4) uses the
training data to create andtrain the model. The algorithm should be
selected based on the objectives, acceptance criteria and theavailable
data.

Note that the activities of training, evaluation and tuning are shown
explicitly in Figure A.2 as beingiterative, however ML is a highly
iterative workflow and it may be necessary to return to any of
theearlier activities, such as sourcing and pre-processing the data as a
result of later activities (e.g.evaluating the model).

### A.2.8 Evaluate the model

The trained model is evaluated against the agreed performance metrics
using validation data; theresults are then used to improve (tune) the
model. Visualization of the results of the evaluation isnormally
required and different ML frameworks support different visualization
options.

In practice several models are typically created and trained, and the
best one chosen based on theresults of the evaluation and tuning.

### A.2.9 Tune the model

The results from evaluating the model against the agreed performance
metrics are used to adjustits settings to fit the data and so improve
performance. The model may be tuned by hyperparametertuning, where the
training activity is modified (e.g. by changing the number of training
steps or bychanging the amount of data used for training), or attributes
of the model are set (e.g. the number ofneurons in a neural network or
the depth of a decision tree).

### A.2.10 Test the model

Once a model has been trained, evaluated, tuned and selected it should
be tested against the testdataset to ensure that the agreed performance
criteria are met. This test data should be completelyindependent of the
training and validation data used up until this point in the workflow.

### A.2.11 Deploy the model

The tuned model typically needs to be re-engineered for deployment along
with its related resources,including the relevant data pipeline -- this
is normally achieved through the ML framework. Targetsinclude embedded
systems and the cloud, where the model can be accessed via a web API.

### A.2.12 Use the model

Once deployed, the model, typically as part of a larger AI-based system,
can be used operationally. Modelsmay perform scheduled batch predictions
at set time intervals or may run on request in real-time.

### A.2.13 Monitor and tune the model

While the model is being used, there is a danger that its situation may
evolve (see 5.1.4 on Evolution)and that the model may 'drift' away from
its intended performance. To ensure that any drift is identifiedand
managed, the operational model should be regularly evaluated against its
acceptance criteria.

It may be deemed necessary to update the model to address the problem of
drift or it may be decidedthat re-training with new data will result in
a more accurate or more robust model, in which case a newmodel may be
created and trained with updated training data. The new model may be
compared againstthe existing model using a form of A/B Testing (see 8.3)
to ensure its performance has not deteriorated.

A.3 Machine learning training and test data
-------------------------------------------

When performed supervised ML, two separate datasets (a training dataset
and test set) are used toprevent overfitting (see A.4.1). The test set
is sometimes called the holdout set.

To support the iterative evaluation and tuning of the model, the
training dataset is split into two -- dataused for training and
validation data used for evaluation. However, this can mean that there
is nowinsufficient data for training. One way to address this problem is
known as cross-validation, where thetraining dataset is split into n
equal parts known as folds (Figure A.3 shows training data in four
folds).

The model is then trained and evaluated n times -- in each case a
different fold is used as the validationset and the remaining folds are
used as the training set. In this way training is improved and
evaluationand tuning can still be performed.

Figure A.3 --- Training data with four folds

A.4 Overfitting and underfitting in machine learning
----------------------------------------------------

### A.4.1 Overfitting

Overfitting occurs when the model learns incorrect relationships from
extraneous information, suchas insignificant details, random
fluctuations and noise in the training data (i.e. too many features
havebeen included in the training data). In effect it's as if the model
has memorized the training data andwhen it is used operationally it
works excellently with data that is very similar to the training data,
butit finds it difficult to generalize and handle new data. One way to
identify overfitting is to ensure anindependent test set is used that is
completely separate from the training data.

### A.4.2 Underfitting

Underfitting occurs when the model is unable to identify the
relationships between inputs and outputsfrom the training data.
Underfitting usually occurs when there is insufficient training data to
provideenough information to derive the correct relationships between
inputs and outputs (i.e. not enoughfeatures included in the training
data), but it can also occur when the selected algorithm does not fitthe
data (e.g. creating a linear model to work with non-linear data). This
typically leads to a simplisticmodel that makes many incorrect
predictions.

A.5 Data quality
----------------

### A.5.1 Data completeness

Data may be of poor quality if, for example, the input sensors are low
quality or badly calibrated -- andthis is more often a problem when
sensor data comes from multiple sources (e.g. laboratories usingslightly
different measurement approaches or a variety of IoT devices).

Missing data can take three main forms, and each has a different effect
on the resultant model[^89]:

a.  If the data is missed completely at random it should have little
    effect given the probabilistic natureof the model (other than to
    reduce accuracy due to lack of data).

```{=html}
<!-- -->
```
b.  If the data from a particular feature is missed (e.g. all data from
    females) then this is more likely tohave an adverse effect on the
    resultant model (unless the model is not used to make predictions
    forfemales operationally).

```{=html}
<!-- -->
```
c.  Worse still, and most difficult to detect, is the third case, where
    a specific set of data values froma given feature are missing
    (e.g. data from females aged 35 to 50). Such problems often occur
    inmedical studies due to the nature of data collection. In this case
    the model is likely to be severelycompromised.

### A.5.2 Data labelling

Supervised learning assumes that the training data is correct. However,
in practice, it is rare fortraining datasets to be labelled correctly
100 % of the time[^90]. Human labellers can make simplerandom mistakes
(e.g. pressing the wrong button), systemic mistakes (e.g. all labellers
were given thewrong instructions), subjective decisions
(e.g. disagreeing on whether the colour of an object is blueor green)
and there is also the possibility that they make deliberate mistakes.
Labels are not alwayssimple classifications into one of two classes and
more complex labelling tasks may mean the correctlabel is questionable.
Labels in one language may be mis-translated into a second language.
Labellingmay be performed a number of ways, with each approach having
inherent risks to data quality:

- Labelling by internal team
- Outsourced labelling
- Crowdsourced labelling
- Synthetic data generation
- AI-based labelling
- Hybrid approaches

### A.5.3 Distributional shift

A major uncertainty in machine learning is the mismatch between the
training data distribution andthe desired data distribution (this
desired data distribution is typically the operational distribution,but
it may sometimes be an idealized distribution that takes account of
bias; see 5.1.5 for more details).

Where there is a mismatch between these two data distributions, the
predictions provided by themodel will be flawed.

Once operational, the distance between the training data distribution
and the desired data distributionoften increases as the predicted
operational data changes (e.g. users may change as a result of usingthe
system or the system may be used by an unexpected group of users). The
distributional (or dataset)shift describes the divergence of the two
data distributions. Unless specifically programmed, most MLsystems are
not able to identify distributional shift and will provide poor
predictions with high levels ofconfidence. If the shift is significant,
then users will notice and (validly) place less trust in
predictionsprovided by the system.

Ideally the system should be regularly tested to detect any significant
mismatch between the currentoperational distribution and the data
distribution the system was last trained on. If a mismatch isdetected,
then the system may need to be re-trained using updated training data.

A.6 Machine learning algorithm/model selection
----------------------------------------------

There is some controversy whether the selection of the algorithm, the
model settings andhyperparameters is a "science or an art"[^91]. There
is no definitive approach that would allow theselection of the optimal
set purely from an analysis of the problem situation -- in practice this
selectionis nearly always partly by trial and error (as shown in the
explicitly iterative part of the machinelearning workflow in Figure
A.2).

The information needed to make this selection includes knowing what
functionality the model isexpected to provide, what data is available to
the learning algorithm and the model, and what non- functional
requirements must be met.

In terms of functionality, the model is typically going to provide
classification, prediction of a value(regression), detection of
anomalies or determination of structure from data (clustering). Knowing
howmuch data is available may allow certain algorithms to be discarded
(e.g. those that that rely on bigdata can be ignored if less data is
available). If the data is labelled then supervised learning is
possible,otherwise another approach is needed. The number of features
that are expected to be used by themodel will also point to the
selection of certain algorithms, as will the number of expected
classesfor clustering. Non-functional requirements may include
constraints on available memory (e.g. anembedded system), the speed of
prediction (e.g. for real-time systems) and, for some situations,
thespeed of training the model with updated training data. Other
non-functional areas of relevance may bethe need for transparency,
interpretability and explainability (see 5.1.7).

A.7 Documenting ML systems
--------------------------

At present, there is no standard format for documenting machine learning
systems and their datasets.

As ML systems become more widespread and are used for more critical
applications there is a concertedeffort to improve both internal and
external documentation by agreeing on the information that shouldbe
recorded and provided about each ML system. The benefits of clear
documentation include:

- clarification of the intended use cases;
- clarification of performance characteristics;
- facilitation of communication between developers and users;
- improved transparency;
- sharing of insights more quickly;
- more sharing of resources;
- reduced redundancy of efforts;
- reduced misuse of ML models;
- increased reuse.

There are various initiatives in this area, including:

- ABOUT ML (Annotation and Benchmarking on Understanding and
    Transparencyof Machine learning Lifecycles) from the Partnership on
    AI[^92];
- Datasheets for Datasets[^93];
- Google Model Cards[^94];
- IBM Factsheets[^95].

Typical content for the documentation of a ML model could include:

- General -- ID, description, developer details, hardware
    requirements, license details, version, dateand point of contact.
- Usage -- primary use case, typical users, secondary use cases,
    self-learning approach, known bias,ethical issues, safety issues,
    transparency, decision thresholds, platform and performance drift.
- Datasets --collection, availability, pre-processing, use, content,
    labelling, size, privacy, security, bias/fairness and
    restrictions/constraints.
- Training and performance -- ML algorithm, ML structure, validation
    dataset, selection of performancemetrics, thresholds for performance
    metrics, actual performance metrics and performanceefficiency
    measures.
- Testing -- test dataset (description and availability),
    independence, results, robustness, drift andportability.

A.8 Machine learning performance metrics
----------------------------------------

### A.8.1 Introduction to ML performance metrics

Different performance metrics are used to evaluate different Machine
Learning (ML) models. Thisdocument is limited to covering performance
metrics for classification problems (a more detaileddiscussion of ML
performance metrics is provided in ISO/IEC 24029-1[^96]). These metrics
areinitially agreed at the start of the ML workflow and then typically
evaluated in two places in the MLworkflow. For evaluation, they are used
by developers to tune their models (e.g. by the selection
ofhyperparameters) until they achieve an acceptable level of performance
with their evaluation dataset.

The metrics are subsequently used to measure the acceptability of the
performance of the final modelwith the (independent) test set.

### A.8.2 Confusion matrix

Imagine inputs are classified by the ML model as either true or false.
In an ideal world, all data would becorrectly classified, however, in
reality the datasets will occasionally overlap meaning that some
datapoints, which should be classified as true will be wrongly
classified as false (a false negative) and some data points, which
should be classified as false will be wrongly classified as true (a
false positive). Theremaining data points will be correctly classified
as either a true negative or a true positive. These foursets of
possibilities are shown in a confusion matrix[^97].

### A.8.3 Accuracy

Accuracy measures the proportion of all classifications that were
correct, thus the accuracy, A, can becalculated as follows:

    A

    n n

     n n

    true positive true negative
    true positive true
    =

    +
    + nnegative + nfalse positive + nfalse negative

    where
     ntrue positiveis the number of true positives;
     ntrue negativeis the number of true negatives;
     nfalse positiveis the number of false positives;
     nfalse negativeis the number of false negatives.

### A.8.4 Precision

Precision measures the proportion of predicted positives that were
correct (how sure you are of yourpredicted positives), thus the
precision, P, can be calculated as follows:

    P

    n
     n n

    true positive

    true positive false positive

    =

    +

### A.8.5 Recall

Recall (or sensitivity) measures the proportion of actual positives that
were predicted correctly (howsure you have not missed any positives),
thus the recall, R, can be calculated as follows:

    R

    n
     n n

    true positive

    true positive false negative

    =

    +

### A.8.6 F1-score

The F1-score provides a balance (the harmonic average) between recall
and precision, thus the F1- score, SF1, can be calculated as follows:

    S PP R

    F1 R

    = *
    2 * +

### A.8.7 Selection of performance metrics

Different performance metrics are appropriate depending on the
situation. Only the most basic andmost common are covered here. If you
need to measure the correctness of textual outputs (e.g. frommachine
translation) then possible metrics include BLEU, ROUGE and METEOR[^98].

Accuracy is suitable when the datasets are symmetric, for instance, when
the counts of false negativesand false positives are similar.

Precision is most useful when you want to be sure of your true positives
(i.e. we want few or no falsepositives). An example of this could be a
military drone attacking terrorist targets. In this scenario we want no
innocent bystanders to be wrongly identified as terrorists. This means
we want no (or veryfew) false positives -- and so Precision should be
high.

Recall is most useful when catching true positives is important (i.e. we
need to sure of all or mostnegatives). An example of this could be for
an autonomous car sensing people in the road ahead. If thereis a
pedestrian, we want to be sure to identify them and therefore we need no
(or few) false negatives --so Recall needs to be very high.

F1 is most useful when the data distribution is uneven.

These performance metrics will provide the average performance of a ML
model, however, in mostsituations it is also important to assure the
performance of the model in expected worst case scenarios.

Bibliography
============

[^1]: ISO/IEC 22989\[\^1\], 人工知能 - 概念と用語集

[^2]: Wikipedia contributors, \"AI effect\", Wikipedia,
    https://en.wikipedia.org/w/index.php?title=AI_effect&oldid=920651009
    (accessed November 20, 2019).

[^3]: ISO/IEC TR
    24028:2020、情報技術-人工知能-人工知能における信頼性の概要

[^4]: 100+ AI use cases / applications:A COMPREHENSIVE GUIDE, applied
    AI, https://www.appliedai.de/hub/library-of-use-case-families,
    accessed Oct 2020.

[^5]: 2019 Edelman AI Survey Results Report, Edelman Digital,
    https://www.digitalmarketingcommunity .com/ researches/ edelman
    -artificial -intelligence -survey -results-report-2019/ (accessed
    Nov 20, 2019).

[^6]: 2019 Edelman AI Survey Results Report, Edelman Digital,
    https://www.digitalmarketingcommunity .com/ researches/ edelman
    -artificial -intelligence -survey -results-report-2019/ (accessed
    Nov 20, 2019).

[^7]: \"The AI Index 2017 Annual Report\", AI Index Steering Committee,
    Human-Centered AI Initiative,Stanford University, Stanford, CA,
    December 2017.

[^8]: 徹底的に。人工知能2019年版、Statista Report
    2019、https://people.stfx.ca/x2011/x2011aqi/ School/ 2018 -2019/
    Winter/ BSAD %20471 %20 - %20Strat/ Case/ AI %20statista
    .pdf、2019年2月。

[^9]: Agrawal, Gans, Goldfarb et al, \"Prediction Machines:The Simple
    Economics of Artificial Intelligence\", Harvard Business Review
    Press, 2018.

[^10]: レポートW.Q.第10版、ギャップジェミニ、https://www.sogeti.com/explore/reports/world-quality-report-201819/、2018年9月。

[^11]: レポートW.Q.第10版、ギャップジェミニ、https://www.sogeti.com/explore/reports/world-quality-report-201819/、2018年9月。

[^12]: ISO/IEC 22989\[\^1\], 人工知能 - 概念と用語集

[^13]: Robot Density rises globally, IFR Press Release,
    https://ifr.org/ifr-press-releases/news/robot-density-rises-globally,
    Feb 2018.

[^14]: Russel J., Phase Change Memory Shows Promise for AI Use Says IBM,
    HPC Wire, https://www .hpcwire .com/ 2018/ 10/ 24/ phase -change
    -memory -shows -promise -for -ai -use -says -ibm/, 2018年10月.

[^15]: TensorFlow, https://www.tensorflow.org/ (accessed May 2020)

[^16]: PyTorch, https://pytorch.org/ (accessed May 2020)

[^17]: MxNet, https://mxnet.apache.org/ (accessed May 2020)

[^18]: CNTK, https://docs.microsoft.com/en-us/cognitive-toolkit/
    (accessed May 2020)

[^19]: Keras, https://keras.io/ (accessed May 2020)

[^20]: Wikipedia contributors, \"AI effect\", Wikipedia,
    https://en.wikipedia.org/w/index.php?title=AI_effect&oldid=920651009
    (accessed November 20, 2019).

[^21]: ISO/IEC 22989\[\^1\], 人工知能 - 概念と用語集

[^22]: IDC Survey Finds Artificial Intelligence to be a Priority for
    Organizations But Few Have Implemented an Enterpris-Wide Strategy,
    July 2019, https://www.idc.com/getdoc.jsp?containerId=prUS45344519,
    accessed Jan 2020.

[^23]: テストの現状報告。2019年、バージョン1.3、PractiTest、https://qablog.practitest.com/state-of-testing/、2019年7月。

[^24]: ISTQB® Worldwide Software Testing Practices Report 2017-18
    (Revised), ISTQB, https://www.istqb .org/ references/ surveys/ istqb
    %C2 %AE -worldwide -software -testing -practices
    -survey-2017-18.html, accessed October 2018.

[^25]: Hackett M., 2018 Trends Survey Results,
    https://www.logigear.com/magazine/survey/2018-trends-survey-results/,
    accessed Nov 2019.

[^26]: テストの現状報告。2019年、バージョン1.3、PractiTest、https://qablog.practitest.com/state-of-testing/、2019年7月。

[^27]: レポートW.Q.第10版、ギャップジェミニ、https://www.sogeti.com/explore/reports/world-quality-report-201819/、2018年9月。

[^28]: The Second IEEE International Conference On Artificial
    Intelligence Testing,
    http://ieeeaitests.com/html/callForPapers.html.

[^29]: ISO/IEC TR
    24028:2020、情報技術-人工知能-人工知能における信頼性の概要

[^30]: DIN SPEC 92001-2, Artificial Intelligence - Life Cycle Processes
    and Quality Requirements - Part 2: Technical and Organizational
    Requirements,
    https://www.din.de/en/innovation-and-research/din-spec-en/projects/wdc-proj:din21:298702628,
    accessed Jan 2020.

[^31]: DIN SPEC 92001-1, Artificial Intelligence - Life Cycle Processes
    and Quality Requirements - Part 1:Quality Meta Model,
    https://www.din.de/en/wdc-beuth:din21:303650673, accessed Jan 2020.

[^32]: IEEE 7000, IEEE Draft Model Process for Addressing Ethical
    Concerns During System Design

[^33]: ISO/IEC TR 24368\[\^5\], 情報技術 - 人工知能 -
    倫理的・社会的関心事の概要

[^34]: ONNX, Open Neural Network Exchange, https://onnx.ai/ (accessed
    May 2020)

[^35]: NNEF, Neural Network Exchange Format,
    https://www.khronos.org/nnef/ (accessed May 2020)

[^36]: PMML。Predictive Model Markup Language,
    http://dmg.org/pmml/v4-4/GeneralStructure.html(accessed May 2020)

[^37]: A guide to using artificial intelligence in public sector, June
    2019, UK Government DigitalService,
    https://www.gov.uk/government/publications/understanding-artificial-intelligence/a-guide-to-using-artificial-intelligence-in-the-public-sector
    (accessed May 2020).

[^38]: IEC 61508、機能安全、IEC 61508

[^39]: ISO 26262 (all parts), ロードビークル - 機能安全

[^40]: 自動運転システム（ADS）。A Vision for Safety 2.0,
    https://www.nhtsa.gov/sites/nhtsa.dot.gov/files/documents/13069a-ads2.0_090617_v9a_tag.pdf
    (accessed May 2020)

[^41]: 自律走行車等の評価のための安全基準の提示https://ul.org/UL4600
    (accessed May 2020)

[^42]: DIN SPEC 92001-1, Artificial Intelligence - Life Cycle Processes
    and Quality Requirements - Part 1:Quality Meta Model,
    https://www.din.de/en/wdc-beuth:din21:303650673, accessed Jan 2020.

[^43]: ISO/IEC/IEEE
    12207:2017、システムおよびソフトウェアエンジニアリング-ソフトウェアライフサイクルプロセス

[^44]: DIN SPEC 92001-2, Artificial Intelligence - Life Cycle Processes
    and Quality Requirements - Part 2: Technical and Organizational
    Requirements,
    https://www.din.de/en/innovation-and-research/din-spec-en/projects/wdc-proj:din21:298702628,
    accessed Jan 2020.

[^45]: ISO/IEC
    25010:2011、システム及びソフトウェア工学-システム及びソフトウェアの品質要求及び評価（SQuaRE）-システム及びソフトウェアの品質モデル

[^46]: ISO/IEC
    5059\[\^4\],ソフトウェアエンジニアリング-システムおよびソフトウェアの品質要求と評価（SQuaRE）-AIベースのシステムの品質モデル

[^47]: ISO/IEC
    25010:2011、システム及びソフトウェア工学-システム及びソフトウェアの品質要求及び評価（SQuaRE）-システム及びソフトウェアの品質モデル

[^48]: Marir et al, , QM4MAS: a quality model for multi-agent systems,
    Int.J. Computer Applications inTechnology, 2016, 54.

[^49]: Salkever et al. A.I. Bias Isn\'t the Problem.Our Society Is,
    Fortune.com,
    https://fortune.com/2019/04/14/ai-artificial-intelligence-bias/,
    accessed Nov 2019.

[^50]: ISO/IEC TR 24027\[\^6\],
    情報技術-人工知能（AI）-AIシステムにおけるバイアスおよびAIによる意思決定支援

[^51]: Explainable AI: the basics - Policy Briefing, The Royal Society,
    Nov 2019.

[^52]: Wikipedia contributors, \"Explainable artificial intelligence,\"
    Wikipedia,
    https://en.wikipedia.org/w/index.php?title=Explainable_artificial_intelligence&oldid=924090418
    (accessedNovember 20, 2019).

[^53]: Increasing transparency with Google Cloud Explainable AI, Product
    News, https://cloud.google.com/ blog/ products/ ai -machine
    -learning/ google -cloud -ai -explanations -to -increase
    -fairness-responsibility-and-trust, accessed Nov 2019, Nov 2019.

[^54]: ISO/IEC TR
    24028:2020、情報技術-人工知能-人工知能における信頼性の概要

[^55]: Russell, Of Myths and Moonshine, contribution to the conversation
    on The Myth of AI,
    https://www.edge.org/conversation/jaron_lanier-the-myth-of-ai,
    accessed Nov 2019.

[^56]: Bird J. et al. \"The evolved radio and its implications for
    modelling the evolution of novel sensors.\"Proceedings of the 2002
    Congress on Evolutionary Computation.CEC\'02.

[^57]: Awad et al, , The Moral Machine experiment, Nature, 563, pages
    59-64, 2018.

[^58]: European Commission High-Level Expert Group on Artificial
    Intelligence, Ethics Guidelines forTrustworthy AI, European
    Commission, April 2019.

[^59]: Banks et al. Requirements Assurance in Machine Learning,
    Proceedings of the AAAI Workshopon Artificial Intelligence Safety
    2019, Jan 2019.

[^60]: ISO/IEC 20889:2018「Privacy Enhanced Data De-identification
    terminology and classification
    oftechniques（プライバシー強化のためのデータ非識別化の用語および技術の分類

[^61]: Leetaru, How Twitter Corrupted Microsoft\'s Tay,
    https://www.forbes.com/sites/kalevleetaru/2016/ 03/ 24/ how -twitter
    -corrupted -microsofts -tay -a -crash -course -in -the -dangers -of
    -ai -in -the-real-world/\#202233ae26d2, 2016年3月.

[^62]: ISO/IEC
    25010:2011、システム及びソフトウェア工学-システム及びソフトウェアの品質要求及び評価（SQuaRE）-システム及びソフトウェアの品質モデル

[^63]: Qiu et al, , Review of Artificial Intelligence Adversarial Attack
    and Defense Technologies, AppliedSciences 9(5):909, Mar 2019.

[^64]: Training Benchmarks M.L. Website: https://mlperf.org/, accessed
    Nov 2019.

[^65]: Stanford DAWN Deep Learning Benchmark.ウェブサイト:
    https://dawn.cs.stanford.edu/benchmark/, accessed Nov 2019.

[^66]: オントロジー・アライメント評価イニシアチブ。ウェブサイト:
    http://oaei.ontologymatching.org/, accessedNov 2019.

[^67]: Kuhn et al, , Software Fault Interactions and Implications for
    Software Testing, IEEE Transactionson Software Engineering 30(6):418
    - 421, July 2004.

[^68]: ISO/IEC/IEEE
    29119-4「ソフトウェアおよびシステムエンジニアリング-ソフトウェアテスト-第4部：テストテクニック」。

[^69]: Kuhn et al, , Software Fault Interactions and Implications for
    Software Testing, IEEE Transactionson Software Engineering 30(6):418
    - 421, July 2004.

[^70]: Wikipedia contributors, \"A/Bテスト,\" Wikipedia,
    https://en.wikipedia.org/w/index.php?title=A/B_testing&oldid=926805728
    (accessed November 21, 2019).

[^71]: Chen et al, , Metamorphic Testing:A Review of Challenges and
    Opportunities, ACM Comput.Surv.51, 1, Article 4, January 2018.

[^72]: Segura et al, , A Survey on Metamorphic Testing, IEEE Trans. on
    Software Engineering, Vol 42,No.9, Sept 2016.

[^73]: Liu et al, , How effectively does metamorphic testing alleviate
    oracle problem?, IEEETransactions on Software Engineering 40, 1,
    4-22, 2014.

[^74]: Pei, K., et al, DeepXplore:Automated Whitebox Testing of Deep
    Learning Systems Commun.ACM,Association for Computing Machinery,
    2019, 62, 137-145 p.

[^75]: Pei et al, DeepXplore:Automated Whitebox Testing of Deep Learning
    Systems, SOSP \'17, October28, 2017, Shanghai, China.

[^76]: Pei et al, DeepXplore:Automated Whitebox Testing of Deep Learning
    Systems, SOSP \'17, October28, 2017, Shanghai, China.

[^77]: Sun et al. Testing Deep Neural Networks,
    https://www.researchgate.net/publication/323747173_Testing_Deep_Neural_Networks,
    accessed Nov 2019, Mar 2018.

[^78]: Sun et al. Testing Deep Neural Networks,
    https://www.researchgate.net/publication/323747173_Testing_Deep_Neural_Networks,
    accessed Nov 2019, Mar 2018.

[^79]: Sun et al. Testing Deep Neural Networks,
    https://www.researchgate.net/publication/323747173_Testing_Deep_Neural_Networks,
    accessed Nov 2019, Mar 2018.

[^80]: Sun et al. Testing Deep Neural Networks,
    https://www.researchgate.net/publication/323747173_Testing_Deep_Neural_Networks,
    accessed Nov 2019, Mar 2018.

[^81]: Pei et al, DeepXplore:Automated Whitebox Testing of Deep Learning
    Systems, SOSP \'17, October28, 2017, Shanghai, China.

[^82]: Tian et al. DeepTest:Automated Testing of
    Deep-Neural-Network-driven Autonomous Cars, ICSE\'18: 40th
    International Conference on Software Engineering, May 2018.

[^83]: Sun et al. Testing Deep Neural Networks,
    https://www.researchgate.net/publication/323747173_Testing_Deep_Neural_Networks,
    accessed Nov 2019, Mar 2018.

[^84]: Nokia\'s revolutionary 5G virtual testing speeds deployment,
    Press Release, https://www.globenewswire .com/ news -release/ 2019/
    04/ 23/ 1807667/ 0/ en/ Nokia -s -revolutionary
    -5G-virtual-testing-speeds-deployment.html, accessed Nov 2019, April
    2019.

[^85]: Official documentation for MORSE project,
    http://www.openrobots.org/morse/doc/0.2.1/morse.html, accessed Nov
    2019.

[^86]: Savva et al. Open-sourcing AI Habitat, an advanced simulation
    platform for embodied AIresearch, https://arxiv.org/abs/1904.01201,
    Nov 2019.

[^87]: NVIDIA DRIVE CONSTELLATION - Virtual Reality Autonomous Vehicle
    Simulator。NVIDIAProducts,
    https://www.nvidia.com/en-us/self-driving-cars/drive-constellation/,
    accessedNov 2019.

[^88]: ISO/IEC 23053\[\^2\],
    機械学習（ML）を用いた人工知能（AI）システムのフレームワーク

[^89]: Keevers, Cross-validation is insufficient for model validation,
    Technical Report, AustralianDefence Science and Technology Group,
    Mar 2019.

[^90]: Frenay et al, , Classification in the Presence of Label Noise:A
    Survey, IEEE Transactions onNeural Networks and Learning Systems,
    May 2014.

[^91]: Henderson et al. Deep reinforcement learning that matters,
    Thirty-Second AAAI Conference onArtificial Intelligence, 2018.

[^92]: Annotation and Benchmarking on Understanding and Transparency of
    Machine LearningLifecycles (ABOUT ML), v0 Final Draft,
    https://www.partnershiponai.org/wp-content/uploads/2019/07/ABOUT-ML-v0-Draft-Final.pdf,
    accessed Jan 2020.

[^93]: Gebru T. et al. Datasheets for Datasets,
    https://arxiv.org/pdf/1803.09010.pdf, accessed Jan 2020.

[^94]: Mitchell M. et al. Model Cards for Model Reporting,
    https://arxiv.org/abs/1810.03993, accessedJan 2020.

[^95]: Arnold M. et al. FactSheets:Increasing Trust in AI Services
    through Supplier\'s Declarations ofConformity,
    https://arxiv.org/abs/1808.07261, accessed Jan 2020.

[^96]: ISO/IEC TR 24029-1\[\^3\], 人工知能（AI） -
    ニューラルネットワークの堅牢性の評価 - 第1部:概要

[^97]: Wikipedia contributors, \"Confusion matrix,\" Wikipedia,
    https://en.wikipedia.org/w/index.php?title=Confusion_matrix&oldid=922488584
    (accessed November 21, 2019).

[^98]: Raj, Metrics for NLG evaluation, Medium.com,
    https://medium.com/explorations-in-language-and-learning/metrics-for-nlg-evaluation-c89b6a781054,
    accessed Nov 2019.
