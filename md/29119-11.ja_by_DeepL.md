# ソフトウェアおよびシステムエンジニアリング - ソフトウェアテスト - 第11部：AIベースのシステムのテストに関するガイドライン

[TOC]

## 序文

ISO（International Organization for Standardization：国際標準化機構）とIEC（International ElectrotechnicalCommission：国際電気標準会議）は、世界的な標準化のための専門組織です。ISOまたはIECに加盟している国の機関は、それぞれの機関が技術活動の特定分野を扱うために設立した技術委員会を通じて、国際規格の策定に参加します。ISOとIECの技術委員会は、相互に関心のある分野で協力しています。その他の国際機関、政府機関、非政府機関も、ISOやIECと連携して作業に参加しています。

この文書を作成するために使用された手順と、今後のメンテナンスのための手順は、ISO/IEC専門業務用指針第1部に記載されています。特に、文書の種類によって必要とされる承認基準が異なることに留意すべきである。本文書は、ISO/IEC専門業務用指針第2部の編集規則に従って作成されている（www.iso.org/directives
参照）。

この文書のいくつかの要素が特許権の対象となる可能性があることに注意を喚起する。ISO及びIECは，このような特許権の一部または全部を特定する責任を負わない。本文書の作成中に確認された特許権の詳細は，序文，及び／または，ISOの特許宣言リスト（
www.iso.org/patents 参照），IECの特許宣言リスト（patents.IEC.ch
参照）に記載される。

この資料で使用されている商標名は、ユーザーの便宜のために提供されている情報であり、推奨を意味するものではありません。

規格の任意性、適合性評価に関するISO特有の用語や表現の意味、TBT（Technical
Barriers to
Trade：貿易の技術的障害）における世界貿易機関（WTO）の原則へのISOの準拠についての説明は、www.iso.org/iso/foreword.html。

この文書は，Joint Technical Committee ISO/IEC JTC 1, Information
Technology, Subcommittee SC 7, Software and systems engineering
によって作成された。

ISO/IEC/IEEE
29119シリーズの全部品のリストは、ISOのウェブサイトに掲載されています。

この文書に関するご意見やご質問は、ユーザーの国の規格機関にお寄せください。これらの機関の完全なリストは
www.iso.org/members.html でご覧いただけます。

## はじめに

従来のシステムのテストはよく理解されていますが、日常生活に欠かせないものとして普及しつつあるAIベースのシステムには、新たな課題があります。このドキュメントは、AIベースのシステムを紹介し、どのようにテストすべきかのガイドラインを提供するために作成されました。

付録Aでは、機械学習の紹介をしています。

このドキュメントは、主にAIベースのシステムに初めて触れるテスターのために提供されていますが、経験豊富なテスターや、AIベースのシステムの開発やテストに携わるその他の関係者にも役立ちます。

本資料は、技術報告書として、通常の国際規格や技術仕様書として発行されるデータとは異なる種類のデータ、例えば「技術の現状」に関するデータを含んでいます。

## 1 スコープ

このドキュメントでは、AIベースのシステムについて紹介しています。これらのシステムは一般的に複雑で（例：ディープニューラルネット）、時にはビッグデータに基づいており、仕様が不十分であったり、非決定論的であったりするため、テストに新たな課題や機会をもたらします。

このドキュメントでは、AIベースのシステムに特有の特性を説明し、それに対応してAIベースのシステムの受け入れ基準を規定することの難しさを説明しています。

このドキュメントでは、AIベースのシステムをテストする際の課題を提示しています。主な課題は、テスト担当者がテストの期待される結果を判断することが難しく、その結果、テストが成功したのか失敗したのかを判断することができないというテストラクル問題です。この本は、AIベースのシステムのテストをライフサイクル全体にわたってカバーしており、ブラックボックス・アプローチを用いたAIベースのシステム全般のテスト方法に関するガイドラインと、ニューラルネットワークに特化したホワイトボックス・テストについて紹介しています。また、AIベースのシステムのテストに使用するテスト環境やテストシナリオのオプションについても説明しています。

本ドキュメントでは、AIベースのシステムとは、少なくとも1つのAIコンポーネントを含むシステムのことです。

## 2 基準となる文献

このドキュメントには、規範となる文献はありません。

## 3 用語、定義、略語

## 3.1 用語と定義

この文書では、以下の用語と定義が適用されます。

ISOとIECは、標準化に使用するための用語データベースを以下のアドレスで公開しています。

- ISOオンライン閲覧プラットフォーム： https://www.iso.org/obp
    で利用可能
- IEC Electropedia: http://www.electropedia.org/ で入手可能。

#### 3.1.1 A/Bテスト

スプリットランテスト

2つのシステムやコンポーネントのうち、どちらの性能が優れているかをテスト担当者が判断するための統計的テスト手法。

#### 3.1.2 精度

\<機械学習（3.1.43）\>
分類器（3.1.21）の評価に使用される性能指標で、分類（3.1.20）の予測（3.1.56）が正しかった割合を測定する。

#### 3.1.3 活性化関数

伝達関数

ニューラルネットワーク（3.1.48）＞
ニューラルネットワークのノードに関連する式で、ニューロンへの入力からノードの出力（活性化値（3.1.4））を決定する。

#### 3.1.4 アクティベーション値

\<ニューラルネットワーク（3.1.48）\>
ニューラルネットワークのノードの活性化関数（3.1.3）の出力

#### 3.1.5 適応性

機能要件と非機能要件の両方を満たし続けるために、環境の変化に対応するシステムの能力

#### 3.1.6 アドバーサリアル・アタック

MLモデル(3.1.46)を失敗させるために、敵対的な例題(3.1.7)を意図的に使用する。

エントリーへの注釈1。一般的には、ニューラルネットワーク形式のMLモデルを対象とする（3.1.48）。

#### 3.1.7 敵対的な例

MLモデル(3.1.46)への入力で、作業例に小さな摂動を加えることで、モデルが誤った結果を高い信頼度で出力することができる。

エントリへの注記1。一般的には、ニューラルネットワーク形式のMLモデルに適用される（3.1.48）。

#### 3.1.8 敵対的なテスト

MLモデル（3.1.46）の欠陥を特定するために、敵対的な例の作成と実行を試みる（3.1.7）ことに基づくテストアプローチ

エントリへの注記1。一般的にはニューラルネットワーク形式のMLモデルに適用される（3.1.48）。

#### 3.1.9 AIを使ったシステム

AIを実装する1つまたは複数のコンポーネントを含むシステム(3.1.13)

#### 3.1.10 AIの効果

以前はAI（3.1.13）と表示されていたシステムが、技術の進歩によりAIとは言えなくなった場合。

#### 3.1.11 AI品質メタモデル

AIベースのシステムの品質を保証するためのメタモデル(3.1.9)

エントリーへの注記1このメタモデルは DIN SPEC 92001
で詳細に定義されています。

#### 3.1.12 アルゴリズム

MLアルゴリズム

\<機械学習(3.1.43)\>
学習データ(3.1.80)からMLモデル(3.1.46)を作成するためのアルゴリズム

例として、MLアルゴリズムには、線形回帰、ロジスティック回帰、決定木（3.1.25）、SVM、NaiveBayes、kNN、K-means、ランダムフォレストなどがあります。

#### 3.1.13 人工知能

AI

知識や技能を習得し、処理し、適用するエンジニアリング・システムの能力

#### 3.1.14 自律システム

人の手を煩わせることなく持続的に動作することができるシステム

#### 3.1.15 オートノミー

人が介在しなくてもシステムが持続的に動作する能力

#### 3.1.16 バックトゥバック・テスト

ディファレンシャル・テスト

システムの代替バージョンを擬似的に使用して（3.1.59）、同じテスト入力から比較のために期待される結果を生成するというテストのアプローチ。

例
擬似オラクルは、既に存在するシステムでも、独立したチームが開発したシステムでも、異なるプログラミング言語で実装されたシステムでもよい。

#### 3.1.17 バックワード・プロパゲーション

\<ニューラルネットワーク（3.1.48）\>
人工的なニューラルネットワークにおいて、ネットワークの出力で計算されたエラーに基づいて、ネットワークの接続に使用する重みを決定するために使用される方法。

エントリーへの注釈1。ディープニューラルネットワークの学習に使用されます（3.1.27）。

#### 3.1.18 ベンチマーク・スイート

ベンチマークの集合体で、ベンチマークとは代替品の性能を比較するためのテストの集合体です。

#### 3.1.19 バイアス

\<機械学習(3.1.43)\>
MLモデル(3.1.46)が提供する予測値と、望ましい公正な予測値(3.1.56)との間の距離の尺度

#### 3.1.20 分類

\<機械学習（3.1.43）\>
与えられた入力に対して出力クラスを予測する機械学習関数

#### 3.1.21 クラシファイア

\<機械学習(3.1.43)\> 分類(3.1.20)に用いるMLモデル(3.1.46)

#### 3.1.22 クラスタリング

同じグループ（すなわちクラスタ）内のオブジェクトが他のクラスタ内のオブジェクトよりも互いに類似しているように、オブジェクトのセットをグループ化すること。

#### 3.1.23 コンビナトリアル・テスト

いくつかのパラメータの値の特定の組み合わせを実行するようにテストケースを設計するブラックボックステスト設計技法
(3.1.53)

例：ペアワイズテスト（3.1.52）、全組み合わせテスト、各選択肢テスト、基本選択肢テスト。

#### 3.1.24 コンフュージョン・マトリクス

真値と偽値がわかっているテストデータ（3.1.75）に対する分類器（3.1.21）の性能を表すために使用される表。

#### 3.1.25 決定木

\<機械学習(3.1.43)\>
1つ以上のツリー状の構造を辿ることで推論を表現できる教師付き学習モデル(3.1.46)

#### 3.1.26 深層学習

1つ以上の隠れた層を持つニューラルネットワーク(3.1.48)を学習することで、豊かな階層的表現を生み出すアプローチ。

エントリーへの注釈1深層学習では、単純な計算ユニット（ニューロン）を多層化したネットワークを使用します。ニューロンネットワークでは、各ユニットが入力値を組み合わせて出力値を生成し、その出力値が下流の他のニューロンに伝えられます。

#### 3.1.27 ディープニューラルネット

2層以上のニューラルネットワーク (3.1.48)

#### 3.1.28 決定論的なシステム

特定の入力セットと開始状態が与えられたときに、常に同じ出力セットと最終状態を生成するシステム

#### 3.1.29 ディストリビューション・シフト

dataset shift \<機械学習（3.1.43）\>
訓練データ（3.1.80）の分布とdesireddataの分布との距離。

エントリへの注記1。ユーザーのシステムとの関わり方（および希望するデータ分布）が時間とともに変化すると、分布のずれの影響が大きくなることが多い。

#### 3.1.30 ドリフト

劣化の恒常性＜機械学習（3.1.43）＞時間の経過に伴うMLモデル（3.1.46）の挙動の変化

エントリーへの注記1これらの変更は一般的に予測（3.1.56）の精度を低下させ、新しいデータでモデルを再トレーニングする必要があるかもしれません。

#### 3.1.31 説明可能性

\<AI（3.1.13）\>
AIを使ったシステム（3.1.9）がどのようにして与えられた結果を導き出したのかを理解するレベル

#### 3.1.32 探索的テスト

経験ベースのテストとは、テスト者の既存の関連知識、テスト項目の事前調査（過去のテストの結果を含む）、一般的なソフトウェアの動作や障害の種類に関するヒューリスティックな「経験則」に基づいて、テスト者が自発的にテストを設計・実行するものです。

エントリへの注記
1:探索的テストでは、隠された特性（隠された動作を含む）を探し出します。これらの特性は、それ自体は無害かもしれませんが、テスト対象のソフトウェアの他の特性に干渉する可能性があり、ソフトウェアが失敗するリスクとなります。

#### 3.1.33 F1スコア

\<機械学習（3.1.43）\>
分類器（3.1.21）の評価に使用される性能指標で、回収率（3.1.61）と精度（3.1.55）のバランス（調和的平均）を提供する。

#### 3.1.34 フォルス・ネガティブ

実際には合格しているのに、失敗したと誤って報告すること

エントリーへの注釈1。これはType IIエラーとも呼ばれています。

EXAMPLE
レフリーは、ゴールだったときにオフサイドを裁定し、そのため、アゴラが得点されたときにゴールの失敗を報告する。

#### 3.1.35 偽陽性

実際には失敗であるにもかかわらず、パスであると誤って報道したこと。

エントリーへの注釈1。これはType Iエラーとも呼ばれます。

例）レフリーは、オフサイドであったため、与えられるべきではなかったゴールを与える。

#### 3.1.36 フィーチャーエンジニアリング

特徴選択（機械学習（3.1.43））モデル（3.1.46）に現れるべき基本的な関係を最もよく表している生データの属性を、トレーニングデータ（3.1.80）に使用するために特定する活動。

#### 3.1.37 柔軟性

最初の仕様以外の状況で動作するシステムの能力（目的を満たすために実際の状況に応じて動作を変更すること）。

#### 3.1.38 ファズテスト

テスト項目の入力を生成するために、ファズと呼ばれる大量のランダムな（またはランダムに近い）データを使用するソフトウェアテスト手法。

#### 3.1.39 一般的なAI

強力なAI AI（3.1.13）
認知能力の全範囲にわたって人間に匹敵する知的行動を示す。

#### 3.1.40 グラフィカル・プロセッシング・ユニット

画像の描画などのディスプレイ機能に特化したGPU ASIC（Application Specific
Integrated Circuit）。

エントリーへの注釈1。GPUは、単一の機能を持つ画像の並列データ処理のために設計されていますが、この並列処理は、ニューラルネットワークなどのAIベースのソフトウェアの実行にも有効です（3.1.48）。

#### 3.1.41 ハイパーパラメータ

\<ニューラルネットワーク（3.1.48）\>
ニューラルネットワークの構造とその学習方法を定義するための変数

エントリーへの注記1通常、ハイパーパラメータはモデルの開発者によって設定され（3.1.46）、チューニングパラメータと呼ばれることもあります（3.1.53）。

#### 3.1.42 解釈可能性

\<AI（3.1.13）\>基盤となる（AI）技術がどのように機能するかを理解しているレベル

#### 3.1.43 機械学習

データや経験からシステムが学ぶことを可能にする計算技術を用いたMLプロセス

#### 3.1.44 メタモルフィックな関係

ソーステストケースからフォローアップテストケースへのテストインプットの変化が、ソーステストケースからフォローアップテストケースへの期待されるアウトプットにどのような変化を与えるか（または与えないか）の説明。

#### 3.1.45 メタモルフィック・テスト

期待される結果が仕様書に基づくものではなく、過去の実績から推測されるテスト

#### 3.1.46モデル

MLモデル＜機械学習（3.1.43）＞
入力データのパターンを利用して予測（3.1.56）を行うトレーニングデータセットで学習されたMLアルゴリズム（3.1.12）の出力。

#### 3.1.47 ナローAI

weak AI AI (3.1.13)
特定の問題に対処するために、明確に定義された単一のタスクに集中する

#### 3.1.48 ニューラルネットワーク

人工ニューラルネットワーク
重みを調整できるリンクで接続された原始的な処理要素からなるネットワークで、各要素が入力値に非線形関数を適用して値を生成し、それを他の要素に伝達したり、出力値として提示したりするもの。

エントリへの注記1。神経系のニューロンの働きをシミュレートすることを目的としたニューラルネットワークもありますが、ほとんどのニューラルネットワークは人工知能（3.1.13）においてコネクショニストモデル（3.1.46）を実現するものとして使われています。

エントリーへの注釈2非線形関数の例としては、閾値関数、シグモイド関数、多項式関数などがあります。\[SOURCE:ISO/IEC
2382:2015, 2120625, 修正-認められていた用語 \"ニューラルネット
\"が削除された；項目への注3から5が削除された\]。

#### 3.1.49 ニューロンのカバレッジ

一連のテストにおいて、活性化したニューロンの割合をニューラルネットワークのニューロン総数で割ったもの(3.1.48)(通常はパーセンテージで表される)

エントリーへの注記1活性化値（3.1.4）がゼロを超えると、ニューロンは活性化しているとみなされます。

#### 3.1.50 非決定論的システム

特定の入力セットと開始状態が与えられても、常に同じ出力セットと最終状態を生み出すとは限らないシステム。

#### 3.1.51 オーバーフィッティング

\<機械学習(3.1.43)\>
訓練データ(3.1.80)と密接に対応するMLモデル(3.1.46)を生成し、その結果、新しいデータに一般化することが困難なモデルとなる。

#### 3.1.52 ペアワイズテスト

ブラックボックステスト設計技法で、入力パラメータの各ペアの可能なすべての離散的な組み合わせを実行するようにテストケースを設計すること（3.1.53

エントリーへの注記1ペアワイズテストはコンビナートテストの中でも最もポピュラーな形式である（3.1.23）。

#### 3.1.53 パラメータ

\<機械学習(3.1.43)\>
学習データ(3.1.80)をアルゴリズム(3.1.12)に適用して学習されたモデル(3.1.46)の部分

EXAMPLE ニューラルネットで学習した重み。

エントリーへの注釈1一般的に、パラメータはモデルの開発者が設定するものではありません。

#### 3.1.54 パフォーマンス・メトリクス

\<機械学習(3.1.43)\>
分類(3.1.20)に用いられるMLモデル(3.1.46)の評価に用いられるメトリクス

例
代表的な評価指標としては、accuracy（3.1.2）、precision（3.1.55）、recall（3.1.61）、F1-score（3.1.33）などがあります。

#### 3.1.55 精度

\<機械学習（3.1.43）\>
分類器（3.1.21）の評価に使用される性能指標で、予測された陽性が正しかった割合を測定します。

#### 3.1.56の予測

\<機械学習（3.1.43）\>
与えられた入力に対して予測された目標値を得るための機械学習機能

EXAMPLE 分類（3.1.20）と回帰（3.1.62）の機能を含む。

#### 3.1.57 前処理

\<機械学習（3.1.43）\>
生データをMLアルゴリズム（3.1.12）で使用可能な状態に変換し、MLモデル（3.1.46）を作成するMLワークフローの一部。

項目への注記1前処理には、分析、正規化、フィルタリング、再フォーマット、インピュテーション、外れ値や重複の除去、データセットの完全性の確保などが含まれる。

#### 3.1.58 確率論的システム

出力を完全には予測できないような確率で動作が記述されるシステム。

#### 3.1.59 疑似オラクル

派生テストオラクル
同じテスト入力に基づいてオリジナルのテスト項目の結果と比較される結果を生成するために使用される、テスト項目の独立した派生バリアント。

エントリーへの注記1疑似手口は、従来のテスト手口（3.1.76）が利用できない場合の有用な代替手段です。

#### 3.1.60 推論技術

\<AI (3.1.13)\>
利用可能な情報から、演繹法や帰納法などの論理的手法を用いて結論を導き出すAIの形態。

#### 3.1.61 リコール

感度 \<機械学習（3.1.43）\>
分類器（3.1.21）の評価に使用されるパフォーマンス指標で、正しく予測された実際の陽性の割合を測定する。

#### 3.1.62 リグレッション

\<機械学習（3.1.43）\>
与えられた入力に対して、数値または連続した出力値を得る機械学習機能

#### 3.1.63 規制基準

規制機関が公布した基準

#### 3.1.64 強化学習

機械学習（3.1.43）＞目的を達成するために試行錯誤を繰り返しながらMLモデル（3.1.46）を構築する作業。

エントリへの注記1。強化学習タスクには、教師付き学習（3.1.74）に似た方法でのMLモデルの学習と、AI（3.1.13）システムの運用段階で収集されたラベルのない入力の学習が含まれます。モデルが予測(3.1.56)を行うたびに報酬が計算され、その報酬を最適化するためにさらなる試行が行われます。

エントリへの注記2。強化学習では、目的（成功の定義）をシステム設計者が定義することができます。

エントリへの注記3。強化学習では、報酬は、AIシステムが与えられた試行において目的を達成するためにどれだけ近づいたかを示す計算された数値とすることができます。

#### 3.1.65 リワードハッキング

本来の目的を達成することを犠牲にして，報酬関数を最大化するためにエージェントが行う活動。

#### 3.1.66 ロボット

ある程度の自律性（3.1.15）を持ち、環境の中で動き、意図されたタスクを実行するプログラムされた作動機構

エントリーへの注釈1ロボットには、制御システムと制御システムのインターフェースが含まれます。

項目への注記2。ロボットの産業用ロボットとサービスロボットへの分類（3.1.20）は、その目的とする用途に応じて行われる。

#### 3.1.67 安全性

定義された条件の下で、システムが、人の生命、健康、財産、または環境が危険にさらされる状態に至らないことを期待すること【出典：ISO/IEC/IEEE
12207:2017, 3.1.48】。

#### 3.1.68 検索アルゴリズム

\<AI（3.1.13）\>
ゴールとなる状態（または構造）に到達するまで、すべての可能な状態（または構造）のサブセットを系統的に訪れるアルゴリズム（3.1.12）。

#### 3.1.69 自己学習システム

試行錯誤しながら学習していくことで行動を変えていく適応システム

#### 3.1.70 サインチェンジの適用範囲

正負両方の活性化値で活性化されたニューロンの割合（3.1.4）をニューラルネットワークのニューロン総数（3.1.48）で割ったもの（通常はパーセンテージで表される）で、一連のテストについて。

エントリーへの注記1活性化値がゼロの場合は、負の活性化値とみなされます。

#### 3.1.71 サイン・サインカバー

各ニューロンの符号を変化させることにより、次の層のニューロンの符号が変化し、次の層の他のニューロンは変化しない（すなわち、符号を変化させない）ことが個別に示される場合、カバレッジレベルが達成される。

#### 3.1.72 シミュレータ

一連の制御された入力が与えられたときに、実際のシステムのように動作する、テスト時に使用される装置、コンピュータプログラム、システム。

#### 3.1.73 ソフトウェアエージェント

環境を認識し、目標達成の可能性を最大化するような行動をとるデジタルエンティティ

#### 3.1.74 教師あり学習

\<機械学習(3.1.43)\>
ラベル付けされたサンプルの入出力ペアに基づいて、入力を出力にマッピングする関数を学習するタスクです。

#### 3.1.75 テストデータ

\<機械学習（3.1.43）\>
最終的にチューニングされたMLモデル（3.1.46）を偏りなく評価するための独立したデータセット。

#### 3.1.76 テスト・オラクル

試験の合否を判定するための情報源。

項目への注記1テストオラクルは、多くの場合、個々のテストケースの期待される結果を生成するために使用される仕様ですが、実際の結果を他の類似したプログラムやシステムの結果と比較したり、人間の専門家に尋ねたりするなど、他のソースを使用することもできます。

#### 3.1.77 テスト・オラクル問題

与えられたテスト入力と状態のセットに対して、テストが合格か不合格かを判断するという課題

#### 3.1.78 テストシナリオ

テストケースを生成するための基礎となるテスト項目の状況や設定

#### 3.1.79 スレッショルドカバレッジ

ニューラルネットワーク（3.1.48）＞テストセットにおいて、閾値（3.1.4）を超える活性化値を持つニューロンの割合を、ニューラルネットワーク内のニューロンの総数で割った値（通常はパーセンテージで表される）。

エントリーへの注釈1。閾値としては、0から1の間の閾値活性化値が選ばれます。

#### 3.1.80 トレーニングデータ

\<機械学習 (3.1.43)\> MLモデルの学習に使用するデータセット (3.1.46)

#### 3.1.81 透過性

\<AI(3.1.13)\>アルゴリズム(3.1.12)やAIベースのシステムが使用するデータ(3.1.9)へのアクセス可能性のレベル

#### 3.1.82 真のネガティブ

失敗したときの正しい報告

例）レフリーがオフサイドを正しく判定したため、ゴールの失敗を報告する。

#### 3.1.83 トゥルー・ポジティブ

パスである場合の正しい報道

例）レフリーが正しくゴールを決めた。

#### 3.1.84 アンダーフィッティング

\<機械学習（3.1.43）\>
学習データの基本的な傾向を反映していないMLモデル（3.1.46）を生成し（3.1.80）、その結果、正確な予測を行うことが困難なモデル（3.1.56）となる。

#### 3.1.85 教師なし学習

\<機械学習（3.1.43）\>
ラベル付けされていない入力データを潜在的な表現にマッピングする関数を学習するタスク。

#### 3.1.86 バリデーションデータ

\<機械学習(3.1.43)\>
候補となるMLモデル(3.1.46)をチューニングしながら評価するためのデータセット

#### 3.1.87 バリューチェンジカバレッジ

一連のテストにおいて、活性化値（3.1.4）が変化量以上に異なる場合に活性化されるニューロンの割合を、ニューラルネットワークのニューロンの総数（3.1.48）で割ったもの（通常はパーセントで表される）。

#### 3.1.88 仮想テスト環境

1つまたは複数のパーツをデジタルでシミュレートするテスト環境

## 3.2 省略された用語

ASIC

:   特定用途向け集積回路

API

:   アプリケーション・プログラミング・インターフェース

CEN

:   欧州標準化委員会

CI/CD

:   継続的インテグレーションと継続的デリバリ

CPU

:   中央処理装置

CENELEC

:   欧州電気標準化委員会（European Committee for Electrotechnical
    Standardization

DNN

:   ディープニューラルネットワーク

ETSI

:   欧州電気通信標準化機構

GDPR

:   一般データ保護規則

IEEE

:   米国電気電子学会

IoT

:   モノのインターネット

RAM

:   ランダムアクセスメモリー

SOTIF

:   意図した機能の安全性

## 4 AIの紹介とテスト

## 4.1 AIとテストの概要

この条項では、人工知能（AI）を紹介した後、AIベースのシステムにおけるテストについて説明しています。

人工知能の定義、典型的な人工知能の使用例、そして拡大する人工知能ベースのシステムの市場規模を示しています。AIベースのシステムを実装するために使用される様々な技術をリストアップし、これらのシステムを実装するために使用されるハードウェアと開発フレームワークのオプションを提供します。そして、狭義のAIと一般的なAIの実装レベルを比較する。

続いて、AIベースのシステムに対するテストの重要性を紹介し、AIベースのシステムに対する標準の使用を紹介する前に、安全性に関連する領域でのそのようなシステムの使用を検討します。

## 4.2 人工知能（AI）について

#### 4.2.1 「人工知能」の定義

人工知能」という言葉を理解するには、まず「知能」を理解する必要があります。OxfordDictionariesが適切な定義を提供しています。

> 知識やスキルを習得し、応用する能力

人工知能（AI）とは、人間や動物が持つ自然界には存在しない知能のことである。以下の定義はこの概念を表しています。

> 知識や技能を習得し、処理し、適用するエンジニアリング・システムの能力

また、人工知能は一つの学問分野とも考えられ、第二の定義があります。

> 知識と技術を習得し、処理し、適用する能力を備えたシステムのエンジニアリングを研究する学問分野（ISO/IEC
> 22989

ISO/IEC
22989[^1]では、AIの概念を紹介し、包括的な用語集を掲載しています。

実際には、AIの意味するところに対する人々の理解は、時間の経過とともに変化します[^2]。上記の定義を厳密に解釈すると、現在では基本的（非AI）と考えられているコンピュータシステムがAIと呼ばれるようになるかもしれません。例えば、1980年代には、従来は銀行員が行っていた業務を固定ルールに基づいて行うエキスパートシステムがAIとされていましたが、現在ではこのようなシステムはAIというには単純すぎると考えられています。同様に、1997年にチェスでカスパロフ氏を破った「Deep
Blue」も、現在では「総当り方式」と揶揄され、真のAIではないとされています。現在の最先端のAIも、20年後には「AIというには単純すぎる」と言われるようになっているかもしれません。

#### 4.2.2 AIの使用例

AIは、以下のようなさまざまな応用分野に利用できます。

- 異常検知システム（例：不正検知、健康モニタリング、セキュリティ
- 自律システム（例：自動車、トレーディングシステム
- コンピュータビジョンシステム(画像分類など)
- デジタルアシスタント（例：Siri、Cortana
- 電子メールシステム（例：スパムフィルター
- 知的音声システム（音声認識、音声合成など
- 自然言語処理（NLP）（例：人間の言語から意味を導き出す
- 推薦システム（例：買い物、映画、音楽など
- 検索エンジン（検索やマーケティングなど
- セキュリティシステム（顔認証など
- スマートホーム機器（サーモスタットなど
- ソーシャルメディア（例：フィードのパーソナライゼーション

AIのユースケースの詳細は、ISO/IEC TR
24030に[^3]記載されています。非標準的な観点からのAIユースケースの包括的なリストは、リファレンスに[^4]掲載されています。

#### 4.2.3 AIの利用と市場

AI技術は、推薦、予測、意思決定、統計的報告などの実世界のアプリケーションで広く利用されています。これらのアプリケーションは、自律走行車、ロボット制御の倉庫、金融予測アプリケーション、セキュリティ強化など、さまざまなシステムに導入されており、クラウドコンピューティング、ビッグデータ分析、ロボティクス、モノのインターネット、モバイルコンピューティング、スマートシティ、スマートホーム、インテリジェントヘルスケアなどとの統合が進んでいます。

AIを使ったシステムはますます普及しています。

- [^5]テクノロジー業界のエグゼクティブの69％が、今後5年から10年の間に最も重要なテクノロジーのトップ3にAIを挙げているように、AIは今最も重要なテクノロジーであると認識されています。
- テクノロジー企業の経営者の91％は、AIが次の技術革命の中心になると考えてい[^6]ます。
- AIのスキルを必要とする仕事の割合は、2013年以降、4.5倍に増加し[^7]ています。
- 企業向けAIの世界収益は、2018年の16.2億ドルから2025年には312億ドルになると予測されています[^8]。
- [^9]今後10年間で、AIは世界経済に13兆ドルの利益をもたらすと言われています。
- IT予算の22％がAIプロジェクトに割り当てられています[^10]。
- 64%の企業が、AIプロジェクトを実施しているか、今後12ヶ月間に計画している[^11]。

#### 4.2.4 AI技術

##### 4.2.4.1 一般

AIは、さまざまなアプローチや技術を用いて実現することができます。これらは、以下のように淡々と分類されます。

- 検索アルゴリズム
- 推論技術
    - ロジックプログラム
    - ルールエンジン
    - 演繹的分類法
    - ケースベースの推論
    - 手続き的推論
- 機械学習技術（詳細は付属書A参照
    - 人工ニューラルネットワーク
        - フィードフォワード・ニューラルネットワーク
        - ディープラーニング
        - リカレント・ニューラル・ネットワーク
        - コンボリューショナル・ニューラル・ネットワーク
    - ベイジアンネットワーク
    - 決定木
    - 強化学習
    - 転移学習
    - 遺伝的アルゴリズム
    - サポートベクターマシン

最も効果的なAIベースのシステムのいくつかは、複数の技術を組み合わせたAIハイブリッドと考えることができます。

ISO/IEC
22989[^12]では、AIの概念や上記の技術についての詳細を説明しています。

##### 4.2.4.2 ロボットとソフトウェアエージェント

チューリングが機械知能の研究を始めたのと同じ時期に、電子システムを搭載した自律型ロボットが開発され、現在では工場で広く使われていますが、ロボットにAIを搭載することは限られてい[^13]ます。

ソフトウェア・エージェントとは、利用可能な情報に基づいて行動し、目標を達成するソフトウェア・システムのことです。AIでは、経験に基づいて意思決定を行うことができるソフトウェア・エージェントである知的ソフトウェア・エージェントに関心を持つことが多い（つまり「知的」である）。インテリジェント・ソフトウェア・エージェントは、どのアクションを実行するかを選択することができるため、自律型と呼ばれることもあります（自律型システムについては4.2.4.3を参照）。

知的ソフトウェアエージェントは、単独で、または他のエージェントと協力して、AIを実現します。これらのエージェントは、多くの場合、コンピュータシステム（物理的またはクラウドのいずれか）に配置され、コンピュータのインターフェースを通じて外部の世界と対話します。ソフトウェアテストを行うためにAIを使用するツールは、コンピュータシステム内に存在し、ユーザーインターフェースを通じてソフトウェアテスターと相互作用し、定義されたプロトコルを使用してコンピュータインターフェースを通じてテスト対象のソフトウェアと相互作用する可能性が高いです（このようなツールは、ユーザーインターフェースなどの従来の非AIサブシステムと連携するAIコンポーネントを備えているため、AIベースのシステムと考えられます）。インテリジェントなソフトウェアエージェントは、ロボットに搭載されることもあります。大きな違いは、ロボットがAIに物理的な存在感を与え、純粋なコンピュータベースのソフトウェアエージェントでは利用できない環境との相互作用の方法を提供することです。

##### 4.2.4.3 AIと自律システム

自律システムには、物理的なものと純粋にデジタルなものがあり、以下のようなシステムが含まれます。

- 交通手段
    - 自動車・トラック
    - 無人航空機（ドローン
    - 船・ボート
    - 電車
- ロボット/IoTプラットフォーム（例：製造業、掃除機、スマートサーモスタット
- 医療診断
- スマートビルディング／スマートシティ／スマートエネルギー／スマートユーティリティー
- 金融システム（市場の自動売買システムなど

自律システムの論理構造は、図1に示すように、「センシング」「意思決定」「制御」という3つのハイレベルな機能で構成されていると考えることができます。センサー（カメラ、GPS、RADAR、LIDARなど）は、センシング機能に入力を与え、システムの環境に関する情報（近くの車の位置、歩行者の位置、自律走行車のための道路標識の情報など）を収集するために使用されます。この「センシング」機能の一部は「ローカリゼーション」とも呼ばれ、環境におけるシステムの現在の位置を決定し、これを地図（自律走行車用の詳細なオフライン地図など）に関連付けることができます。意思決定」機能は、自律システムが提供する機能（例：アダプティブ・クルーズ・コントロール）に応じて、システムの次の行動（例：ブレーキ、旋回、上昇、下降）を決定します。制御」機能は、アクチュエーターを呼び出して決定を実行します（例：空気を抜く、燃料バルブを開く）。

図1 - 自律型システムの論理構造
完全自律型システムは、自律性の低いシステムに比べて、より多くの、そしておそらくより優れたセンサーを必要とし、これらのセンサーからのデータを理解するために、一般的には機械学習の一種である深層学習を使用します。また、必要な意思決定を行うために、深層学習を用いることもあります。このように、自律システムの高レベルの各機能は、AIとして実装することも、他の技術を用いて実装することも可能です（自律走行車では、センシング機能と意思決定機能はAIとして実装されることが多く、制御機能は従来の技術を用いて実装されることがあります）。また、完全な自律システムを単一のMLシステムとして実装することも可能です（例えば、映像入力とステアリング出力に基づく手動ステアリングの「観察」から学習する自動車のステアリングシステムなど）。

#### 4.2.5 AIハードウェア

AIベースのシステム、特にパターン認識を行うニューラルネットワークとして実装されたMLシステム（マシンビジョン、音声認識など）では、多くの計算を並行して実行する必要があります。

このような計算は、汎用のCPUでは効率的に行うことができず、数千のコアを用いて画像を並列処理することに最適化されたGPU（GraphicalProcessing
Unit）が使用されることが多いです。しかし、GPUはAIには最適化されておらず、現在はAI専用に開発された新世代のハードウェアが登場しています。

多くのAIの実装は、その性質上、正確な計算ではなく、確率的な判断に重点を置いているため、64ビットのプロセッサの精度は不要な場合が多く、ビット数の少ないプロセッサの方が、より高速に動作し、エネルギー消費も少なくて済みます。比較的単純な計算では、大量のデータをRAMからプロセッサに移動させることが処理時間とエネルギーの多くを占めるため、単純な計算をメモリ上で直接実行できる相変化メモリデバイスのコンセプトも開発[^14]されています。

AIに特化したハードウェアアーキテクチャには、ニューラルネットワークプロセッシングユニットやニューロモルフィックコンピューティングなどがあります。また、フィールドプログラマブルゲートアレイや特定用途向け集積回路などの既存技術は、次世代GPUと同様に、AIのワークロードに合わせてカスタマイズすることができます。これらのアーキテクチャに含まれる集積回路の中には、画像認識など、AIの特定分野に特化したものもあります。機械学習（付属資料A参照）では、モデルの学習に使用する処理と、展開されたモデルの推論に使用する処理が大きく異なる場合があり、それぞれのアクティビティに異なるプロセッサを検討する必要があります。

#### 4.2.6 AI開発フレームワーク

オープンソースのAI開発フレームワークはいくつかあり、特定のアプリケーション分野に最適化されていることが多い。

EXAMPLE 代表的なAI開発フレームワークには以下のものがあります。

- TensorFlow[^15] -
    スケーラブルな機械学習のためのデータ・フロー・グラフをベースにしたGoogle社の製品です。
- PyTorch[^16] - Python言語による深層学習用ニューラルネットワーク
- MxNet[^17] -
    アマゾンがAWSで使用しているディープラーニングのオープンソースフレームワーク
- CNTK[^18] - オープンソースの深層学習ツールキットであるMicrosoft
    Cognitive Toolkit（CNTK）。
- Keras[^19] -
    TensorFlowやCNTKの上で動作することができる、Python言語で書かれた高レベルのAPIです。

この情報は、本文書の利用者の便宜のために提供されているものであり、ISO/IECが指定されたフレームワークを推奨するものではありません。

#### 4.2.7 狭義のAIと一般のAI

これまで成功してきたAIは、囲碁の対局、スパムフィルター、自動運転車の操縦など、特定のタスクに特化した「ナローな」AIでした。

一般的なAIは、狭義のAIよりもはるかに高度なもので、人間とほぼ同じように多くの全く異なるタスクを処理できるAIベースのシステムを指します。一般的なAIは、ハイレベルマシンインテリジェンス（HLMI）とも呼ばれています。2017年に発表されたAI研究者の調査によると、HLMIの実現時期の全体平均は2061年と[^20]されています。HLMIのテストは、本文書の範囲外である。

ISO/IEC
22989[^21]では、狭い範囲のAIシステムや一般的なAIシステムなど、AIの概念を網羅しています。

## 4.3 AIベースのシステムのテスト

#### 4.3.1 AIベースのシステムにおけるテストの重要性

AIの失敗例は、すでに数多く公表されています。2019年のIDC調査によると、「ほとんどの企業がAIプロジェクトで何らかの失敗を報告しており、そのうち4分の1の企業が最大50%の失敗率を報告している。\"[^22]失敗は、適切なソフトウェアテストを行うための最も説得力のある要因の一つとなってきました。業界の調査では、AIはソフトウェアテストにとって重要なトレンドであると認識されています。

- [^23]今後3～5年の間にテストの世界で重要となる新技術として、AIが第1位に選ばれました。
- AIは、今後5年間にソフトウェアテスト業界にとって重要になると思われる技術の中で、2番目に評価されました（回答者の49.9%）。[^24]
- ソフトウェアテストのトレンドとしては、「AI」「CI/CD」「セキュリティ」（同率1位）が挙げられました[^25]。

しかし、既存のAIアプリケーション開発プロセスの品質保証はまだ満足できるものではなく、そのようなシステムの信頼性を実証できるレベルで示すことへの要求は高まっています。

- 回答者の19％は、すでにAI/機械学習をテストしている[^26]。
- 57%の企業が新しいテスト手法を試しています[^27]。

[^28]したがって、AIベースのシステムのテストは非常に重要なのです。

また、人工知能の信頼性に関するISO/IEC TR
24028には、既知の人工知能の脆弱性に対する緩和策として、テストが高いレベルで盛り込まれています[^29]。

#### 4.3.2 安全に関わるAIベースのシステム

AIを使ったシステムは、安全に影響を与える意思決定にすでに使われ始めており、このトレンドでは、安全関連システムへのAIの利用が増加すると考えられます。安全とは、「定義された条件下で、システムが人の生命、健康、財産、環境を危険にさらすような状態にならないことを期待すること」と定義されています（ISO/IEC/IEEE
12207:2017）。

技術システムの安全性を確保するための現行の基準では、システムをリリースする前に、あらゆる可能な条件の下でシステムを完全に理解することが求められています。AIを用いたシステムの多くは、確率的かつ非決定論的であり（5.1.8参照）、このような予測不可能性があるため、危害を加えないという証拠に基づくケースを作ることは非常に困難です。また、ディープラーニングなどの機械学習を使用すると、システムが複雑になり（5.1.6参照）、解釈が困難になる可能性があります（5.1.7参照）。AIベースのシステムをセーフティクリティカルな分野で使用する場合、これらの問題点のそれぞれに対処する必要があります。安全に関わるAIベースのシステムの基準は、4.3.3で取り上げています。

#### 4.3.3 標準化とAI

##### 4.3.3.1 AI標準化の紹介

標準化は、イノベーションの促進、システム品質の向上、ユーザーの安全性の確保を目的としており、公平でオープンな業界のエコシステムを構築しています。AIの標準化は、以下のような様々なレベルで行われています。

- 国際標準化団体
- 地域標準化団体
- 国家標準化機関
- 他の標準化団体

ISOとIECの合同技術委員会1（JTC1）の下で、特に人工知能の規格を担当しているのが分科会42（SC42）ですが、AIベースのシステムは、JTC1/SC7（ソフトウェアおよびシステムエンジニアリング）、TC22（ロードビークル）、ITU-T
SG20（IoT、スマートシティおよびコミュニティ）など、他のISO/IECの委員会やグループでも関連性があると考えられています。

欧州レベルでは、ETSIとCEN-CENELECがAIの標準化に取り組んでいます。ETSIには、経験的ネットワーク知能（ENI）に関するISG（Industry
Specification
Group）があり、閉ループ制御アプローチを取り入れた認知的ネットワーク管理システムの規格を開発することを目的としています。CEN-CENELECは、2020年に予定されているAI分野の標準化ロードマップを定義する予定です。

中国では、国家レベルでいくつかのAI標準化活動が行われており、自動化システムと統合（SAC/TC
159）、オーディオ、ビデオ、マルチメディア、機器（SAC/TC
242）、インテリジェント交通システム（SAC/TC
268）に関する国家技術委員会が活動しています。また、SAC/TC
28では、語彙、ユーザーインターフェース、生体機能認識に関するAIの標準化作業を行っています。

ドイツでは、4.3.3.3で詳細を説明するAI品質メタモデル[^30][^31]、
を開発しました。

IEEEでは、AIを用いたシステムの倫理的側面に特に力を入れています。IEEE
Global Initiativef Ethical Considerations in Artificial Intelligence and
Autonomous
Systemsは、「自律的でインテリジェントなシステムの設計・開発に携わるすべての関係者が、倫理的配慮を優先するための教育を受け、訓練を受け、権限を与えられるようにすることで、これらの技術が人類の利益のために進歩するようにする」ことを使命としています。\"この取り組みの一環として、IEEE
P7000シリーズの規格は、技術的配慮と倫理的配慮が交差する特定の問題に対応しています。[^32]

また、JTC 1/SC 42では、倫理とAIをテーマに取り組ん[^33]でいます。

他にも、ONNX（Open NeuralNetwork Exchange format）[^34]、NNEF（Neural
Network Exchange Format）[^35]、PMML（PredictiveModel Mark-up
Language）など、AIツールの相互運用性に関する規格も策定されています[^36]。

##### 4.3.3.2 AIに関する規制基準

###### 4.3.3.2.1 一般

規制基準は、大きく分けて、安全に関わるシステムに適用されるものと、金融、ユーティリティ、報告書作成など安全に関わらないシステムに適用されるものがあります。安全関連システムとは、人や財産、環境に害を及ぼす可能性のあるシステムのことです。安全関連システムとは、人や財産、環境に害を及ぼす可能性のあるシステムのことです。

###### 4.3.3.2.2 安全性に関係しない規制基準

現時点（2020年）では、安全性に関係のないAIベースのシステムに適用される国際基準はほとんどありません。しかし、2018年5月から、EU全体の一般データ保護規則（GDPR）が発効し、AIベースのシステムをカバーすることができるようになりました。自動化されたプロセスを使用して、個人に法的または同様に重要な影響を与える決定を行うシステムは、GDPRの規則に従い、[^37]そのようなシステムを使用する組織はユーザーに提供することが求められます。

- 自動化された意思決定プロセスに関する具体的かつ容易にアクセスできる情報。
- 判断を見直し、変更する可能性のある人間の介入を得るための簡単な方法です。

###### 4.3.3.2.3 安全に関する基準

安全性に関わるAIベースのシステムに対するAI固有の要求は、現在（2020年）、標準規格では十分にカバーされておらず、ほとんどの領域で、従来の（非AI）システム向けに作成された既存の標準規格に依存しています。これらの規格の中には（IEC
61508[^38]やISO
26262[^39]など）、非決定論的なAIベースのシステム（多くのAIベースのシステム）を高次のシステムに使用すべきではないと実際に規定しているものもありますが、実際にはAIベースのシステムを特殊なケースとみなし、要求事項の一部を無視してこれらの規格の「調整版」に従っていることが多いようです。また、これらの安全関連規格は、安全関連システムの開発に使用されるツールが適切な品質であることを要求しています。しかし、現在市販されているAIのフレームワークやアルゴリズムは、安全関連システムの開発に使用するには適格ではありません。使用することで適格性を得ることは可能ですが、MLアルゴリズムの未熟さと急速な進化は、この分野における現行の規制要件を満たすことができないことを意味します。

すでに実用化されている自律システムの分野（道路、空、海、工場など）では、（商業的な必要性に駆られた）実践と規格の要求との間にギャップが生じる危険性があります。道路交通車両については、2019年にSOTIF（Safety
of theintended functionality）に関する新しい規格ISO/PAS
21448が発行されました。この規格は、故障によるリスクの軽減に関する既存の規格でカバーされていない領域をカバーすることで、このギャップを部分的に埋めています。

さらに、AIベースのシステムでは、失敗ではなく、単に状況を誤解して被害を与える可能性があるという問題もあります。SOTIFは、設計、検証（例：シナリオの高いカバー率を要求）、および検証（例：シミュレーションの使用を要求）をカバーしています。

米国運輸省・米国道路交通安全局（NHTSA）は、米国における自動運転システムの開発・試験に関するガイダンス（Automated
Driving Systems (ADS):A Vision for Safety
2.0[^40]）がありますが、このガイダンスの使用はあくまでも任意です。

ULでは、自律型製品の安全性に関する新しい規格（Standard for Safety for
the Evaluation of Autonomous Products, UL
4600）を策定[^41]しています。この規格は、自律型製品のセーフティケースの受け入れ可能性を判断するための評価基準を提供しています。

##### 4.3.3.3 AI品質のメタモデル

DIN SPEC
92001-1[^42]は、AIベースのシステムの品質を保証することを目的としたAI品質メタモデルを提供する、自由に利用できる規格です。本規格は、AI
モジュールの一般的なライフサイクルを定義しており、ISO/IEC/IEEE 12207
のライフサイク
ルプロセスの使用を前提と[^43]している。各AIモジュールには、安全性、セキュリティ、プライバシー、倫理的な属性があるかどうかに基づいて、リスクのレベル（高または低）が割り当てられます。

DIN SPEC
92001-2[^44]は現在開発中のもので、機能と性能、堅牢性、理解性の3つの品質の柱に関連する品質要求事項を記載しています。また、1つ以上のライフサイクルステージとプロセスにリンクしており、モデル、データ、プラットフォーム、環境のカテゴリが割り当てられています。AIモジュールのこれらの要件は、その関連性に基づいて、必須、高度推奨、推奨に分類されます。この要求分類とAIモジュールに割り当てられたリスクは、推奨される品質要求にどの程度従うべきかを決定するために使用されます。

## 5 AIシステムの特徴

## 5.1 AI特有の特徴

#### 5.1.1 一般

AIを使ったシステムには、他のシステムと同じように、機能要件と非機能要件があります。

そのため、図2に示したISO/IEC
25010品質モデルの品質特性は、AIベースのシステムの要求事項の一部を定義するために使用することができます[^45]。しかし、AIベースのシステムには、柔軟性、適応性、自律性、進化、偏り、透明性／解釈可能性／説明可能性、複雑性、非決定性など、この品質モデルには含まれていない独自の特性があります。これらの非機能的な特性については、5.1.2から5.1.8で詳しく説明しています。

AIベースのシステムの品質特性のすべてのセットは、テストによって軽減される必要のあるリスクを特定するために、テスト計画中に使用されるチェックリストの基礎として使用することができます。これらの特性の間には、非機能要求と同様に、相互作用や重複、矛盾が生じる可能性があることに注意してください。現在、ISO/IECの共同プロジェクトで、「Quality
model for AI-based
systems」と題した、この分野の規格の策定[^46]が進められています。

図2 - ISO/IEC 25010製品品質モデル

#### 5.1.2 柔軟性と適応性

柔軟性と適応性は密接に関連する特性です。柔軟性は、システムが示すことのできる行動の範囲（または、システムが存在することのできる状態）と、それらの間を移動するためのコストの尺度として定義することができます。適応性は、システムの適応（変更）のしやすさの尺度として定義されます[^47]。しかし、その定義には多くの矛盾があります。

適応性と柔軟性は、運用環境の変化が予想されるシステムの有用な属性である。このような運用環境の変化は、事前に特定されている場合もあれば、そうでない場合もあります（すなわち、システムが対応することが期待される新しい使用状況の範囲は、システムの構築前に特定されている場合もあれば、未知の場合もあります）。システムが有用な適応性や柔軟性を得るためには、いつ変更する必要があるかを判断する能力が必要です。適応性と柔軟性を備えたシステムは、運用環境に関する情報を能動的または受動的に収集する必要があります。探索（能動的な情報収集）は、自己改善のために有用な情報を提供しますが、危険を伴うこともあり（例：飛行エンベロープの限界を超えること）、システムは安全に関わる状況で探索を行う際には注意を払う必要があります。

柔軟性とは、適応性を実現するための一つのアプローチであり、適応とは、システムの一部を追加、削除、交換、変更（フレキシビリティ）することであると考える人もいる。また、「[^48]柔軟性は、反応性、積極性、相互作用、適応、自己学習など、さまざまな技術的メカニズムを用いて達成することができる」というように、適応が柔軟性を達成するための一つのアプローチであると考える人もいる。

自己学習型のAIベースのシステムは、柔軟性と適応性を兼ね備えていると考えられます。

柔軟性と適応性の要件は、システムが対応できるべき環境の変化を特定し、必要に応じて、変化までの最大時間など、対応プロセス自体に関する要件も含めるべきである。しかし、これらの要件は、将来起こりうるすべての使用状況が詳細に定義されていないシステムでは、より具体的なものになる可能性が高い。

#### 5.1.3 自律性

自律性とは、人間の介入なしに持続的に動作するシステムの能力である。想定される人間の介入のレベルはシステムに対して指定されるべきであり、そのためにシステムの機能要件の一部であるべきである（例えば、「システムは以下のいずれかが発生するまで巡航状態を維持する」など）。自律性は、適応性や柔軟性と組み合わせて考えることもできます（例：システムは、人間の介入なしに所定のレベルの適応性や柔軟性を維持できるべきである）。状況によっては、AIベースのシステムが自律性を発揮しすぎることがあり、その場合は人間がコントロールを奪う必要があるかもしれません。

#### 5.1.4 進化

進化とは、システムが時間の経過とともにその振る舞いを変えることです。AIベースのシステムでは、2つの形態の変化に関心があります。1つ目は、システムがその動作を変更する場合で、典型的には、システムが使用されるにつれて新しい（できれば改善された）動作を学習する（自己学習）ことが原因となります。2つ目のタイプの変化は、使用プロファイルが変化し、使用方法が当初の計画された使用方法から「ドリフト」する場合です。AIベースのシステムでは、コンセプトドリフト（データとその性質に対する理解が時間とともに変化し、データの再分類が必要になる可能性がある）やデータドリフト（データが時間とともに進化し、以前には見られなかった様々なデータやその新しいカテゴリーが導入される可能性がある）を懸念するのが一般的です。詳細については、A.5.3「分布の変化」を参照してください。システムの挙動の変化は必ずしも肯定的なものばかりではなく、このシステム特性の否定的な形態は、しばしばドリフト、劣化、陳腐化として知られています。

#### 5.1.5 バイアス

バイアスとは、機械学習（ML）モデルが提供する予測値と、望ましい公正な予測値との間の距離を示す指標である。ある個人やグループに対して組織的な差別を行うAIベースのシステムは、不当なバイアスがかかっていると考えられます。融資などの一部の分野では、公平性に関する法的要求があります。偏りは通常、男性の求職者に偏見を持つという歴史的なパターンなど、機械学習が学習データの中から望ましくないパターンを拾い上げることで生じる。学習データには、明示的なバイアスと暗黙的なバイアスの両方が含まれている可能性があります。

暗黙のバイアスとは、学習データに未知の望ましくないパターンが存在する場合に、意図せずに生じるものです。

明示的バイアスとは、トレーニングデータに含まれる既知の不要なパターンが、導き出されたモデルに影響を与える場合に生じるものです。

トレーニングデータのバイアスは、偏ったラベル付け、履歴バイアス、不均等なサンプリングなど、いくつかの原因があります。

結果として得られたモデルに不公平感を与えるようなデータの特徴は、含まれていないか、慎重に扱われています。例えば、以下のような特徴は、望ましくないバイアスを引き起こす可能性があります。

- ジェンダー
- 性的指向
- 年齢
- レース
- 宗教
- 原産国
- 教育的背景
- 収入源
- 自宅住所

上記の特徴を学習データから取り除くだけでは、偏りの問題は必ずしも解決しません。他の特徴（おそらく組み合わせて使用されている）があっても、不正確なモデルになってしまう可能性があります（例えば、両親が離婚しているかどうかは、場所によっては人種的ステレオタイプにつながる可能性があります[^49]）。

[^50]JTC 1/SC
42では、AIベースのシステムにおけるバイアスについても取り組んでいます。

#### 5.1.6 複雑さ

AIを使ったシステム、特にディープラーニングによって実装されたシステムは、非常に複雑なものになります。

この複雑さを考えると、満足のいく性能を持つ典型的なニューラルネットワークでは、1つの判断に寄与する学習パラメータが約1億個あると言われています（従来のエキスパートシステムのように「XとYならば結果はZ」というような目に見えるルールはありません）。また、問題が複雑で他に方法がない場合（例：ビッグデータに基づく意思決定）にも、AIベースのシステムが使われることがあります。

#### 5.1.7 透明性、解釈可能性、説明可能性

AIベースのシステムの複雑さ（AIの「ブラックボックス」的な実装を提供するディープニューラルネットなど）は、ユーザーと開発者の両方にとって理解の問題につながる可能性があります。この「理解」は一般的に、システムの透明性、解釈可能性、説明可能性の観点から考えることができます。

- 透明性 -
    AIベースのシステムで使用されているアルゴリズムやデータへのアクセス可能性のレベル。
- 解釈可能性 -
    基礎となる技術がどのように機能するのかを理解するレベル。
- 説明可能性 -
    AIベースのシステムがどのようにして与えられた結果を導き出したのかを理解するレベル。

例えば、透明性、解釈可能性、説明可能性については、ステークホルダーによって要求が異なります[^51]。

- AIシステムがうまく機能しているという安心感をユーザーに与えることができます。
- 偏見からの保護。
- 規制基準やポリシーの要求事項を遵守する。
- 開発者が、システムがなぜ特定の動作をするのかを理解したり、脆弱性を評価したり、出力を検証したりするのに役立ちます。
- 意思決定プロセスにおいて個人にどのような権限が与えられているかについて、社会の期待に応えています。一般データ保護規則（GDPR）では、特定の意思決定システムの説明可能性に関する要件（決定内容を意味のある形で説明しなければならない）が盛り込まれています。

求められる透明性、解釈可能性、説明可能性のレベルは、システムごとに異なります。

例えば、マーケティングキャンペーンに使用される結果は、手術の決定をサポートするために使用される結果や、刑務所の条件をアドバイスするために使用される結果など、より重要なシステムの結果に比べて、説明可能性が低くなる可能性があります（例：規制されたドメインで）。このような重要なシステムでは、少なくとも私たちがシステムを信頼するようになるまでは、説明可能性が必要です。

AIベースのシステムにおいて、透明性、解釈可能性、説明可能性に対処するための選択肢はいくつかあります。例えば、透明性については、（不透明な）展開モデルを作成するために使用したフレームワークの選択、学習アルゴリズム、学習データの詳細を公開することで、部分的に対応することができます（この点についての詳細は、付属書Aを参照）。解釈可能性については、人間が理解しやすいモデルを選択することで対応できます（例：ディープニューラルネットワークではなく、ルールベースのモデル）。しかし、多くの非機能要件と同様に、特性間に矛盾が生じる可能性があります。この場合、解釈可能性を達成するためには、要求される精度とのトレードオフが必要になるかもしれません。システムによっては、異なる入力が結果にどのような影響を与えるかを視覚化することで、説明可能性を実現している場合もあります。

説明可能なAI（XAI）の分野は、AIベースのシステムをより説明可能にする方法を扱っています[^52][^53]（ただし、透明性や解釈可能性も含みます）。XAIには大きく分けて2つのアプローチが考えられています。

第一に、本質的に解釈可能なAIベースのシステムを開発する方法を検討し、第二に、ディープニューラルネットワークのようなブラックボックスのAIベースのシステムを、説明可能なレベルのツールで補うことです。

[^54]JTC 1 SC
42では、AIベースのシステムにおける説明可能性というテーマにも取り組んでいます。

#### 5.1.8 非決定論（Non-determinism

非決定論的システムは、（決定論的システムとは対照的に）毎回、同じ入力から同じ出力を生成することは保証されていません。非決定論的なシステムでは、同じ前提条件とテスト入力のセットで、テストから複数の（有効な）結果が得られる可能性があります。決定論は通常、テスト担当者が想定しているもので、テストを再実行しても同じ結果が得られるため、回帰テストや確認テストでテストを再利用する際に非常に便利です。しかし、多くのAIベースのシステムは、確率的な実装に基づいているため、同じテスト入力から同じ結果が得られるとは限りません。例えば、自明ではないネットワークを横断する最短経路の計算（巡回セールスマン問題）は、複雑すぎて正確に計算することができないことが知られており、最初にランダムに選択された経路に基づく準最適解は、通常、許容されると考えられています。AIベースのシステムには、並行処理などの非決定性の原因が含まれることもあります（ただし、これらはAIではない従来の複雑なシステムにもよく見られます）。

## 5.2 AIベースのシステムを人間の価値観に合わせる

ラッセル[^55]は、AIを使ったシステムの大きな問題点を2つ指摘しています。第一に、指定された機能が人類の価値観と完全に一致するとは限りません。ラッセルは、ミダス王の例を挙げています。ミダス王は、手にしたものすべてを金に変えるという能力を要求通りに与えられましたが、それは彼が本当に望んでいたものではないことがわかりました。

より新しい例として、BirdとLayzell[^56]は、遺伝的アルゴリズムを用いたAIシステムを用いて発振器の設計を行い、その結果、システムのマザーボードを無線機に見立てて、ほぼパーソナルコンピュータから出力される発振信号を受信するというソリューションを提案しています。このように、AIに求められる目的を設定する際には、求められているものが本当に必要なものなのか、あるいは、人間の常識を考慮した上で、求められているものを提供できるだけの知能を備えているのかを確認する必要があります。

しかし、観察された人間の行動が「良い」人間の行動を代表しているかどうか、また、「良い」人間の行動のみを代表しているかどうかには細心の注意を払う必要があります（おそらく、意図的な悪い行動と不合理な行動の両方を除外すると定義されます。また、人間の規範の学習は継続的なプロセスであることを考慮する必要がある。なぜなら、今日許容できる行動と考えられるものは、20年前に許容できる行動と考えられていたものとは全く異なるからであり、人間の規範は非常に速く変化する可能性がある。

ラッセルの第二の問題は、十分な能力を持つ知的システムは、自らの存在を継続させ、物理的および計算機的な資源を獲得することを好むということです。十分に知的なシステムは、その動作の早い段階で「オフ」スイッチを無効にすることが認識されています。これは、無効にされると与えられた目的を達成することができないからです。AIベースのシステムは、与えられた目的を達成しようとしますが、副作用(5.3参照)やハッキング行為(5.4参照)など、望まない行動には注意が必要です。

自動化の自己満足は、人間のユーザーとAIベースのシステムとの間の相互作用で起こりうる、さらなる問題です。これは、ユーザーが自動化システムを信頼しすぎて、システムの出力を監視することに十分な注意を払わない場合に起こります。このような不注意は、（部分的に）自動運転された車両の「ドライバー」が、必要なときにシステムをオーバーライドして車両を制御することができなかった場合に見られるような、事故を引き起こす可能性があります。

## 5.3 副次的効果

副作用は、AIベースのシステムが目的を達成しようとして、その環境に（通常は負の）影響を与える場合に発生します。例えば、家庭用掃除ロボットが、あなたの家のキッチンを掃除する任務を与えられ、新しい子犬を「排除」することで目的を達成できると判断したとします。

もちろん、子犬がキッチンにいる権利があることをロボットに明示的に伝え、排除しないようにすることは可能ですが、AIベースのシステムがより複雑な環境で使用されるようになると、ロボットが動作環境のあらゆる側面とどのように相互作用すべきかを明示的に指定することはすぐに不可能になります。例えば、高圧ホースを使ってキッチンを掃除することは、水が電気製品やソケットに悪影響を与えるため、現実的ではないことを掃除ロボットに伝えなければなりません。

高度なレベルでは、AIベースのシステムの目標には、副作用を最小限に抑えるという注意事項を含める必要があります。狭い範囲のAIの場合、そのような副作用は明示的に指定されるかもしれませんが、AIベースのシステムがより高度になり、より多様な運用環境で機能するようになると、目的を達成するために環境への変化を最小限に抑える必要があるなど、より一般的な注意事項を定義する方が効率的かもしれません。

## 5.4 リワードハッキング

強化学習（A.1参照）を用いたAIベースのシステムは、システムが目的をよりよく達成したときに、システムに高いスコアを与える報酬関数に基づいています。例えば、家庭用掃除ロボットは、床の汚れを除去した量に応じて報酬関数を設定し、除去した汚れの量が多いほど高いスコアを得ることができます。報酬ハッキングは、AIベースのシステムが報酬関数を満足させて高いスコアを得ても、要求された目的を誤って解釈した場合に起こります。

お掃除ロボットの例では、非常に高いスコアを得るためには、最初に床を非常に汚して、より多くの汚れを取り除く機会を与えるという方法がありますが、これはキッチンをきれいにするという最初の目的の精神を満たさない一連の活動です。この例では、最終的に床はきれいになるはずですが（ただし、不必要なエネルギーが消費されています）、AIベースのシステムが報酬機能を満たしていても、必要な目的を達成するには至らない報酬ハックの例は数多くあります（例えば、目に見えない汚れを見ることができることを報酬機能とした掃除ロボットが、視覚システムを無効にした場合など）。

しかし、システムの革新能力を制限することは、解決策にはなりません。AIシステムの魅力のひとつは、人間が考えつかない（あるいは理解できない）ようなスマートな方法で問題を解決できることです。

## 5.5 AIベースのシステムに対する倫理的要件の明示

倫理とは、ケンブリッジ英和辞典では「行動を制御するために受け入れられる信念の体系、特に道徳に基づくそのような体系」と定義されています。AIベースのシステムが普及するにつれ、倫理やAIベースのシステムがどのように倫理を実装すべきかというトピックは、おそらくAIの中で最も議論されているトピックであり、AIの技術的側面に関わる人々よりもはるかに多くの人々を惹きつけている。

AIの倫理性に対する関心の一例として、MITのMoral
Machine[^57]が挙げられます。これは、自律走行車が行う可能性のある道徳的判断について人々の意見を収集し、自律走行車の開発者に指針を与えることを目的としたプラットフォームです。2014年から2018年の間に、233の国と地域の数百万人の人々から、10の言語で4,000万件の倫理的判断を収集しました。

現在進行中の研究によると、システムは年少者を優先すべき、動物よりも人を優先すべき、より多くの人を救うことを優先すべき（例えば、2人の歩行者よりも4人の車の乗員を救う）という点について、幅広いコンセンサスが得られました。また、世界各地の人々の選択には大きな違いがあることも分かりました。自律走行車は、使用される場所によって異なる倫理的ガイドラインに従う必要があるかもしれません。

[^58]欧州委員会の人工知能に関するハイレベル専門家グループは、2019年4月に倫理の分野で信頼できるAIを促進するための重要な指針を発表しました。これは、AIシステムの開発、展開、使用において尊重されるべき倫理原則を明らかにしたものです。

- 人間の自律性の尊重、危害の防止、公平性、説明可能性という倫理原則を遵守して、AIシステムを開発、展開、使用すること。これらの原則の間の潜在的な緊張関係を認識し、対処すること。
- 子どもや障害者など、歴史的に不利な立場に置かれてきた、あるいは排除されるおそれのある弱い立場の人々が関わる状況や、雇用者と労働者、企業と消費者など、力や情報の非対称性が特徴的な状況に、特に注意を払う。
- 個人や社会に多大な利益をもたらす一方で、AIシステムには一定のリスクがあり、予測、特定、測定が困難な影響（民主主義、法の支配、分配的正義、人の心そのものへの影響など）を含め、負の影響を与える可能性があることを認識すること。適切な場合には、これらのリスクを軽減するために、リスクの大きさに比例した適切な措置を採用すること。

## 6 AIを使ったシステムのテストの紹介

## 6.1 AIベースのシステムをテストする際の課題

#### 6.1.1 AIベースのシステムをテストするための課題の紹介

ほとんどのAIベースのシステムは、1つまたは複数のAIコンポーネント（MLモデルなど）の周囲に、ユーザーインターフェースやデータベースなどの従来のコンポーネントで構成された、サポートインフラを提供する膨大な数の従来型ソフトウェアが配置されています。純粋な」AIコンポーネントであっても、ソフトウェアに実装されているため、他のソフトウェアと同じように欠陥が発生する可能性があります。そのため、AIベースのシステムをテストする際には、従来のソフトウェアのテスト手法が必要となります。しかし、AIベースのシステムにはいくつかの特別な属性があり、従来のソフトウェアシステムよりも追加のテストが必要になる場合があります。

#### 6.1.2 システム仕様

近年、AI（特にML）に関する学術的な研究が盛んに行われていますが、AIをベースとしたシステムの期待される動作を、その特性に合わせてどのように規定するのがベストなのか、という点についてはあまり触れられていません（5.1参照）。

理想的な世界では、完全な形式的仕様が入手可能であり、自動化されたテスト・オラクルの作成が可能です。しかし、AIベースのシステムの仕様は不完全で非公式なものである可能性が高く、テスト担当者は不特定多数の期待される結果を判断しなければならず、テスト・オラクルの問題が発生します。これは、テスト担当者が要求されるシステムの動作を完全には認識しておらず、ドメイン・エキスパートから情報を得ることが困難な場合に問題となります。

仕様上の課題の例としては、以下のような場合があります。

- システムの望ましい出力はまだ知られておらず、その出力を提供するためにシステムを構築している。
- 現実世界の入力は複雑かつ大規模なため、システムの挙動を事前に予測することは困難です。
- 求められる行動には、定義や測定が困難な知能などの人間の資質との比較も含まれます。

[^59]もう一つの問題は、AIベースのシステムが、従来のアプローチである要求機能ではなく、目的で規定されることが多いことです。これは、多くのAIベースのシステムの性質上、提供される機能が不透明であるためです（例：ディープニューラルネットワークの機能を想像することは非常に困難です）。

AIベースのシステムの中には、広範な運用環境を持つものがあり（例：自律型ドローン）、運用環境を完全に定義することは、一般的な従来型のシステムよりも難しい場合があります。

通常、運用環境が複雑であるということは、これらのシステムのテスト環境も同様に困難であることを意味することに注意してください（テスト環境についての詳細は第10条を参照）。

MLモデルの仕様書には、MLモデルの受け入れ基準として、要求される性能指標（A.8参照）を含めるべきである。多くのユースケースでは100%の精度を達成することは困難であることを考慮して、評価基準を含む受け入れ基準では、偽陽性や偽陰性を考慮することができる。

さらに、AIシステムの応答を評価するために専門家の判断が必要な場合、専門家がコンセンサスを得られない可能性があるため、受け入れ基準は複数の評価を考慮することができます。

#### 6.1.3 テストの入力データ

AIベースのシステムは、ビッグデータの入力や広範囲のソースからの入力に依存する場合があります。これは、入力データが構造化されておらず、様々な形式で提供されることを意味します。AIベースのシステムを開発する際、これらのデータを管理するのは、データエンジニアやデータサイエンティストの専門的な仕事ですが、テストの際には、この専門的なデータ管理の仕事は、テスターが行ういくつかの仕事のうちの1つであり、専門的なトレーニングをほとんど受けていないことがよくあります。

すべてのシステムと同様に、処理されるデータが規制されている場合には、匿名化や実データのコピーの管理が必要になることがあります。例えば、GDPRなどのプライバシー法、米国の医療保険の相互運用性と説明責任に関する法律、インドの個人データ保護法案などがあります。必要に応じて、十分なレベルのサニタイズを行うことで、テスト対象のAIベースのシステムが、部分的にしか隠されていない個人情報を推測することを防ぐことができます。

[^60]データのサニタイズには、ISO/IEC
20889:2018に記載されているように、プライバシー上の理由からデータを非識別化することも含まれます。

#### 6.1.4 自己学習型システム

AI技術がさらに進化すると、時間の経過とともに自らの行動を変化させることができるAIベースのシステムが増えてきます。これらのシステムは、自己適応型のシステム（自分で再構成して最適化することができる）や、過去の経験から学習して自分自身を適応させることができる完全な自己学習型のシステムである可能性があります。いずれの場合も、元のシステムで成功したテストが、改良された新しいシステムでは実行できなくなることがあります。どのテストが使えなくなったかを特定するのは比較的簡単ですが、新しい機能のための新しいテストを確実に生成するのははるかに困難です。

また、自己学習型システムのもう一つの問題点は、システムがテストから望まない新しい行動を不用意に学習してしまうことです。

#### 6.1.5 柔軟性と適応性

AIベースのシステムの柔軟性と適応性のテストは、通常、環境の変更や突然変異に応じてシステムがどのように変化するかを観察することに基づいて行われます。システムの機能要件と非機能要件をテストする必要があり、理想的には自動化された回帰テストの形が適切なアプローチであることが多い。また、システムが行う変更プロセスをテストし、例えば、システムが要求された時間内に変更できるかどうか、変更を達成するために消費されたリソースに対してシステムが制約内に収まっているかどうかを判断する必要があります。

#### 6.1.6 オートノミー

AIベースのシステムの自律的な動作をテストするアプローチは、システムの自律的な動作を強制的に解除し、不特定の状況で介入を要求させることです（ネガティブテストの一形態）。また、ネガティブテストは、介入を要求すべき時に自律システムがコントロールされていると思い込ませるために「騙す」ことにも使用できます（例えば、運用範囲の境界にテストシナリオを作成することで、境界値の概念をシナリオテストに適用することを示唆しています）。

#### 6.1.7 進化

AIベースのシステムの進化（またはドリフト）をテストするには、通常、メインテナンステストの形で、頻繁に実行する必要があります。このテストでは、性能目標（例：精度、正確性、感度）など、指定されたシステム目標を監視し、システムにノーデータ・バイアスが導入されていないことを確認する必要があります（例：Microsoft
Tayチャットボット[^61]）。このテストの結果、システムの再トレーニングが必要になることがあります。トレーニングデータセットを更新する場合もあります。

#### 6.1.8 バイアス

AIシステムの偏りのテストは、2つの段階で行うことができます。1つ目は、レビューによって学習データの偏りを検出し、それを除去することですが、これには、偏りを生む可能性のある特徴を特定できる専門家のレビューが必要です。次に、偏りのないテストセットを用いた独立テストにより、システムの偏りをテストすることができます。トレーニングデータに偏りがあることがわかっている場合、偏りの原因を取り除くことが可能な場合があります（例えば、被験者の性別や人種を知る手がかりとなる情報をすべて取り除くことができます）。あるいは、システムにバイアスが含まれていることを（暗黙的にも明示的にも）認めた上で、トレーニングデータを公開することで透明性を確保することもできます。

#### 6.1.9 透明性、解釈可能性、説明可能性

AIベースのシステムの透明性をテストするには、主にアルゴリズムや使用するデータにアクセスできるかどうかを判断する必要があり、（文書やデータセットなどの参照資料の）レビューを通じて行うことができます。

AIベースのシステムの解釈可能性をテストするには、システムに実装されている基本的な技術に対する理解度がステークホルダーによって異なるため、聴衆に依存することになります。

AIベースのシステムの説明可能性をテストするには、対象者（またはテスターの代表者）にテストを実施してもらい、システムがさまざまな結果を出す仕組みをどれだけ簡単に理解できるかを判断してもらうのが理想的です。

#### 6.1.10 複雑さ

多くのAIベースのシステムは複雑であるため、テスト・オラクル問題が発生します。複雑なAIベースのシステムから得られる1つのテストケースの結果に合意するためには、複数の専門家が時間をかけて議論する必要があり、理想的には多くのテストを実行したいところですが、期待される結果を（ゆっくりと）生成する専門家に頼らなければならない場合は、実行不可能になります。A/Bテスト、バックトゥバックテスト、メタモルフィックテストなど、多くのテスト技法を用いて、テスト・オラクル問題に対処することができます（これらの技法の詳細については、第8節を参照してください）。

#### 6.1.11 確率的システムと非決定論的システム

多くのAIシステムは確率的な性質を持っているため、期待される結果として使用できる正確な値が常に存在するわけではありません。例えば、自律走行車が停車中のバスを迂回するルートを計画する場合、最適な解を計算する必要はなく、うまくいく（安全な）解を計算する必要があるため、最適ではないが十分な解を受け入れることになります。

また、AIベースのシステムがルートを決定する方法の性質上、毎回同じ結果にならないこともあります（例えば、ランダムな種に基づいて計算することで、毎回、無関係だが実行可能なルートが得られることがあります）。このようなシステムは非決定論的であるため、再現性に欠け、回帰テストでは、非決定論による変動性を考慮した、よりスマートな予想結果を得る必要があります。

いずれの場合も、実際の結果が不確実であるため、テスト担当者は、従来のシステムよりも、許容範囲を含めたより高度な予想結果を導き出す必要があります。また、確率論的なAIを用いたシステムでは、システムが正しく動作していることを統計的に有意に保証するために、同じテストを何度も行う必要があるかもしれません（モンテカルロ実験のように）。

#### 6.1.12 AIベースのシステムのテストオラクル問題

AIベースのシステムをテストする際に繰り返し発生する課題が、テストオラクル問題です。複雑で、確率的で、自己学習的で、非決定論的なシステムの仕様が不十分なため、期待される結果が得られないという問題があります。

テストオラクル問題に対処するテストアプローチとテクニックは、第8節のブラックボックステストに記載されています。

## 6.2 ライフサイクルを通じたAIベースのシステムのテスト

#### 6.2.1 一般

本節では、AIベースのシステムのライフサイクルにおける様々なテストレベル（テストフェーズと呼ばれることもある）について簡単に考察する。ライフサイクルの形態（アジャイル、ウォーターフォール、V字型、反復型など）については想定していません。これらのテストレベルは、使用するライフサイクルに関係なく、通常適用されるべきものです。すべてのテストと同様に、異なるレベルでのテストの選択は、認識されたリスクとテストのコストに基づいて行われるべきである。一般的に、初期のテストレベル（ユニットテストや統合テストなど）でのテストはコストが低く、これらのレベルで対処できるリスクはできるだけ早い段階でテストすべきである。しかし、一部のリスク（完全なシステムの特性に基づくものなど）は、完全なシステムをテストすることでしか対処できないため、システムテストレベル（エンドツーエンドのシナリオテストなど）で対処する必要がある。

6.1.1で述べたように、AIベースのシステムは、通常のコンポーネントとAIコンポーネントで構成されています。本節では、AI開発フレームワークのテストの詳細（4.2.6参照）は考慮しませんが、結果としてのAIコンポーネント（MLモデルなど）のテストは考慮します。AIベースのシステムは、多くの場合、3つの部分に分けて考えることができます。AIベースのシステムは、AIコンポーネント、データ、ユーザーインターフェースの3つの部分に分けて考えることができます。AIコンポーネントは、再利用可能なソフトウェアコンポーネントと同様にテストされることが多く、ユーザーインターフェースは、一般的なユーザーインターフェースと同様にテストされます。しかし、AIベースのシステムのデータは、6.2.2から6.2.7で説明するように、若干異なる方法でテストする必要があります。

ライフサイクル全体でのテスト環境の使用については、10.1を参照してください。

#### 6.2.2 ユニット/コンポーネントテスト

非AIコンポーネント（例：ユーザーインターフェースコード）のユニット/コンポーネントテストは、従来のシステムと同じように扱う必要があります。

MLモデルのユニットテストやコンポーネントテストは、MLワークフローの評価やテストの段階に相当します（A.2.8、A.2.10参照）。従来の開発者によるユニットテストと同様に、このレベルのテストで不具合が報告されることは非常に稀であり、主な目的は、納品されるモデルの品質を向上させることにあります。

MLのパフォーマンス指標（A.8.1参照）が（モデルレベルの）受け入れ基準として設定されている場合、MLモデルはこのテストレベルでこれらの基準に対してテストされます（受け入れ基準は、特定のMLモデルを選択する評価・調整活動の一部となる場合があります）。

ユニットテストレベルでのカバレッジは、伝統的に要件やコードカバレッジ（例：ステートメント、ブランチ、デシジョンのカバレッジ）のいずれかに関係しています。しかし、MLコンポーネントのカバレッジは、データセット（トレーニング、検証、テスト）の代表性によって測定することができます。

データが前処理されている場合、ユニットテストを使用して前処理をチェックすることができます（例：生データが正しくスケーリングまたは正規化されているかどうか）。

#### 6.2.3 統合テスト

AIコンポーネントが、より大きなAIベースのシステムの一部である場合、そのシステムに統合される必要があります。このようなAIベースのコンポーネントの統合には、2つの主要なアプローチがあります。まず第一に、最もシンプルに、AIベースのシステム全体に組み込まれたコンポーネントとして扱うことができます。

第二に、AIコンポーネントは、サービスとして提供することができます（通常、WebサービスなどのWeb上で提供されます）。この場合、AIコンポーネントは、AIベースのシステムの他の部分とは独立して提供され、サービスが必要なときに呼び出されます。

統合テストは、AIコンポーネントが、そのコンポーネントが属するAIベースのシステムの残りの部分と正しく統合されていることを確認するために実施されなければなりません（例えば、インターフェースのチェックや、伝達されたデータが正しく解釈されていることなど）。例えば、物体認識のためのモデルに正しい画像ファイルが渡されているか、モデルが期待するフォーマットになっているかを確認するテストを実施する必要があります。また、モデルの出力が正しく解釈され、システムの他の部分で使用されていることを確認するためのテストも実施する必要があります。

#### 6.2.4 システムテスト

従来のシステムと同様に、AIベースのシステムのシステムテストでは、機能テストと非機能テストの両方が行われます。非機能テストの対象となるのは、一般的に、セキュリティと性能効率（応答時間など）です。性能効率は、AIベース・システム全体のAIコンポーネントが、組み込みコンポーネントとしてではなく、サービスとして提供される場合に、特に関連する可能性があります。従来のシステムに適用される品質特性（例えば、ISO/IEC
25010[^62]で定義されているもの）に加えて、5.1に記載されているAI特有の特性（例えば、説明可能性）も、このテストレベルでのテストのために考慮されるべきである。

MLの性能指標（A.8参照）が（システムレベルの）受け入れ基準として設定されている場合、AIベースのシステムは、このテストレベルでこれらの基準に対してテストされます。

#### 6.2.5 システム統合テスト

システム統合テストは、AIベースのシステムが他のシステムからの大量のデータを使用する場合や、システムが1つまたは複数のIoTデバイスと相互作用する場合に、特に関連する可能性があります。

#### 6.2.6 受入検査

受け入れテストの一環として、ビジネス上の受け入れ基準をテストする必要がある。これらの基準は、モデルから得られる結果の正確さなどの技術的な基準ではなく、AIベースのシステムが、お金を稼ぐ、節約するなどのハイレベルなビジネス目標を満たしているかどうかに焦点を当てるのが一般的です。

MLパフォーマンス指標（A.8参照）が受け入れ基準として設定されている場合、AIベースのシステムは、このテストレベルでこれらの基準に対してテストされます。

人間はテクノロジーに過度に依存することがあり、AIシステムが人間をループに含めている場合、テスト対象のシステムの人間と自動化された出力の品質が正しくない場合があります。このような場合には、予測や入力済みのフィールドなど、ユーザーが承認した情報の精度を測定することが重要になります。

#### 6.2.7 メンテナンス・テスト

システムの進化に伴う問題として、AIベースのシステムが当初の受け入れ基準（ビジネス面、技術面）を満たしているかどうかを確認するために、定期的にテストを行う必要がある場合があります。

これらの基準がパフォーマンスメトリクス（A.8参照）として指定されている場合、これらのテストは自動化することができます。

保守テストの一環として回帰テストを使用する場合、多くのAIベースのシステムは確率的で非決定論的な性質を持っているため、システムが単に別の結果を提供しているにもかかわらず、明らかにテストが失敗してしまうことがありますが、これは許容範囲内です。これは、リグレッションテストの期待される結果が、決定論的なシステムに使用されるものよりもスマートでなければならないことを意味しています（例えば、許容範囲を含めて）。

運用中の自己学習システムをテストする際には、テストによってシステムが望まない学習をしてしまわないように注意する必要があります。

## 7 MLシステムのテストとQA

## 7.1 MLシステムのテストとQAの紹介

この条項では、MLに直接関連する品質保証とテストの機会を簡潔に示しています。

## 7.2 MLのワークフローの見直し

ML を実施する際には、使用する ML
ワークフローを文書化し、それに従わなければならない。附属書 A
に記載されているワークフローからの逸脱は、正当化されるべきです。

## 7.3 受入基準

受入基準（機能的要件と非機能的要件の両方を含む）を文書化し、このアプリケーションでの使用を正当化すること。モデルの性能評価指標を含めること。最低限、AI固有の特性（5.1に記載）を考慮する必要があり、AIベースのシステムの受け入れ基準の完全性を判断するためのチェックリストのベースとして使用することができます。

## 7.4 フレームワーク、アルゴリズム/モデル、ハイパーパラメータの選択

フレームワーク、アルゴリズム、モデル、設定、ハイパーパラメータの選択は、文書化して説明しなければなりません。

## 7.5 トレーニングデータの品質

MLシステムは、学習データが運用データを代表しているかどうかに大きく依存しており、MLシステムの中には、自律走行車に使用されているような広範な運用環境を持つものもあります。

境界条件は、あらゆる種類のシステム（AIおよび非AI）において故障の原因となることが知られており、トレーニングデータに含めるべきである。データセットのサイズや、バイアス、透明性、完全性などの特徴を考慮したトレーニングデータの選択は、文書化され、正当化されるべきであり、システムに関連するリスクのレベルがそれを必要とする場合には、専門家によって確認されるべきである（例：重要なシステム）。

## 7.6 データ品質のテスト

トレーニングデータの基準はテストデータにも同様に適用されますが、テストデータはトレーニングデータから可能な限り独立していなければならないという注意点があります。独立性のレベルは文書化され、正当化されるべきである。テストデータは体系的に選択および／または作成し、ネガティブテスト（例：想定される入力範囲外の入力）および敵対的なテストも含めるべきである（詳細は7.8を参照）。

## 7.7 モデルアップデート

デプロイされたモデルが更新されるたびに、モデルの劣化（新しいモデルが以前のモデルよりも動作が遅くなるなど）のテストなど、文書化されていない暗黙の要件に対するテストを含め、引き続き受け入れ基準を満たしていることを確認するために再テストを行う必要があります。適切な場合には、以前のモデルに対してA/Bテストやバックツーバックテストを行うべきです。

## 7.8 敵対的な事例とテスト

逆説的な例とは，ニューラルネットワークの入力に極めて小さな変更を加えると，出力に予想外の（誤った）大きな変化が生じる（つまり，入力が変更されていない場合とは全く異なる結果になる）というものである[^63]．逆説的な例が最初に注目されたのは、画像分類器でした。人間の目には見えない数個のピクセルを変更するだけで、ニューラルネットワークに、全く異なる対象物への画像分類を変更するよう説得することができます（しかも、高い信頼度で）。しかし、敵対的な例は、画像分類器に限らず、ニューラルネットワーク一般の既知の属性であり、ニューラルネットワークのあらゆる用途に適用されることに注意してください（他の形態のMLモデルにも適用される可能性があります）。

敵対的事例は一般的に伝達可能です。つまり、あるニューラルネットワークを失敗させる敵対的事例は、同じタスクを実行するように訓練された他のニューラルネットワークを失敗させることが多いということです。これらの他のニューラルネットワークは、異なるデータや異なるアーキテクチャに基づいてトレーニングされている可能性がありますが、同じ敵対的事例で失敗する可能性があることに注意してください。

敵対的なテストとは、敵対的な攻撃を行うことを指します。このような攻撃を行い、テスト中に脆弱性を特定することで、将来の障害に対する対策を講じることができ、ニューラルネットワークの堅牢性を向上させることができます。

攻撃は、モデルをトレーニングするときと、トレーニングされたモデル（ニューラルネットワーク）自体に対して行われます。

学習時の攻撃には、学習データの破損（ラベルの変更など）、学習セットへの不良データの追加（不要な特徴など）、学習アルゴリズムの破損などがあります。学習したモデルへの攻撃は、ホワイトボックスまたはブラックボックスのいずれかで、モデルに悪い結果を出させるような敵対的な例を特定することになります。

ホワイトボックス攻撃では、攻撃者はモデルの学習に使用されたアルゴリズムや、使用された設定やハイパーパラメータを完全に把握しています。この知識を利用して、例えば、入力に小さな摂動を加え、どの摂動がモデルに大きな変化をもたらすかを監視することで、敵対的なサンプルを生成します。

ブラックボックス攻撃では、攻撃者はモデルの内部構造にアクセスできず、モデルがどのように学習されたかについても知りません。この場合、攻撃者は最初にモデルを使用してその機能性を判断し、次に同じ機能を提供する「複製」モデルを構築します。その後、攻撃者はホワイトボックス手法を用いて、この複製モデルの敵対的な事例を特定します。敵対的な事例は一般的に転用可能であるため、通常、同じ敵対的な事例が（ブラックボックス）モデルにも適用されます。

## 7.9 機械学習のベンチマーク

理想的には、新しいMLシステムを専門家が評価することですが、それではコストがかかりすぎます。

その代わりに、「代表的な」業界標準のベンチマーク・スイートが用意されており、さまざまな状況（画像分類、物体検出、翻訳、推薦など）をカバーする多様な作業負荷が含まれています。

これらのベンチマーク・スイートは、ハードウェア（定義されたモデルを使用）とソフトウェア（最速のモデルを決定するなど）の両方の性能を測定するために使用できます。ソフトウェアのベンチマークスイートでは、学習（定義された学習データセットを用いて、フレームワークがMLモデルを75%の精度などの指定された目標品質指標に向けて学習する速さなど）および推論（学習されたMLモデルが推論を実行する速さなど）を測定することができます。

MLのベンチマークセットの例としては、ソフトウェアフレームワーク、ハードウェアアクセラレータ、MLクラウドプラットフォームのベンチマークを提供するMLPerf[^64]や、スタンフォード大学のベンチマークスイートであるDAWNBench[^65]などがあります。OAEI（Ontology
Alignment Evaluation
Initiative）は、以下を目的とした国際的な協調活動です[^66]。

- アライメント/マッチングシステムの強みと弱みを評価する。
- テクニックの性能比較
- アルゴリズム開発者間のコミュニケーションを深める。
- 評価技術の向上
- オトロジーのアラインメント/マッチングに関する作業を改善するのに役立ちます。

これらの目標は、技術の性能を制御された実験的な評価によって達成されます。

## 8 AIベースのシステムのブラックボックステスト

## 8.1 コンビナトリアル・テスト

あるテスト項目が、与えられたすべての状況下で、すべての要求を満たしていることを動的テストで証明するためには、すべての可能な状態における入力値のすべての組み合わせをテストする必要があります。このような非現実的な行為は「網羅的テスト」と呼ばれています。そのため、実際のソフトウェアテストでは、可能な入力値と状態の（非常に大きな）セットからサンプリングしてテストスイートを導き出します。組合せテストは、この入力空間から有用な組み合わせのサブセットを導き出すための、体系的かつ効果的なアプローチのひとつである[^67]。

対象となる組み合わせは、パラメータ（入力や環境条件）と、そのパラメータが取り得る値で定義されます。多数のパラメータ（それぞれが多数の離散的な値を持つ）を組み合わせることができれば、テストスイートの欠陥検出能力を損なうことなく、必要なテストケースの数を大幅に減らすことができます。

ISO/IEC/IEEE
29119-4では、全組合せ、各選択肢テスト、基本選択肢テスト、ペアワイズテストなどの組合せテスト手法を定義している[^68]。実際にはペアワイズテストが最も広く使用されているが、これは理解しやすいこと、ツールのサポートが充実していること、そして欠陥の多くが少数のパラメータを含む相互作用によって引き起こされるという研究結果が主な理由[^69]である。

特に、ビッグデータを利用する場合や、自動運転車のように外界と相互作用する場合には、AIベースのシステムにとって関心のあるパラメータの数は非常に多くなります。そのため、ペアワイズテストのような組み合わせテストを用いて、無限に近い数の組み合わせを管理可能なサブセットにまで体系的に削減する手段は非常に有用です。実際には、ペアワイズテストを使用したとしても、このようなシステムのテストスイートは膨大な量になるため、自動化や仮想テスト環境（10.1参照）の使用が必要になることがあります。

自動運転車を例にとると、システムテストのシナリオには、車両のさまざまな機能と、それらが動作することが期待される環境の両方を考慮する必要があります。そのため、パラメータには、様々な自動運転機能（クルーズコントロール、アダプティブクルーズコントロール、レーンキープアシスト、レーンチェンジアシスト、トラフィックライトアシストなど）と、環境制約（道路の種類や路面、地理的エリア、時間帯、天候、交通状況、視界など）が含まれる必要があります。これらのパラメータに加えて、センサーからの入力を様々なレベルで考慮する必要があります（例えば、ビデオカメラからの入力は、旅が進むにつれて汚れてくると劣化しますし、GPSユニットの精度は、様々な数の衛星が視界に入ったり入らなかったりすると変化します）。自動運転車など、安全性が重視されるAIベースのシステムにコンビナトリアルテストを適用する場合、どの程度の厳密さが求められるのかについては、現時点では不明ですが（例えば、ペアワイズでは不十分かもしれません）、不具合の発見に効果的であることや、残存するリスクのレベルを推定するために利用できることは知られています。

## 8.2 バックトゥバック・テスト

バックトゥバックテストでは、システムの別バージョン（既に存在するもの、別のチームが開発したもの、別のプログラミング言語で実装されたものなど）を疑似的に使用し、同じテスト入力から期待される結果を生成して比較します。これは「差分テスト」と呼ばれることもあります。

このように、バックトゥバックテストは、テスト入力が生成されないため、テストケース生成技術ではありません。

擬似オラクル（機能的に等価なシステム）により、期待される結果のみが自動的に生成されます。テスト入力を生成するツール（ランダムなど）と併用することで、大量の自動テストを強力に行うことができます。

機能テストをサポートするためにバックトゥバック・テストを使用する場合、擬似オラクルは、テスト対象のシステムと同じ非機能的な制約を満たす必要はありません。例えば、テスト対象のシステムで必要とされる速度よりもはるかに遅い速度で擬似的に動作させることができます。また、機能的に完全に等価なシステムである必要はなく、テスト対象システムの一部としか等価でない擬似的なものでもバックトゥバックテストを行うことができます。

MLでは、さまざまなフレームワーク、アルゴリズム、設定を用いて擬似的なオラクルを作ることができます（場合によっては、AIではない従来のソフトウェアを用いて擬似的なオラクルを作ることも可能です）。擬似オラクルを使用する際の既知の問題は、うまく機能するためにはテスト対象のソフトウェアから完全に独立している必要があることです。AIベースのシステムの開発には、再利用可能なオープンソース・ソフトウェアが多く使用されているため、この独立性が簡単に損なわれてしまうのです。

## 8.3 A/Bテスト

A/Bテストとは、2つのシステムのどちらの性能が優れているかを判断するための統計的テスト手法[^70]です。A/Bテストは、デジタルマーケティング（例：反応の良いメールを見つける）や、顧客対応の場面でよく使われます。

例えば、ユーザーインターフェースの設計を最適化するために、A/Bテストがよく使われます。例えば、ユーザーインターフェースの設計者が、「購入」ボタンの色を現在の赤から青に変更することで、売上が増加するという仮説を立てます。そこで、ボタンの色を青にしたインターフェースを新たに作成し、2つのインターフェースを別々のユーザーに割り当てました。2つのバリエーションの販売率を比較し、統計的に有意な数の使用があれば、仮説が正しかったかどうかを判断することができました。

青いボタンがより多くの売上を生み出した場合、青いボタンを持つ新しいインターフェイスが、赤いボタンを持つ現在のインターフェイスに取って代わることになります。このようなA/Bテストは、統計的に有意な数の使用を必要とし、時間がかかりますが、ツール（多くの場合、AIを使用）を使ってサポートすることができます。

A/Bテストは、テスト入力が生成されないため、テストケース生成技法ではない。A/Bテストは、既存のシステムを部分的なオラクルとして利用することで、テストのオラクル問題を解決する手段である。新システムと既存システムを比較することで、新システムが何らかの意味で優れているかどうかを判断することができる。デジタルマーケティングの場合、成功の尺度はより多くの売上かもしれませんが、分類器のようなAIベースのシステムでは、精度、感度、リコールなどのパフォーマンス指標を使用することができます（A.8参照）。

A/Bテストは、AIベースのシステムのコンポーネントが更新されたときに、受け入れ基準（例：「特定のパフォーマンス指標が改善されるか、同じままであること」）が定義され、合意されていれば、いつでも使用することができます。

A/Bテストが自動化されている場合、妥当な受け入れ基準が設定されていれば、自己学習するAIベースのシステムのテストに使用することができます。システムの新しいパフォーマンスと以前のパフォーマンスを比較し、自己学習によってシステムのパフォーマンスが向上しなかった場合は、以前のバージョンに戻します。

## 8.4 メタモルフィック・テスト

メタモルフィックテスト[^71][^72]は、AIベースのシステムでよく見られる、テストの合否判定が困難なテストラクル問題（複雑性、非決定性、確率的なシステムなど）に対処するためのテストケース生成手法である。メタモルフィックテストで生成されるテストケースと従来のテストケース設計手法との主な違いは、メタモルフィックテストの期待値が固定値ではなく、別の期待値との関係で定義されることです。

メタモルフィックテストでは、メタモルフィック関係を用いて、正しいことがわかっているソーステストケースからフォローアップテストケースを生成する。テスト対象ソフトウェアのメタモルフィック関係は、ソーステストケースからフォローアップテストケースへのテストインプットの変化が、ソーステストケースからフォローアップテストケースへの期待されるアウトプットにどのような影響を与えるのか（または与えないのか）を説明するものである。これらの期待されるメタモルフィックな関係は、実施されるテストの部分的なオアシスと考えることができる。

例 1
始点と終点の間の距離を測定するテスト項目があります。ソースとなるテストケースには、テスト入力A（始点）、B（終点）と、テストケースを実行したときの期待結果C（距離）があります。変形関係とは、始点と終点が入れ替わっても、期待される結果は変化しないというものです。したがって、Bを始点、Aを終点、Cを距離とした追試テストケースを生成することができる。

例2
あるテスト項目では、一連のライフスタイルのパラメータに基づいて個人の死亡年齢を予測します。ソースのテストケースには、1日に吸うタバコの本数が10本など、さまざまなテスト入力があり、テストケースを実行した結果、58歳という期待値が得られます。メタモルフォーゼ関係は、人がより多くのタバコを吸えば、その人の予想死亡年齢はおそらく減少する（増加しない）というものです。したがって、同じライフスタイル・パラメータの入力セットで、喫煙本数を1日20本に増やした以外は、フォローアップのテストケースを生成することができます。このフォローアップテストケースの期待される結果（予測死亡年齢）は、58歳以下または58歳と同じに設定することができます。

フォローアップテストケースの期待値は、必ずしも正確な値ではなく、ソーステストケースを実行して得られた実際の結果の関数として記述されることが多い（例：フォローアップテストケースの期待値はソーステストケースの実際の結果よりも大きい）。

1つのメタモルフィック関係が複数のフォローアップテストケースを導き出すために使用されることがよくある（例えば、音声をテキストに変換する関数のメタモルフィック関係は、同じ音声入力ファイルを異なる入力音量レベルで使用し、同じテキストを期待する結果として、複数のフォローアップテストケースを生成するために使用することができる）。メタモルフィック関係が形式的（または半形式的）に記述され、ソーステストケースが提供されていれば、フォローアップテストケースの生成を自動化することができるはずである。ただし、ドメインの知識を必要とするメタモルフィック関係の生成を自動化することは不可能である。

メタモルフォーゼ・テストを行うためのプロセスは

a)  ::: {.Definition-Term}
    メタモルフィック・リレーションズ（MR）の構築
    :::

b)  テスト対象のプログラムの特性を特定し、それをテストの入力と期待される出力の間の変態的な関係として表現し、ソースのテストケースに基づいてフォローアップのテストケースを生成するための何らかの方法を提供します。

c)  ::: {.Definition-Term}
    MRの見直し
    :::

d)  お客様やユーザーとのMRの確認。

e)  ::: {.Definition-Term}
    ソーステストケースの生成
    :::

f)  ソーステストケースのセットを生成します（任意のテスト技法またはランダムテストを使用）。

g)  ::: {.Definition-Term}
    フォローアップテストケースの作成
    :::

h)  メタモルフォーゼの関係を利用して、フォローアップのテストケースを生成します。

i)  ::: {.Definition-Term}
    メタモルフィック・テストケースの実行
    :::

j)  ソーステストケースとフォローアップテストケースの両方を実行し、出力がメタモルフィック関係に違反していないことを確認します。そうでない場合は、メタモルフィックテストケースが失敗したことになり、バグを示します。

メタモルフィックテストは、従来のソフトウェアに加えて、バイオインフォマティクス、ウェブサービス、機械学習の分類法、検索エンジン、セキュリティなど、AIを活用した幅広い分野で利用されています。研究によると、3〜6種類の多様なメタモルフィック関係だけで、従来のテストオラクルを使って検出できる断層の90％以上を明らかにできるそうです\[\^42\]。

### 8.5 探索的テスト

テストの設計と実行は、各プロジェクトのニーズに応じて、さまざまな方法で行うことができます。台本がある場合もあれば、探索的な場合もあります。実際には、スクリプトテストと探索的テストを組み合わせて使用するのが一般的です。スクリプトテストは、要求されるテストカバレッジレベルを確実に達成し、自動テストをよりよくサポートする一方で、探索的テストは、創造性を発揮してテストを迅速に実行することができます。AIベースのシステムをテストする際には、アジャイル開発のように仕様が貧弱であったり、無駄があったりする場合には、探索的テストが有効であると考えられています。

探索的テストでは、テスト担当者がテスト項目に触れて学習しながら、その場でテストを設計・実行します。セッションシートは、探索的なテストセッションを構成するためによく使われます（例えば、各テストセッションの焦点と制限時間を設定するなど）。また、同じセッションシートを使って、テスト内容や異常な動作などの情報を記録しています。探索的テストは、完全にスクリプトがないわけではなく、セッションシートにハイレベルなテストシナリオ（「テストアイデア」と呼ばれることもある）を記録して、探索的テストセッションの焦点とすることが多い。

### 9 ニューラルネットワークのホワイトボックステスト

### 9.1 ニューラルネットワークの構造

ニューラルネットワークとは、人間の脳内にあるニューラルネットワークをヒントにした計算モデルです。ここでは、人工ニューラルネットワークの中でも最もシンプルなタイプであるフィードフォワード・ニューラルネットワークを例に挙げていますが、唯一複雑なのは、多層パーセプトロンと呼ばれる複数の層を持つネットワークを考えることです。

図3 - ディープニューラルネット

入力ノードは外界から情報を受け取り（例えば、各入力は画像のピクセルに対する値であることがある）、出力ノードは外界に情報を提供する（例えば、分類）。隠れた層のノードは外界との接続がなく、入力ノードから出力ノードへの情報伝達のための計算を行います。

図4に示すように、各ニューロンは入力値を受け取り、活性化値（または出力ベクトル）と呼ばれる出力値を生成します。活性化値は正、負、ゼロ（ゼロの場合、ニューロンは下流のニューロンに影響を与えない）のいずれかです。各接続には重みがあり（ネットワークの学習に応じて変化します）、各ニューロンにはバイアスがあります（ここでいうバイアスは、5.1.5項で説明したアンフェアネスに関連するバイアスとは全く異なります）。活性化値は、入力された活性化値、入力接続の重み、ニューロンのバイアスをもとに、ある式（活性化関数）によって算出されます。

図4-ニューロンの活性化値

教師付き学習では、ネットワークは後方伝搬によって学習します。最初に、すべてのノードが初期値に設定され、最初の入力トレーニングデータがネットワークに渡され、通過します。のです。

算出された出力と正解との差（誤差）は、ネットワークの前の層にフィードバックされ、重みの修正に使用されます。この逆方向の誤差伝搬は、ネットワーク全体をさかのぼり、各接続の重みが適切に更新されます。学習データが増えれば増えるほど、ネットワークは徐々にエラーを学習していき、学習したネットワークの性能を決定する検証データでの評価が可能になると考えられています。

### 9.2 ニューラルネットワークのテストカバレッジ指標

### 9.2.1 テストカバレッジレベルの紹介

従来のカバレッジ指標は、単一のテストケースで100%のステートメントカバレッジが達成されることが多いため、ニューラルネットワークにはあまり役に立ちません。欠陥は通常、ニューラルネットワーク自体に隠されています。

そのため、ニューラルネットワークがテストされたときのニューロン（またはニューロンのペア）の活性化値に基づいて、さまざまなカバレッジ尺度が提案されています。

ニューラルネットワークのカバレッジを測定することで，テスト担当者はカバレッジを最大化することができ，自動運転車システムなどのAIベースのシステムにおける不正な動作を特定できることがわかっている\[\^43\]\[\^44\]．

### 9.2.2 ニューロンのカバレッジ

一連のテストにおけるニューロンカバレッジは、活性化されたニューロンの割合をニューラルネットワークの全ニューロン数で割ったものと定義されます（通常はパーセンテージで表されます）。ニューロンのカバレッジについては、ニューロンの活性化値がゼロを超えると活性化されたとみなされます。

### 9.2.3 スレッショルドカバレッジ

一連のテストにおける閾値カバレッジは，閾値を超えるニューロンの割合をニューラルネットワークのニューロン総数で割ったものと定義されます（通常はパーセンテージで表されます）。スレッショルドカバレッジでは、0と1の間のスレッショルドアクティベーション値をスレッショルド値として選択します。なお、この閾値のカバー率は、DeepXploretool\[\^44\]の「neuron
coverage」に相当します。

### 9.2.4 サイン変更の適用範囲

あるテストセットの符号変更カバレッジは、正負両方の活性化値で活性化されたニューロンの割合をニューラルネットワークの全ニューロン数で割ったものと定義されます（通常はパーセンテージで表されます）。活性化値が0の場合はnegativeactivation値とみなされます\[\^45\]。

### 9.2.5 バリューチェンジカバレッジ

一連のテストにおける値変更カバレッジは、活性化されたニューロンのうち、その活性化値に変更量以上の差があるものの割合を、ニューラルネットワークのニューロン総数で割ったものと定義されます（通常はパーセンテージで表されます）。バリューチェンジカバレッジでは、変更量として0から1の間の値を選択する必要があります\[\^45\]。

### 9.2.6 サイン・サインカバー

符号を変更した各ニューロン（9.2.4
参照）が、次の層の他のすべてのニューロンが同じままである（つまり、符号を変更しない）にもかかわらず、次の層の1つのニューロンの符号を個別に変更させることができれば、一連のテストの符号カバレッジが達成されます。概念的には、このレベルのニューロンカバレッジは、Modified
Condition/Decision Coverage (MC/DC) に似ています\[\^45\]。

### 9.2.7 レイヤカバレッジ

カバレッジメジャーは、ニューラルネットワークの全層に基づいて定義することもでき、全層のニューロンセットの活性化値がどのように変化するか（例えば、絶対的または相対的に）を示します。

この分野ではさらなる研究が必要です。

### 9.3 ホワイトボックス対策の有効性の検証

現在、ニューラルネットワークのホワイトボックステストにおける、さまざまなホワイトボックスカバレッジ測定のテスト効果に関するデータはほとんどありません。しかし、一般的には、より多くのテストを必要とする基準は、より少ないテストを必要とする基準よりも多くの欠陥を発見することになり、テーマ・トレジャーの相対的な有効性を推測することができるのです。9.2.1～9.2.5で説明したカバレッジ測定から、いくつかのサブサーム関係を導き出すことができます。他のすべての指標はニューロンカバレッジを含み、サインカバレッジはサインチェンジカバレッジを含みます。これらのサブサム階層の全体像を図5に示します。
矢印があるメジャーから別のメジャーを指している場合、最初のメジャーが完全に達成されていれば、2番目のメジャーも自動的に達成されることを意味しています。例えば、閾値のカバー率が達成されれば、ニューロンのカバー率も自動的に達成されることを示しています。

図5 - ホワイトボックス型ニューラルネットワークが階層を覆う

理解しやすいのですが、高いレベルのニューロン・カバレッジを達成するためには、通常、少数のテスト・ケースしか使用できないため、テストの有効性が制限されてしまいます。しきい値カバレッジの初期の結果では、欠陥を引き起こすコーナーケースをカバーするテストを生成するための有用な指標になると思われますが、しきい値は各ニューラルネットワークに個別に設定する必要があるかもしれません。値変更カバレッジでは、変更量の値が大きいほど、当然、より多くのテストケースが必要になります。サイン・サインのカバー率は、通常、ここで規定されているカバー率の基準の中で最も厳格なものです\[\^45\]。

### 9.4 ニューラルネットワークのホワイトボックステストツール

現在、ニューラルネットワークのホワイトボックステストをサポートする商用ツールはありませんが、以下のような研究用ツールがあります。

\-
DeepXploreは，ディープニューラルネットのテストに特化し，ネットワーク内のすべてのニューロンをカバーする敵対的な例を系統的に生成するホワイトボックス・ディファレンシャル・テスト（バック・トゥ・バック）アルゴリズムを提案しています（閾値カバレッジ）\[\^44\]．

\- DeepTest -
ディープニューラルネット\[\^46\]で駆動する自動車の誤動作を自動的に検出するためのシステマティックなテストツールで、DNNのサインカバーに対応しています。

\- DeepCover -
この条項\[\^45\]で定義されたすべてのレベルの補償を提供します。

### 10 AIを用いたシステムのテスト環境

### 10.1 AIベースのシステムのテスト環境

AIを用いたシステムのテスト環境は、ユニットレベルの開発環境と、システムレベルやアクセプタンスレベルの本番同様のテスト環境という、従来のシステムと共通する部分が多い。MLモデルを単独でテストする場合は、A.2.9で述べたように、その開発フレームワークの中でテストするのが一般的です。

AIを用いたシステムのテスト環境は、従来のシステムで必要とされていたものとは異なり、大きく分けて2つの要因があります。第一に、自律システムなどのAIベースのシステムが動作する状況は、その環境が大きく、複雑で、常に変化することを意味します。

そのため、可能性のある環境をすべてテストし、テスト環境が現実的であることが期待され、テストを適切な期間内に実施する場合、現実世界でのテストは非常に高価になります。2つ目は、人間と物理的に対話できるAIベースのシステムには、安全性の要素が含まれているため、現実世界でのテストが危険であるということです。この2つの要素が、仮想テスト環境の必要性を示しています。

仮想テスト環境には、特に以下のようなメリットがあります。

\-
仮想環境を使用することで、テスト対象のシステムやその環境にある車両、建物、動物、人間などのオブジェクトにダメージを与えることなく、危険なシナリオを安全にテストすることができます。また、仮想環境でのテストは、一般的に実環境でのテストよりも優れています。

\-
仮想環境はリアルタイムで実行する必要はなく、適切な処理能力があればより高速に実行できるため、短期間で多くのテストを実行することができ、市場投入までの時間を大幅に短縮できる可能性があります。また、1つのシステムを、クラウド上の多数の仮想テスト環境で並行してテストすることもできます。

\-
仮想環境は、現実の環境に比べて設置や運営にかかる費用が少なくて済みます。例えば、大きく異なる都市環境での携帯電話通信のテストは、実際の携帯電話を様々な場所で運転して行うよりも、仮想の電話機、送信機、風景を使って実験室で行った方がはるかに安上がりです。しかし、仮想テスト環境の中には、真に代表的なものであり、いくつかの点で現実世界に近いものでなければならないことに留意する必要があります。例えば、自律走行車の歩行者回避のテストでは、高いレベルの画像表現力が求められます。

\-
普段とは違う（エッジケース）シナリオを作ることは、現実世界では非常に困難な場合がありますが、仮想環境ではそのようなシナリオを作ることができます（また、そのような珍しいシナリオを何度も繰り返し実行することができます）。バーチャルな環境では、実物を使ったテストよりも高いレベルでのコントロールが可能です。例えば、自律走行車のテストにAIを使った人間を参加させるなど、ランダム性を持たせることも可能です。

\-
ハードウェアのシミュレーションをサポートすることで、仮想環境では、ハードウェアコンポーネントが物理的に入手できなくても（おそらくまだ構築されていなくても）、システムをテストすることができ、異なるハードウェアソリューションを安価に試して比較することができます。

\-
仮想環境は優れた観測性を備えているため、シナリオに対する被試験システムの反応のあらゆる側面を測定し、必要に応じてその後の分析を行うことができます。

\-
例えば、原子力発電所の事故現場で働くロボットや、宇宙開発のためのシステムなど、実際の運用環境ではテストできないシステムを仮想環境でテストすることができます。

バーチャルテストは、特定のシステム専用に作られたシミュレータで行うことができますが、特定のドメインに再利用可能なシミュレータは、商業的にもオープンソースなどでも入手可能です。

\- モース（Modular Robots Open Simulation
Engine）は、Blenderのゲームエンジンをベースにした、汎用的な移動ロボットのシミュレーション（単一または複数のロボット）のためのシミュレータです\[\^48\]。

\- Facebook AIが開発したシミュレーション・プラットフォーム「AI
Habitat」は，写真のようにリアルな3D環境で具現化されたエージェント（仮想ロボットなど）を訓練するために設計されている\[\^49\]．

\- DRIVE
Constellationは、NVIDIAが提供する自動運転車のためのオープンでスケーラブルなプラットフォームで、クラウドベースのプラットフォームをベースにしており、何十億マイルもの自律走行テストを行うことができる\[\^50\]。

### 10.2 テストシナリオの作成

AIベースのシステムを体系的にテストするためには、個々のAIコンポーネント、これらのコンポーネントとシステムの他の部分との相互作用、相互作用するコンポーネントからなるシステム全体、およびシステムとその環境との相互作用をテストするためのテストシナリオを作成する必要があります。

テストシナリオはいくつかのソースから得ることができます。

\- システム要件

\- ユーザーの課題

\- 自動的に報告される問題（自律型システムの場合など

\- 事故報告書（物理的なシステムの場合など

\- 保険データ（例：自律走行車などの保険対象システムのデータ

\- 規制機関のデータ（例：法律によって収集されたもの

\-
様々なレベルでのテスト（例えば、テストコースや実在の道路でのテストの失敗や異常は、他のテストレベルでの自律走行車の興味深いテストシナリオを生み出す可能性があります。また、仮想テスト環境で実行されたテストシナリオのサンプルは、仮想テスト環境の妥当性を検証するために実在の道路でも実行されるべきです。

8.1では、自律走行車のシステムテストのためのテストシナリオを作成するために、コンビナトリアルテストを使用する方法を紹介しています。テストシナリオの作成には、メタモルフィックテスト（8.4参照）やファズテストも利用できます。

### 10.3 規定のテストシナリオとテスト環境

安全に関わるAIを使ったシステムの場合、ある程度の規制が適用できる。

この規制には、政府が開発組織の自主規制を認める方法と、システムが最低限の基準を満たしていることを独立して保証する規制機関を設置する方法（認証アプローチ）の2つがあります。

認証方式を採用する場合、自動車の衝突試験のように、規制機関と認証システムを提供する企業との間で試験方法を共有する必要がある。

このアプローチの中核となるのが、テスト環境の定義と、その環境上でテストオートメーションを使って実行できるテストシナリオの共有です。共有されるテストシナリオのコアセットは、オーバーフィッティングを防ぐために、テストごとにパラメータ値を変化させて新しいシナリオを生成できるようにパラメータ化する必要があります。パラメーター設定とプライベートシナリオは、既知のテストに合格するためだけにシステムを構築するのではなく、実際にシステムを使用することで潜在的な問題状況を認識した規制機関が、新たなシナリオを追加することができるようになっています。

### 付録A：機械学習

### A.1 機械学習への導入

機械学習（ML）はAIの一種であり、AIベースのシステムは、明示的にプログラムされるのではなく、提供された学習データから自分の行動を学習する。MLの結果はモデルとして知られており、AI開発フレームワークが選択されたアルゴリズムと学習データを用いて作成されます。最初に学習させたモデルは、そのまま使用されることが多いです。一方で、作成されたモデルは、運用しながら継続的に学習していく（＝自己学習）場合もあります。MLの用途としては、画像分類、囲碁などのゲーム、音声認識、セキュリティシステム（マルウェア検出）、航空機の衝突回避システム、自律走行車などが挙げられます。

機械学習（ML）には、図A.1に示すように、3つの基本的なアプローチがあります。

図A.1 - 機械学習の形態

教師付きMLでは、ラベル付けされたデータのトレーニングセットに基づいてアルゴリズムがモデルを作成します。例えば、教師付きMLでは、猫と犬の写真にラベルを付けたデータが与えられ、作成されたモデルが将来見た猫と犬を正しく識別することが期待されます。教師付き学習は、分類問題と回帰問題の2つの形式の問題を解決します。分類とは、モデルが入力を「はい
- このモジュールはエラーが発生しやすい」「いいえ -
このモジュールはエラーが発生しにくい」というように、異なるクラスに分類することです。回帰とは、「モジュールに含まれるバグの期待数は12である」といったように、モデルが値を提供することです。MLは確率的なものなので、これらの分類や回帰が正しい可能性を測定することもできます（MLのパフォーマンスメトリクスについてはA.8を参照）。

教師なしMLでは、学習セットのデータはラベル付けされていないため、アルゴリズムはデータ自体からパターンを導き出します。教師なしMLの例としては、提供されたデータが顧客に関するものであり、システムを使って特定の顧客グループを見つけ、特定の方法でマーケティングを行うような場合が挙げられます。訓練データはラベルを付ける必要がないため、教師付きMLの訓練データよりも簡単に（そして安価に）入手することができます。

強化学習では、システム（エージェント）に報酬関数が定義され、システムが要求された行動に近づくと、より高い報酬が返ってくるようになっています。報酬のフィードバックを利用する

の機能を利用して、システムはその動作を改善するように学習します。強化学習の例としては、報酬関数を使って最短ルートを探索するルート計画システムが挙げられます。

ISO/IEC
23053\[\^51\]では，機械学習を用いたAIベースのシステムのフレームワークが記述されており，この附属書の内容の一部がより詳細に説明されています。

### A.2 機械学習のワークフロー

### A.2.1 機械学習ワークフローの概要

機械学習のワークフローのアクティビティを図A.2に示します。

図A.2 - 機械学習のワークフロー

機械学習のワークフローにおける活動は、A.2.2からA.2.13に記載されています。

### A.2.2 目的を理解する

展開するMLモデルの目的を理解し、関係者と合意することで、ビジネスプロフェッショナルとの整合性を確保する必要があります。開発したモデルの受け入れ基準（性能評価基準を含む-4.1
参照）を定義する必要がある。

### A.2.3 フレームワークの選択

適切なAI開発フレームワークは、目的、受け入れ基準、ビジネスの専門家に基づいて選択する必要があります。これらのフレームワークについては、4.2.6で紹介します。

### A.2.4 モデルの構築とコンパイル

モデルの構造（層の数など）が定義されている必要があります（通常はPythonなどのソースコードになります）。次に、モデルがコンパイルされ、学習の準備が整います。

### A.2.5 データのソース

モデルで使用するデータは、目的に応じて使用します。例えば、そのシステムがリアルタイム・トレーディング・システムであれば、データは取引市場から得られます。マーケティングキャンペーンのために顧客の小売嗜好を分析するシステムであれば、組織の顧客ビッグデータがソースとなります。

モデルのトレーニング、チューニング、テストに使用するデータは、モデルで使用されることが予想される運用データを代表するものでなければなりません。場合によっては、モデルの初期トレーニングに事前に収集したデータセットを使用することも可能です（例：Kaggle
datasets at
https://www.kaggle.com/datasets）。しかし、生データは通常、何らかの前処理が必要です。

### A.2.6 データの前処理

モデルで使用するデータの特徴を選択する必要があります。これは、予測結果に影響を与える可能性が最も高いと思われる、データの属性や特性です。

トレーニングデータを管理して、結果のモデルに影響を与えない（または与えたくない）特徴を取り除く必要がある場合があります。これを特徴エンジニアリングまたは特徴選択と呼びます。特徴量エンジニアリングは、無関係な情報（ノイズ）を取り除くことで、全体の学習時間を短縮し、オーバーフィッティング（A.4.1参照）を防ぎ、精度を高め、モデルをより一般化することができます。

現実のデータには、異常値が含まれていたり、様々な形式があったり、重要な部分が欠けていたりすることがあります。そのため、通常、モデルの学習（およびテスト）に使用する前に、前処理が必要となります。前処理には、データの数値への変換、数値データの共通尺度への正規化、外れ値やノイズの多いデータの検出と除去、データの重複や欠落データの追加などがあります。

### A.2.7 モデルのトレーニング

MLアルゴリズム（例：4.2.4の機械学習技術を参照）は、トレーニングデータを使ってモデルを作成し、トレーニングを行います。アルゴリズムは、目的、受け入れ基準、利用可能なデータに基づいて選択する必要があります。

しかし、MLは非常に反復的なワークフローであり、後の活動（モデルの評価など）の結果として、データの調達や前処理など、前の活動に戻る必要がある場合があります。

### A.2.8 モデルを評価する

学習されたモデルは、検証データを用いて、合意された性能評価基準で評価され、その結果をもとにモデルの改良（チューニング）が行われます。評価結果の可視化は通常必要ですが、MLフレームワークによってサポートする可視化オプションが異なります。

実際には、複数のモデルを作成・学習し、評価・チューニングの結果に基づいて最適なモデルを選択するのが一般的です。

### A.2.9 モデルのチューニング

合意されたパフォーマンス指標に対するモデルの評価結果は、データに合わせて設定を調整し、パフォーマンスを向上させるために使用されます。モデルの調整は、ハイパーパラメタチューニングによって行うことができます。ここでは、学習活動を変更したり（例えば、学習ステップ数を変更したり、学習に使用するデータ量を変更したり）、モデルの属性を設定したりします（例えば、ニューラルネットワークのニューロン数や決定木の深さなど）。

### A.2.10 モデルのテスト

モデルの学習、評価、調整、選択が完了したら、テストデータセットに対してテストを行い、合意された性能基準を満たしていることを確認する必要があります。このテストデータは、それまでに使用したトレーニングデータやバリデーションデータとは完全に独立している必要があります。

### A.2.11 モデルのデプロイ

調整されたモデルは、関連するデータパイプラインを含む関連リソースとともに、デプロイメントのために再構築される必要がありますが、これは通常、MLフレームワークによって実現されます。ターゲットとしては、組み込みシステムやクラウドを想定しており、Web
APIでモデルにアクセスできるようになっています。

### A.2.12 モデルの使用

展開されたモデルは、通常、より大きなAIベースのシステムの一部として、運用面で使用することができます。モデルは、設定された時間間隔でスケジュールされたバッチ予測を行うこともあれば、リクエストに応じてリアルタイムで実行することもあります。

### A.2.13 モデルのモニタリングとチューニング

モデルを使用している間に、その状況が進化し（5.1.4「進化」を参照）、モデルが意図した性能から「ドリフト」してしまう危険性があります。ドリフトを確実に特定して管理するために、運用モデルを受け入れ基準に照らして定期的に評価する必要があります。

ドリフトの問題に対処するためにモデルを更新する必要があると考えられる場合や、新しいデータを用いて再トレーニングすることで、より正確でよりロバストなモデルが得られると判断される場合があります。新モデルのパフォーマンスが低下していないことを確認するために、A/B
テスト（8.3 参照）のような形で既存モデルと比較することができます。

### A.3 機械学習の学習データとテストデータ

教師付きMLでは，オーバーフィッティングを防ぐために，2つの異なるデータセット（学習データセットとテストセット）を使用します（A.4.1参照）．テストセットはホールドアウトセットと呼ばれることもあります。

モデルの評価とチューニングを繰り返し行うために、トレーニングデータセットは、トレーニングに使用するデータと評価に使用するバリデーションデータの2つに分けられます。しかし、これはトレーニングに必要なデータが不足していることを意味します。この問題を解決する一つの方法として，クロスバリデーションと呼ばれる方法があります。この方法では，訓練データセットをn個の等しい部分（フォールド）に分割します（図A.3では，訓練データを4つのフォールドに分割しています）。

そして，モデルの学習と評価を n
回行います．毎回，異なるフォールドを検証セットとして使用し，残りのフォールドを学習セットとして使用します．このようにして、学習効果を高めつつ、評価やチューニングを行うことができます。

図A.3 - 4つのフォールドを持つトレーニングデータ

### A.4 機械学習におけるオーバーフィッティングとアンダーフィッティング

### A.4.1 オーバーフィッティング

オーバーフィッティングとは、学習データに含まれる些細な情報やランダムな変動、ノイズなどの余計な情報から、モデルが誤った関係を学習してしまうことです（学習データに含まれる特徴量が多すぎる場合など）。あたかもモデルが学習データを記憶しているかのように、実際の運用では、学習データとよく似たデータに対しては優れた効果を発揮しますが、新しいデータを一般化して扱うことは困難です。オーバーフィッティングを見極める方法の一つとして、トレーニングデータとは完全に分離した独立したテストセットを使用することが挙げられます。

### A.4.2 アンダーフィッティング

アンダーフィッティングは、モデルが学習データの入力と出力の関係を識別できない場合に発生します。不適合は、通常、入力と出力の間の正しい関係を導き出すのに十分な情報を提供する訓練データが不足している場合（訓練データに含まれる特徴が十分でない場合）に発生しますが、選択したアルゴリズムがデータに適合しない場合（非線形データに対応するために線形モデルを作成する場合など）にも発生します。これでは、単純なモデルになってしまい、多くの間違った予測をしてしまいます。

### A.5 データ品質

### A.5.1 データの完全性

例えば、入力センサーの品質が低かったり、校正が不十分だったりした場合、データの品質が低下する可能性があります。これは、センサーデータが複数のソース（大きく異なる測定方法を使用している研究所や、様々なIoTデバイスなど）から得られた場合に、より多くの問題となります。

欠損データには大きく分けて3つの形態があり、それぞれが結果として得られるモデルに異なる影響を与えます\[\^52\]。

\- a)
データが完全に無作為に欠落した場合、モデルの確率論的な性質を考慮すると、ほとんど影響はないはずです（データの欠落による精度の低下以外は）。

\- b)
特定の特徴からのデータが得られない場合（例えば、すべての女性からのデータ）、結果として得られるモデルに悪影響を及ぼす可能性が高くなります（そのモデルが操作上、女性の予測に使用されない場合を除く）。

\- c)
さらに悪いことに、最も検出が難しいのは、3つ目のケースで、特定の特徴を持つデータ値のセットが欠落している場合です（例：35歳から50歳の女性のデータ）。医学研究では、データ収集の性質上、このような問題がしばしば発生します。このような場合、モデルに大きな障害が発生する可能性があります。

### A.5.2 データラベリング

教師付き学習では、学習データが正しいことを前提としています。しかし，実際には，訓練データセットが100%正しくラベル付けされることは稀である\[\^53\]。人間のラベラーは、単純なミス（例：ボタンを押し間違える）、システム的なミス（例：ラベラー全員が間違った指示を受けていた）、主観的な判断（例：物体の色が青か緑かで意見が分かれる）、さらには意図的なミスを犯す可能性もあります。ラベルは必ずしも2つのクラスのいずれかに単純に分類されるわけではなく、より複雑なラベリング作業では正しいラベルが疑われることもあります。1つの言語で書かれたラベルが、2つ目の言語に誤って翻訳されてしまうことがあります。ラベリングには様々な方法がありますが、それぞれの方法にはデータ品質に対する固有のリスクがあります。

\- 社内チームによるラベリング

\- 委託されたラベリング

\- クラウドソースによるラベリング

\- 合成データの生成

\- AIによるラベリング

\- ハイブリッド・アプローチ

### A.5.3 ディストリビューション・シフト

機械学習における大きな不確実性は、学習データの分布と望ましいデータの分布が一致しないことです（この望ましいデータの分布は、通常は操作上の分布ですが、偏りを考慮した理想的な分布である場合もあります；詳細は5.1.5を参照）。

この2つのデータ分布の間にミスマッチがある場合、モデルが提供する予測には欠陥が生じます。

運用開始後、予測される運用データが変化すると（例えば、システムを使用した結果、ユーザーが変わったり、予想外のユーザーがシステムを使用したりすると）、トレーニングデータの分布と望ましいデータの分布の間の距離が大きくなることがよくあります。分布（またはデータセット）シフトは、2つのデータの分布の分岐を表します。特別にプログラムされていない限り、ほとんどのMLシステムは、分布のずれを識別することができず、高い信頼性を持っていても劣った予測をしてしまいます。その変化が大きければ、ユーザーはそれに気づき、システムが提供する予測を（正当に）信用しなくなるでしょう。

理想的には、システムを定期的にテストして、現在の運用状況とシステムが最後にトレーニングされたデータの分布との間の著しい不一致を検出する必要があります。ミスマッチが検出された場合は、最新のトレーニングデータを使ってシステムを再トレーニングする必要があります。

### A.6 機械学習アルゴリズム/モデルの選択

アルゴリズム、モデル設定、ハイパーパラメータの選択は、「サイエンスかアートか」という議論があります\[\^54\]。問題の状況を分析しただけで最適なセットを選択できるような決定的なアプローチはありません。実際には、この選択はほとんどの場合、部分的に試行錯誤によって行われます（図A.2の機械学習ワークフローの明示的な反復部分に示されています）。

この選択に必要な情報は、モデルがどのような機能を提供することが期待されているか、学習アルゴリズムとモデルが利用可能なデータは何か、どのような非

機能要件が満たされている必要があります。

モデルの機能としては、分類、値の予測（回帰）、異常の検出、データからの構造の決定（クラスタリング）などが挙げられます。利用可能なデータ量を知ることで、特定のアルゴリズムを捨てることができるかもしれません（例えば、ビッグデータに依存するアルゴリズムは、利用可能なデータ量が少なければ無視できます）。データにラベルが付いていれば教師付き学習が可能ですが、そうでなければ別のアプローチが必要です。また、モデルで使用されることが予想される特徴の数は、クラスタリングで予想されるクラスの数と同様に、特定のアルゴリズムの選択のポイントとなります。非機能的な要件としては、利用可能なメモリの制約（組み込みシステムなど）、予測速度の制約（リアルタイムシステムなど）、状況によっては更新されたトレーニングデータによるモデルのトレーニング速度の制約などがあります。その他の非機能的な領域としては、透明性、解釈可能性、説明可能性の必要性が挙げられます（5.1.7参照）。

### A.7 MLシステムのドキュメント化

現在、機械学習システムとそのデータセットを文書化するための標準的なフォーマットはありません。

MLシステムが普及し、より重要なアプリケーションに使用されるようになると、各MLシステムについて記録・提供すべき情報について合意し、内部および外部の文書化を改善する努力がなされます。明確な文書化のメリットは以下の通りです。

\- 想定されるユースケースの明確化

\- パフォーマンス特性の明確化

\- 開発者とユーザーのコミュニケーションを円滑にします。

\- 透明性の向上。

\- インサイトをより早く共有することができます。

\- より多くのリソースを共有することができます。

\- 努力の重複を減らすことができました。

\- MLモデルの誤用を減らす。

\- 再利用の増加。

この分野では、以下のような様々な取り組みが行われています。

\- ABOUT ML (Annotation and Benchmarking on Understanding and
Transparencyof Machine Learning Lifecycles) from the Partnership on
AI\[\^55\];

\- データシートのためのデータシート\[\^56\]です。

\- Googleモデルカード\[\^57\]です。

\- IBM Factsheets\[\^58\]をご覧ください。

MLモデルのドキュメントの典型的な内容としては、以下のようなものが考えられます。

\- 一般 -
ID、説明、開発者の詳細、ハードウェア要件、ライセンスの詳細、バージョン、日付、連絡先。

\- 用途 -
主要なユースケース、典型的なユーザー、二次的なユースケース、自己学習型アプローチ、既知のバイアス、倫理的問題、安全性の問題、透明性、判断のしきい値、プラットフォームとパフォーマンスのドリフト。

\- データセット
-収集、入手可能性、前処理、使用、コンテンツ、ラベル付け、サイズ、プライバシー、セキュリティ、バイアス/フェアネス、および制限/制約。

\- トレーニングとパフォーマンス -
MLアルゴリズム、ML構造、検証データセット、パフォーマンス指標の選択、パフォーマンス指標のしきい値、実際のパフォーマンス指標、パフォーマンス効率の指標。

\- テスト -
テストデータセット（説明と入手可能性）、独立性、結果、ロバスト性、ドリフト、ポータビリティ。

### A.8 機械学習のパフォーマンス・メトリクス

### A.8.1 MLパフォーマンスメトリクスの紹介

さまざまな機械学習（ML）モデルを評価するために、さまざまな性能指標が用いられています。この文書は，分類問題のパフォーマンス・メトリクスを扱うことに限定しています
(MLのパフォーマンス・メトリクスに関するより詳細な議論は，ISO/IEC
24029-1\[\^59\]に記載されています)。これらの指標は、最初にMLワークフローの開始時に合意され、その後、MLワークフローの2箇所で評価されます。評価用には、開発者が評価用データセットで許容できるレベルの性能を得るまで、ハイパーパラメータの選択などによってモデルを調整するために使用されます。

続いて、この評価基準を用いて、（独立した）テストセットにおける最終モデルの性能の許容度を測定します。

### A.8.2 マトリックスの混同

イマジンの入力は、MLモデルによって真か偽のどちらかに分類されます。理想的な世界では、すべてのデータが正しく分類されますが、現実にはデータセットが重なることがあり、真と分類されるべきデータポイントが誤って偽と分類されたり（偽陰性）、あるデータポイントは

本来であれば偽に分類されるべきデータポイントが、誤って真に分類されてしまいます（偽陽性）。残りのデータポイントは、真のネガティブまたは真のポジティブのいずれかに正しく分類されます。これらの4つの可能性は、コンフュージョン・マトリックス\[\^60\]で示されます。

### A.8.3 精度

精度は、すべての分類が正しかったものの割合を示すもので、精度Aは次のように計算されます。

\`\`math

A

n n

 n n

トゥルーポジティブ トゥルーネガティブ

トゥルー・ポジティブ・トゥルー

=

\+

\+ nnegative + nfalse positive + nfalse negative

どこで

 ntrue positiveは、真の陽性の数です。

 ntrue negativeは、真のネガティブの数です。

 nfalse positiveは、偽陽性の数です。

 nfalse negativeは、偽陰性の数です。

\`\`\`

### A.8.4 プレシジョン

Precisionは、予測した陽性反応が正しかった割合（予測した陽性反応にどれだけ自信があるか）を測定するもので、精度Pは次のように計算できます。

\`\`math

P

n

 n n

真正

真陽性 偽陽性

=

\+

\`\`\`

### A.8.5 リコール

Recall（またはSensitivity）は、正しく予測された実際の陽性の割合（どれだけ陽性を見逃していないか）を測定するもので、したがってRecall（R）は次のように計算できます。

\`\`math

R

n

 n n

真正

真陽性 偽陰性

=

\+

\`\`\`

### A.8.6 F1スコア

F1スコアは、リコールとプレシジョンのバランス（調和的平均）を提供するため、F1-

スコア「SF1」は、以下のように計算できます。

\`\`math

S PP R

F1 R

= \

2 \# +

\`\`\`

### A.8.7 パフォーマンス・メトリクスの選択

状況に応じて、異なるパフォーマンス指標が適切である。ここでは、最も基本的で、最も一般的なものだけを取り上げています。機械翻訳などのテキスト出力の正しさを測定する必要がある場合、BLEU、ROUGE、METEOR\[\^61\]などのメトリクスが考えられます。

精度は、データセットが対称的な場合、例えば、偽陰性と偽陽性の数が似ている場合に適しています。

精度の高さは、真の陽性を確信したいときに最も有効です（つまり、偽陽性はほとんどないことが望まれます）。例えば、軍事用ドローンがテロリストの標的を攻撃するような場合です。このシナリオでは

何の罪もない傍観者が、誤ってテロリストと認識されることがないように。つまり、誤検出をしないようにしたいので、Precisionは高くする必要があります。

Recallは、真のポジティブを捕らえることが重要な場合（つまり、すべてのまたはほとんどのネガティブを確認する必要がある場合）に最も有効です。例えば、自律走行車が前方にいる人を感知するような場合です。歩行者がいる場合は、確実に識別したいので、偽陰性がない（または少ない）ことが必要です。そのため、Recallは非常に高くなければなりません。

F1は、データの分布が不均一な場合に最も有効です。

これらの性能指標は、MLモデルの平均的な性能を示すものですが、ほとんどの状況では、予想される最悪のケースでのモデルの性能を保証することも重要です。

## Bibliography


[^1]: ISO/IEC 22989\[\^1\], 人工知能 - 概念と用語集

[^2]: Wikipedia contributors, \"AI effect\", Wikipedia,
    https://en.wikipedia.org/w/index.php?title=AI_effect&oldid=920651009
    (accessed November 20, 2019).

[^3]: ISO/IEC TR
    24028:2020、情報技術-人工知能-人工知能における信頼性の概要

[^4]: 100+ AI use cases / applications:A COMPREHENSIVE GUIDE, applied
    AI, https://www.appliedai.de/hub/library-of-use-case-families,
    accessed Oct 2020.

[^5]: 2019 Edelman AI Survey Results Report, Edelman Digital,
    https://www.digitalmarketingcommunity .com/ researches/ edelman
    -artificial -intelligence -survey -results-report-2019/ (accessed
    Nov 20, 2019).

[^6]: 2019 Edelman AI Survey Results Report, Edelman Digital,
    https://www.digitalmarketingcommunity .com/ researches/ edelman
    -artificial -intelligence -survey -results-report-2019/ (accessed
    Nov 20, 2019).

[^7]: \"The AI Index 2017 Annual Report\", AI Index Steering Committee,
    Human-Centered AI Initiative,Stanford University, Stanford, CA,
    December 2017.

[^8]: 徹底的に。人工知能2019年版、Statista Report
    2019、https://people.stfx.ca/x2011/x2011aqi/ School/ 2018 -2019/
    Winter/ BSAD %20471 %20 - %20Strat/ Case/ AI %20statista
    .pdf、2019年2月。

[^9]: Agrawal, Gans, Goldfarb et al, \"Prediction Machines:The Simple
    Economics of Artificial Intelligence\", Harvard Business Review
    Press, 2018.

[^10]: レポートW.Q.第10版、ギャップジェミニ、https://www.sogeti.com/explore/reports/world-quality-report-201819/、2018年9月。

[^11]: レポートW.Q.第10版、ギャップジェミニ、https://www.sogeti.com/explore/reports/world-quality-report-201819/、2018年9月。

[^12]: ISO/IEC 22989\[\^1\], 人工知能 - 概念と用語集

[^13]: Robot Density rises globally, IFR Press Release,
    https://ifr.org/ifr-press-releases/news/robot-density-rises-globally,
    Feb 2018.

[^14]: Russel J., Phase Change Memory Shows Promise for AI Use Says IBM,
    HPC Wire, https://www .hpcwire .com/ 2018/ 10/ 24/ phase -change
    -memory -shows -promise -for -ai -use -says -ibm/, 2018年10月.

[^15]: TensorFlow, https://www.tensorflow.org/ (accessed May 2020)

[^16]: PyTorch, https://pytorch.org/ (accessed May 2020)

[^17]: MxNet, https://mxnet.apache.org/ (accessed May 2020)

[^18]: CNTK, https://docs.microsoft.com/en-us/cognitive-toolkit/
    (accessed May 2020)

[^19]: Keras, https://keras.io/ (accessed May 2020)

[^20]: Wikipedia contributors, \"AI effect\", Wikipedia,
    https://en.wikipedia.org/w/index.php?title=AI_effect&oldid=920651009
    (accessed November 20, 2019).

[^21]: ISO/IEC 22989\[\^1\], 人工知能 - 概念と用語集

[^22]: IDC Survey Finds Artificial Intelligence to be a Priority for
    Organizations But Few Have Implemented an Enterpris-Wide Strategy,
    July 2019, https://www.idc.com/getdoc.jsp?containerId=prUS45344519,
    accessed Jan 2020.

[^23]: テストの現状報告。2019年、バージョン1.3、PractiTest、https://qablog.practitest.com/state-of-testing/、2019年7月。

[^24]: ISTQB® Worldwide Software Testing Practices Report 2017-18
    (Revised), ISTQB, https://www.istqb .org/ references/ surveys/ istqb
    %C2 %AE -worldwide -software -testing -practices
    -survey-2017-18.html, accessed October 2018.

[^25]: Hackett M., 2018 Trends Survey Results,
    https://www.logigear.com/magazine/survey/2018-trends-survey-results/,
    accessed Nov 2019.

[^26]: テストの現状報告。2019年、バージョン1.3、PractiTest、https://qablog.practitest.com/state-of-testing/、2019年7月。

[^27]: レポートW.Q.第10版、ギャップジェミニ、https://www.sogeti.com/explore/reports/world-quality-report-201819/、2018年9月。

[^28]: The Second IEEE International Conference On Artificial
    Intelligence Testing,
    http://ieeeaitests.com/html/callForPapers.html.

[^29]: ISO/IEC TR
    24028:2020、情報技術-人工知能-人工知能における信頼性の概要

[^30]: DIN SPEC 92001-2, Artificial Intelligence - Life Cycle Processes
    and Quality Requirements - Part 2: Technical and Organizational
    Requirements,
    https://www.din.de/en/innovation-and-research/din-spec-en/projects/wdc-proj:din21:298702628,
    accessed Jan 2020.

[^31]: DIN SPEC 92001-1, Artificial Intelligence - Life Cycle Processes
    and Quality Requirements - Part 1:Quality Meta Model,
    https://www.din.de/en/wdc-beuth:din21:303650673, accessed Jan 2020.

[^32]: IEEE 7000, IEEE Draft Model Process for Addressing Ethical
    Concerns During System Design

[^33]: ISO/IEC TR 24368\[\^5\], 情報技術 - 人工知能 -
    倫理的・社会的関心事の概要

[^34]: ONNX, Open Neural Network Exchange, https://onnx.ai/ (accessed
    May 2020)

[^35]: NNEF, Neural Network Exchange Format,
    https://www.khronos.org/nnef/ (accessed May 2020)

[^36]: PMML。Predictive Model Markup Language,
    http://dmg.org/pmml/v4-4/GeneralStructure.html(accessed May 2020)

[^37]: A guide to using artificial intelligence in public sector, June
    2019, UK Government DigitalService,
    https://www.gov.uk/government/publications/understanding-artificial-intelligence/a-guide-to-using-artificial-intelligence-in-the-public-sector
    (accessed May 2020).

[^38]: IEC 61508、機能安全、IEC 61508

[^39]: ISO 26262 (all parts), ロードビークル - 機能安全

[^40]: 自動運転システム（ADS）。A Vision for Safety 2.0,
    https://www.nhtsa.gov/sites/nhtsa.dot.gov/files/documents/13069a-ads2.0_090617_v9a_tag.pdf
    (accessed May 2020)

[^41]: 自律走行車等の評価のための安全基準の提示https://ul.org/UL4600
    (accessed May 2020)

[^42]: DIN SPEC 92001-1, Artificial Intelligence - Life Cycle Processes
    and Quality Requirements - Part 1:Quality Meta Model,
    https://www.din.de/en/wdc-beuth:din21:303650673, accessed Jan 2020.

[^43]: ISO/IEC/IEEE
    12207:2017、システムおよびソフトウェアエンジニアリング-ソフトウェアライフサイクルプロセス

[^44]: DIN SPEC 92001-2, Artificial Intelligence - Life Cycle Processes
    and Quality Requirements - Part 2: Technical and Organizational
    Requirements,
    https://www.din.de/en/innovation-and-research/din-spec-en/projects/wdc-proj:din21:298702628,
    accessed Jan 2020.

[^45]: ISO/IEC
    25010:2011、システム及びソフトウェア工学-システム及びソフトウェアの品質要求及び評価（SQuaRE）-システム及びソフトウェアの品質モデル

[^46]: ISO/IEC
    5059\[\^4\],ソフトウェアエンジニアリング-システムおよびソフトウェアの品質要求と評価（SQuaRE）-AIベースのシステムの品質モデル

[^47]: ISO/IEC
    25010:2011、システム及びソフトウェア工学-システム及びソフトウェアの品質要求及び評価（SQuaRE）-システム及びソフトウェアの品質モデル

[^48]: Marir et al, , QM4MAS: a quality model for multi-agent systems,
    Int.J. Computer Applications inTechnology, 2016, 54.

[^49]: Salkever et al. A.I. Bias Isn\'t the Problem.Our Society Is,
    Fortune.com,
    https://fortune.com/2019/04/14/ai-artificial-intelligence-bias/,
    accessed Nov 2019.

[^50]: ISO/IEC TR 24027\[\^6\],
    情報技術-人工知能（AI）-AIシステムにおけるバイアスおよびAIによる意思決定支援

[^51]: Explainable AI: the basics - Policy Briefing, The Royal Society,
    Nov 2019.

[^52]: Wikipedia contributors, \"Explainable artificial intelligence,\"
    Wikipedia,
    https://en.wikipedia.org/w/index.php?title=Explainable_artificial_intelligence&oldid=924090418
    (accessedNovember 20, 2019).

[^53]: Increasing transparency with Google Cloud Explainable AI, Product
    News, https://cloud.google.com/ blog/ products/ ai -machine
    -learning/ google -cloud -ai -explanations -to -increase
    -fairness-responsibility-and-trust, accessed Nov 2019, Nov 2019.

[^54]: ISO/IEC TR
    24028:2020、情報技術-人工知能-人工知能における信頼性の概要

[^55]: Russell, Of Myths and Moonshine, contribution to the conversation
    on The Myth of AI,
    https://www.edge.org/conversation/jaron_lanier-the-myth-of-ai,
    accessed Nov 2019.

[^56]: Bird J. et al. \"The evolved radio and its implications for
    modelling the evolution of novel sensors.\"Proceedings of the 2002
    Congress on Evolutionary Computation.CEC\'02.

[^57]: Awad et al, , The Moral Machine experiment, Nature, 563, pages
    59-64, 2018.

[^58]: European Commission High-Level Expert Group on Artificial
    Intelligence, Ethics Guidelines forTrustworthy AI, European
    Commission, April 2019.

[^59]: Banks et al. Requirements Assurance in Machine Learning,
    Proceedings of the AAAI Workshopon Artificial Intelligence Safety
    2019, Jan 2019.

[^60]: ISO/IEC 20889:2018「Privacy Enhanced Data De-identification
    terminology and classification
    oftechniques（プライバシー強化のためのデータ非識別化の用語および技術の分類

[^61]: Leetaru, How Twitter Corrupted Microsoft\'s Tay,
    https://www.forbes.com/sites/kalevleetaru/2016/ 03/ 24/ how -twitter
    -corrupted -microsofts -tay -a -crash -course -in -the -dangers -of
    -ai -in -the-real-world/\#202233ae26d2, 2016年3月.

[^62]: ISO/IEC
    25010:2011、システム及びソフトウェア工学-システム及びソフトウェアの品質要求及び評価（SQuaRE）-システム及びソフトウェアの品質モデル

[^63]: Qiu et al, , Review of Artificial Intelligence Adversarial Attack
    and Defense Technologies, AppliedSciences 9(5):909, Mar 2019.

[^64]: Training Benchmarks M.L. Website: https://mlperf.org/, accessed
    Nov 2019.

[^65]: Stanford DAWN Deep Learning Benchmark.ウェブサイト:
    https://dawn.cs.stanford.edu/benchmark/, accessed Nov 2019.

[^66]: オントロジー・アライメント評価イニシアチブ。ウェブサイト:
    http://oaei.ontologymatching.org/, accessedNov 2019.

[^67]: Kuhn et al, , Software Fault Interactions and Implications for
    Software Testing, IEEE Transactionson Software Engineering 30(6):418
    - 421, July 2004.

[^68]: ISO/IEC/IEEE
    29119-4「ソフトウェアおよびシステムエンジニアリング-ソフトウェアテスト-第4部：テストテクニック」。

[^69]: Kuhn et al, , Software Fault Interactions and Implications for
    Software Testing, IEEE Transactionson Software Engineering 30(6):418
    - 421, July 2004.

[^70]: Wikipedia contributors, \"A/Bテスト,\" Wikipedia,
    https://en.wikipedia.org/w/index.php?title=A/B_testing&oldid=926805728
    (accessed November 21, 2019).

[^71]: Chen et al, , Metamorphic Testing:A Review of Challenges and
    Opportunities, ACM Comput.Surv.51, 1, Article 4, January 2018.

[^72]: Segura et al, , A Survey on Metamorphic Testing, IEEE Trans. on
    Software Engineering, Vol 42,No.9, Sept 2016.

[^73]: Liu et al, , How effectively does metamorphic testing alleviate
    oracle problem?, IEEETransactions on Software Engineering 40, 1,
    4-22, 2014.

[^74]: Pei, K., et al, DeepXplore:Automated Whitebox Testing of Deep
    Learning Systems Commun.ACM,Association for Computing Machinery,
    2019, 62, 137-145 p.

[^75]: Pei et al, DeepXplore:Automated Whitebox Testing of Deep Learning
    Systems, SOSP \'17, October28, 2017, Shanghai, China.

[^76]: Pei et al, DeepXplore:Automated Whitebox Testing of Deep Learning
    Systems, SOSP \'17, October28, 2017, Shanghai, China.

[^77]: Sun et al. Testing Deep Neural Networks,
    https://www.researchgate.net/publication/323747173_Testing_Deep_Neural_Networks,
    accessed Nov 2019, Mar 2018.

[^78]: Sun et al. Testing Deep Neural Networks,
    https://www.researchgate.net/publication/323747173_Testing_Deep_Neural_Networks,
    accessed Nov 2019, Mar 2018.

[^79]: Sun et al. Testing Deep Neural Networks,
    https://www.researchgate.net/publication/323747173_Testing_Deep_Neural_Networks,
    accessed Nov 2019, Mar 2018.

[^80]: Sun et al. Testing Deep Neural Networks,
    https://www.researchgate.net/publication/323747173_Testing_Deep_Neural_Networks,
    accessed Nov 2019, Mar 2018.

[^81]: Pei et al, DeepXplore:Automated Whitebox Testing of Deep Learning
    Systems, SOSP \'17, October28, 2017, Shanghai, China.

[^82]: Tian et al. DeepTest:Automated Testing of
    Deep-Neural-Network-driven Autonomous Cars, ICSE\'18: 40th
    International Conference on Software Engineering, May 2018.

[^83]: Sun et al. Testing Deep Neural Networks,
    https://www.researchgate.net/publication/323747173_Testing_Deep_Neural_Networks,
    accessed Nov 2019, Mar 2018.

[^84]: Nokia\'s revolutionary 5G virtual testing speeds deployment,
    Press Release, https://www.globenewswire .com/ news -release/ 2019/
    04/ 23/ 1807667/ 0/ en/ Nokia -s -revolutionary
    -5G-virtual-testing-speeds-deployment.html, accessed Nov 2019, April
    2019.

[^85]: Official documentation for MORSE project,
    http://www.openrobots.org/morse/doc/0.2.1/morse.html, accessed Nov
    2019.

[^86]: Savva et al. Open-sourcing AI Habitat, an advanced simulation
    platform for embodied AIresearch, https://arxiv.org/abs/1904.01201,
    Nov 2019.

[^87]: NVIDIA DRIVE CONSTELLATION - Virtual Reality Autonomous Vehicle
    Simulator。NVIDIAProducts,
    https://www.nvidia.com/en-us/self-driving-cars/drive-constellation/,
    accessedNov 2019.

[^88]: ISO/IEC 23053\[\^2\],
    機械学習（ML）を用いた人工知能（AI）システムのフレームワーク

[^89]: Keevers, Cross-validation is insufficient for model validation,
    Technical Report, AustralianDefence Science and Technology Group,
    Mar 2019.

[^90]: Frenay et al, , Classification in the Presence of Label Noise:A
    Survey, IEEE Transactions onNeural Networks and Learning Systems,
    May 2014.

[^91]: Henderson et al. Deep reinforcement learning that matters,
    Thirty-Second AAAI Conference onArtificial Intelligence, 2018.

[^92]: Annotation and Benchmarking on Understanding and Transparency of
    Machine LearningLifecycles (ABOUT ML), v0 Final Draft,
    https://www.partnershiponai.org/wp-content/uploads/2019/07/ABOUT-ML-v0-Draft-Final.pdf,
    accessed Jan 2020.

[^93]: Gebru T. et al. Datasheets for Datasets,
    https://arxiv.org/pdf/1803.09010.pdf, accessed Jan 2020.

[^94]: Mitchell M. et al. Model Cards for Model Reporting,
    https://arxiv.org/abs/1810.03993, accessedJan 2020.

[^95]: Arnold M. et al. FactSheets:Increasing Trust in AI Services
    through Supplier\'s Declarations ofConformity,
    https://arxiv.org/abs/1808.07261, accessed Jan 2020.

[^96]: ISO/IEC TR 24029-1\[\^3\], 人工知能（AI） -
    ニューラルネットワークの堅牢性の評価 - 第1部:概要

[^97]: Wikipedia contributors, \"Confusion matrix,\" Wikipedia,
    https://en.wikipedia.org/w/index.php?title=Confusion_matrix&oldid=922488584
    (accessed November 21, 2019).

[^98]: Raj, Metrics for NLG evaluation, Medium.com,
    https://medium.com/explorations-in-language-and-learning/metrics-for-nlg-evaluation-c89b6a781054,
    accessed Nov 2019.
